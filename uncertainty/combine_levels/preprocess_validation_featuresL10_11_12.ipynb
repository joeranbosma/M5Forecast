{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest0\\joeran\\M5Forecast\\uncertainty\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guest0\\joeran\\M5Forecast\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os, gc\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.layers import (Dense, Dropout, Flatten, Input, BatchNormalization, Lambda, \n",
    "                                     concatenate, Embedding, Reshape)\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# own imports\n",
    "from evaluation import Referee, load_data, select_dates, select_day_nums\n",
    "from train import BatchCreator, Logger, plot_confidence_series, plot_some_confidence_intervals\n",
    "from model_builder import (get_pinball_losses, get_simple_dist_model, get_simple_dense_model,\n",
    "                           get_variable_dist_model, get_direct_dist_model)\n",
    "from flow import model_predict, denorm_preds, warp_preds_to_ref_form, plot_some, evaluate_model\n",
    "from preprocess import preprocess, get_features, pandas_cat_data, reset_categorical_features\n",
    "\n",
    "\n",
    "# Setup file paths\n",
    "data_dir = 'data/'\n",
    "os.environ['DATA_DIR'] = data_dir\n",
    "sub_dir = 'submissions_uncertainty/'\n",
    "plt.rcParams['font.size'] = 13\n",
    "\n",
    "# Hardcode requested quantiles\n",
    "quantiles = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement logic for obtaining the features for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Referee\n",
      "Converting true sales to quantile form\n",
      "Calculating weights for each level...\n",
      "Calculating scale for each level...\n",
      "Finished setup.\n"
     ]
    }
   ],
   "source": [
    "# Load in the data set\n",
    "calendar, sales_train_validation, sell_prices = load_data()\n",
    "\n",
    "sales_true = select_dates(sales_train_validation, day_end=1913, num_days=28, include_metadata=True)\n",
    "sales_train = select_dates(sales_train_validation, day_start=1, num_days=1913-28, include_metadata=True)\n",
    "\n",
    "ref = Referee(sales_true, sales_train, sell_prices, calendar, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For each level:  \n",
    "\n",
    "1. Read the (prepared) dataset\n",
    "2. Get the validation batch creator\n",
    "3. Get the denormalised predictions\n",
    "4. Save to file\n",
    "\n",
    "Then:\n",
    "\n",
    "1. Combine predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from flow import restore_tags_converted_sales, read_converted_sales\n",
    "from lightgbm_kernel import read_data, encode_categorical, reduce_mem_usage\n",
    "from feature_extraction import aggregate_adapted_fe\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input sizes for the model\n",
    "input_shapes = {\n",
    "    1: (178,),\n",
    "    2: (181,),\n",
    "    3: (187,),\n",
    "    4: (181,),\n",
    "    5: (188,),\n",
    "    6: (184,),\n",
    "    7: (191,),\n",
    "    8: (194,),\n",
    "    9: (201,),\n",
    "    10: (3237,),\n",
    "    11: (3240,),\n",
    "    12: (3255,),\n",
    "}\n",
    "\n",
    "# select whether to perform the category-reset step of the preprocessing\n",
    "# reset_categories = {d: False if d in [3] else True for d in range(1, 1+12)}\n",
    "reset_categories = {d: True for d in range(1, 1+12)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import categorical_features\n",
    "level = 3\n",
    "DAYS_PRED = 28\n",
    "prediction_lag = DAYS_PRED\n",
    "n_years = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "Mem. usage decreased to  0.12 Mb (41.9% reduction)\n",
      "Calendar has 1969 rows and 14 columns\n",
      "Mem. usage decreased to 130.48 Mb (37.5% reduction)\n",
      "Sell prices has 6841121 rows and 4 columns\n",
      "Sales train validation has 30490 rows and 1919 columns\n",
      "Mem. usage decreased to  0.00 Mb (74.3% reduction)\n",
      "1 28\n",
      "Mem. usage decreased to  0.08 Mb (36.9% reduction)\n",
      "Mem. usage decreased to  0.00 Mb (0.0% reduction)\n",
      "Mem. usage decreased to 45.67 Mb (41.7% reduction)\n",
      "Melted sales train validation has 1913 rows and 3 columns\n",
      "Selecting 2190 rows (112.8%)\n",
      "Mem. usage decreased to  0.15 Mb (53.6% reduction)\n",
      "Converting to pandas data..\n",
      "Resetting categories..\n",
      "['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'shift_t28', 'shift_t29', 'shift_t30', 'rolling_std_t7', 'rolling_std_t30', 'rolling_std_t60', 'rolling_std_t90', 'rolling_std_t180', 'rolling_mean_t7', 'rolling_mean_t30', 'rolling_mean_t60', 'rolling_mean_t90', 'rolling_mean_t180', 'rolling_skew_t30', 'rolling_kurt_t30', 'year', 'month', 'week', 'day', 'dayofweek', 'is_year_end', 'is_year_start', 'is_quarter_end', 'is_quarter_start', 'is_month_end', 'is_month_start', 'is_weekend']\n",
      "Saving to file..\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/eval_prep/level_1_simple_fe_2016_04_24_2016_05_22_normalised_demand_lag_28.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d17605aa0cce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    107\u001b[0m     )\n\u001b[0;32m    108\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Saving to file..\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0meval_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Finished.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_pickle\u001b[1;34m(self, path, compression, protocol)\u001b[0m\n\u001b[0;32m   2723\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpickle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2725\u001b[1;33m         \u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2726\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2727\u001b[0m     def to_clipboard(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mto_pickle\u001b[1;34m(obj, filepath_or_buffer, compression, protocol)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"infer\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/eval_prep/level_1_simple_fe_2016_04_24_2016_05_22_normalised_demand_lag_28.pickle'"
     ]
    }
   ],
   "source": [
    "for level in range(1, 1+12):\n",
    "    # read data for pipeline from lightgbm-poisson-w-scaled-pinball-loss.ipynb\n",
    "    calendar, sell_prices, sales_train_val, submission = read_data()  # with memory reduction\n",
    "\n",
    "    # read comverted sales\n",
    "    converted_sales = read_converted_sales(level=level, data_dir=data_dir)\n",
    "\n",
    "    ### Replace demand with normalised sales\n",
    "    sales_train_val = converted_sales\n",
    "\n",
    "    ## Count\n",
    "    NUM_ITEMS = sales_train_val.shape[0]  # 1 / ... / 70 / ... / 30,240\n",
    "    if DAYS_PRED is None:\n",
    "        DAYS_PRED = submission.shape[1] - 1  # 28\n",
    "    print(NUM_ITEMS, DAYS_PRED)\n",
    "    \n",
    "    nrows = int(365 * n_years * NUM_ITEMS)\n",
    "\n",
    "    ## Encode categorical features\n",
    "    calendar = encode_categorical(\n",
    "        calendar, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n",
    "    ).pipe(reduce_mem_usage)\n",
    "\n",
    "    sales_train_val = encode_categorical(\n",
    "        sales_train_val, categorical_features[level],\n",
    "    ).pipe(reduce_mem_usage)\n",
    "\n",
    "    sell_prices = encode_categorical(sell_prices, [\"item_id\", \"store_id\"]).pipe(\n",
    "        reduce_mem_usage\n",
    "    )\n",
    "\n",
    "    ## Reshape\n",
    "    sales_train_val = pd.melt(sales_train_val,\n",
    "                              id_vars=['id', *categorical_features[level]],\n",
    "                              var_name='day', value_name='demand')\n",
    "    print('Melted sales train validation has {} rows and {} columns'.format(*sales_train_val.shape))\n",
    "    \n",
    "    #################################### ADD EVAL ###\n",
    "    \n",
    "    # select which days to add\n",
    "    day_start = 1914\n",
    "    day_end = 1941\n",
    "    n_days = day_end - day_start + 1\n",
    "\n",
    "    # select a 'prototype' for the new days\n",
    "    df_bit = sales_train_val[sales_train_val.day == 'd_1913'].copy()\n",
    "    df_bit['demand'] = -1\n",
    "    df_bit = df_bit.reset_index(drop=True)\n",
    "    \n",
    "    # repeat the prototype for n days\n",
    "    df_eval = pd.concat([df_bit]*n_days, ignore_index=True)\n",
    "\n",
    "    # set the day to the corresponding values, e.g. d_1914, ..., d_1941\n",
    "    validation_d_cols = ['d_%d'%d for d in range(day_start, day_end+1)]\n",
    "    df_eval.day = np.repeat(validation_d_cols, df_bit.index.size)\n",
    "    \n",
    "    # merge with 'normal' train/val sales\n",
    "    sales_train_val = pd.concat((sales_train_val, df_eval), ignore_index=True)\n",
    "    #################################### END OF ADD EVAL ###\n",
    "    \n",
    "    print(\"Selecting {} rows ({:.1%})\".format(nrows, nrows / sales_train_val.index.size))\n",
    "    data = sales_train_val.iloc[-nrows:, :]\n",
    "\n",
    "    ## Add calendar features\n",
    "    # calendarの結合\n",
    "    # drop some calendar features(不要な変数の削除:weekdayやwdayなどはdatetime変数から後ほど作成できる。)\n",
    "    calendar.drop(['weekday', 'wday', 'month', 'year'],\n",
    "                  inplace=True, axis=1)\n",
    "\n",
    "    # notebook crash with the entire dataset (maybe use tensorflow, dask, pyspark xD)(dayとdをキーにdataに結合)\n",
    "    data = pd.merge(data, calendar, how='left', left_on=['day'], right_on=['d'])\n",
    "    data.drop(['d', 'day'], inplace=True, axis=1)\n",
    "\n",
    "    # add sell price if all of the columns 'store_id', 'item_id', 'wm_yr_wk' are available\n",
    "    # sell price\n",
    "    if np.prod([col in data.columns for col in ['store_id', 'item_id', 'wm_yr_wk']]):\n",
    "        # get the sell price data (this feature should be very important)\n",
    "        data = data.merge(sell_prices, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "        print('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n",
    "\n",
    "    # memory\n",
    "    del sell_prices\n",
    "    gc.collect()\n",
    "\n",
    "    data = reduce_mem_usage(aggregate_adapted_fe(data, DAYS_PRED=DAYS_PRED))\n",
    "    \n",
    "    if reset_categories[level]:\n",
    "        print(\"Converting to pandas data..\") if verbose else None\n",
    "        # set categorical features\n",
    "        data, available_cat_features = pandas_cat_data(data)\n",
    "\n",
    "        print(\"Resetting categories..\") if verbose else None\n",
    "        # reset categorical features, set NaN events as additional category, set NaN shift/std/mean/kurt/skew to zero\n",
    "        data = reset_categorical_features(data, available_cat_features)\n",
    "\n",
    "    # select features\n",
    "    features = get_features(level=level, prediction_lag=prediction_lag,\n",
    "                            sell_price_features=('sell_price' in data.columns))\n",
    "    print(features) if verbose else None\n",
    "    \n",
    "    #################################### SAVE EVAL ###\n",
    "    eval_df = data[(data['date'] >= '2016-04-24') & (data['date'] <= '2016-05-22')]\n",
    "    \n",
    "    fn = data_dir + 'eval_prep/level_{}_simple_fe_{}_{}_normalised_demand_lag_{}.pickle'.format(\n",
    "        level, eval_df.date.min().date().strftime(\"%Y_%m_%d\"), eval_df.date.max().date().strftime(\"%Y_%m_%d\"),\n",
    "        DAYS_PRED\n",
    "    )\n",
    "    print(\"Saving to file..\")\n",
    "    eval_df.to_pickle(fn)\n",
    "    print(\"Finished.\")\n",
    "    \n",
    "    del data\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redid lavel 3 with reset categories = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01324b84642f4e1fafc49cf89f9ff391": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2573316ddee1409d897edfcfbd87e8ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5712bde02e284e70bc5e9c1d7367502a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_01324b84642f4e1fafc49cf89f9ff391",
       "placeholder": "​",
       "style": "IPY_MODEL_860311c3b0564afa891a4c6e24da2d3a",
       "value": " 42840/42840 [00:49&lt;00:00, 860.84it/s]"
      }
     },
     "5f64a626e4d342ccaa955ea1c68afacd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6314c6fca7a845618625e72ac301f6a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2573316ddee1409d897edfcfbd87e8ba",
       "max": 42840,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7370a0011b714f6dbba43bf3f3725de8",
       "value": 42840
      }
     },
     "7370a0011b714f6dbba43bf3f3725de8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "860311c3b0564afa891a4c6e24da2d3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8c18e2818bf648e08be0bc8e855fcb3f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6314c6fca7a845618625e72ac301f6a7",
        "IPY_MODEL_5712bde02e284e70bc5e9c1d7367502a"
       ],
       "layout": "IPY_MODEL_5f64a626e4d342ccaa955ea1c68afacd"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
