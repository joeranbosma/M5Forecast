{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\18-09-19 Document structure\\business\\Study\\Master\\Cognitive Computing\\P3\\Machine learning in practice\\git\\Private\\M5Forecast\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "E:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os, gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, LeakyReLU\n",
    "from tensorflow.keras.layers import Flatten, Input, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, concatenate, Reshape, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "os.environ['DATA_DIR'] = 'data/'\n",
    "os.environ['SUB_DIR'] = 'submissions_uncertainty/'\n",
    "for dirname, _, filenames in os.walk(os.environ['DATA_DIR']):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Hardcode requested quantiles\n",
    "quantiles = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    f, ax = plt.subplots(1, 1, figsize=(18, 6))\n",
    "    ax.plot(history.history['loss'], label='Train')\n",
    "    ax.plot(history.history['val_loss'], label='Validation')\n",
    "    ax.set_ylim(0)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinball Loss function for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"custom_layer/features.csv\", index_col=0)\n",
    "target_df = pd.read_csv(\"custom_layer/targets.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "      <th>w_3</th>\n",
       "      <th>w_4</th>\n",
       "      <th>w_5</th>\n",
       "      <th>w_6</th>\n",
       "      <th>w_7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-30</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-31</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-02-01</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-02-02</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            wday  month  snap_CA  w_1  w_2  w_3  w_4  w_5  w_6  w_7\n",
       "date                                                               \n",
       "2011-01-29     1      1        0    1    0    0    0    0    0    0\n",
       "2011-01-30     2      1        0    0    1    0    0    0    0    0\n",
       "2011-01-31     3      1        0    0    0    1    0    0    0    0\n",
       "2011-02-01     4      2        1    0    0    0    1    0    0    0\n",
       "2011-02-02     5      2        1    0    0    0    0    1    0    0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.005</th>\n",
       "      <th>0.025</th>\n",
       "      <th>0.165</th>\n",
       "      <th>0.25</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.75</th>\n",
       "      <th>0.835</th>\n",
       "      <th>0.975</th>\n",
       "      <th>0.995</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>0.429084</td>\n",
       "      <td>0.500587</td>\n",
       "      <td>0.628482</td>\n",
       "      <td>0.662077</td>\n",
       "      <td>0.743227</td>\n",
       "      <td>0.824528</td>\n",
       "      <td>0.861499</td>\n",
       "      <td>0.989619</td>\n",
       "      <td>1.086067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-30</th>\n",
       "      <td>0.834881</td>\n",
       "      <td>1.032595</td>\n",
       "      <td>1.265410</td>\n",
       "      <td>1.343565</td>\n",
       "      <td>1.503276</td>\n",
       "      <td>1.660740</td>\n",
       "      <td>1.728179</td>\n",
       "      <td>2.014386</td>\n",
       "      <td>2.143587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-31</th>\n",
       "      <td>0.838167</td>\n",
       "      <td>0.991256</td>\n",
       "      <td>1.246369</td>\n",
       "      <td>1.329488</td>\n",
       "      <td>1.508577</td>\n",
       "      <td>1.680692</td>\n",
       "      <td>1.763780</td>\n",
       "      <td>1.976137</td>\n",
       "      <td>2.167394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-02-01</th>\n",
       "      <td>0.424697</td>\n",
       "      <td>0.501162</td>\n",
       "      <td>0.629065</td>\n",
       "      <td>0.668797</td>\n",
       "      <td>0.758173</td>\n",
       "      <td>0.842714</td>\n",
       "      <td>0.878296</td>\n",
       "      <td>1.015888</td>\n",
       "      <td>1.090475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-02-02</th>\n",
       "      <td>0.438830</td>\n",
       "      <td>0.511583</td>\n",
       "      <td>0.630601</td>\n",
       "      <td>0.669403</td>\n",
       "      <td>0.754088</td>\n",
       "      <td>0.842727</td>\n",
       "      <td>0.883785</td>\n",
       "      <td>1.007879</td>\n",
       "      <td>1.084537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0.005     0.025     0.165      0.25       0.5      0.75  \\\n",
       "date                                                                     \n",
       "2011-01-29  0.429084  0.500587  0.628482  0.662077  0.743227  0.824528   \n",
       "2011-01-30  0.834881  1.032595  1.265410  1.343565  1.503276  1.660740   \n",
       "2011-01-31  0.838167  0.991256  1.246369  1.329488  1.508577  1.680692   \n",
       "2011-02-01  0.424697  0.501162  0.629065  0.668797  0.758173  0.842714   \n",
       "2011-02-02  0.438830  0.511583  0.630601  0.669403  0.754088  0.842727   \n",
       "\n",
       "               0.835     0.975     0.995  \n",
       "date                                      \n",
       "2011-01-29  0.861499  0.989619  1.086067  \n",
       "2011-01-30  1.728179  2.014386  2.143587  \n",
       "2011-01-31  1.763780  1.976137  2.167394  \n",
       "2011-02-01  0.878296  1.015888  1.090475  \n",
       "2011-02-02  0.883785  1.007879  1.084537  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(inp_shape, quantiles):\n",
    "    # clear previous sessions\n",
    "    K.clear_session()\n",
    "\n",
    "    inp = Input(inp_shape, name=\"input\")\n",
    "    x = inp\n",
    "    x = Dense(16)(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = Dense(2)(x)  # represents mu, sigma\n",
    "    x = Dense(len(quantiles))(x)  # returns 9 points, one for each quantile\n",
    "    out = x\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 9)                 27        \n",
      "=================================================================\n",
      "Total params: 2,989\n",
      "Trainable params: 2,989\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "quantiles = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]\n",
    "model = get_model(inp_shape=(train_df.columns.size,), quantiles=quantiles)\n",
    "model.compile(optimizer=\"adam\", loss=\"MAE\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.values\n",
    "y = target_df.values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 269 samples, validate on 68 samples\n",
      "Epoch 1/300\n",
      "269/269 [==============================] - 2s 9ms/sample - loss: 1.1917 - val_loss: 0.9042\n",
      "Epoch 2/300\n",
      "269/269 [==============================] - 0s 749us/sample - loss: 0.8217 - val_loss: 0.7791\n",
      "Epoch 3/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.7792 - val_loss: 0.6893\n",
      "Epoch 4/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.7038 - val_loss: 0.6442\n",
      "Epoch 5/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.6833 - val_loss: 0.6131\n",
      "Epoch 6/300\n",
      "269/269 [==============================] - 0s 130us/sample - loss: 0.6547 - val_loss: 0.5905\n",
      "Epoch 7/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.6272 - val_loss: 0.5632\n",
      "Epoch 8/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.5996 - val_loss: 0.5338\n",
      "Epoch 9/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.5675 - val_loss: 0.5071\n",
      "Epoch 10/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.5388 - val_loss: 0.4832\n",
      "Epoch 11/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.5117 - val_loss: 0.4658\n",
      "Epoch 12/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.4850 - val_loss: 0.4283\n",
      "Epoch 13/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.4563 - val_loss: 0.4109\n",
      "Epoch 14/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.4329 - val_loss: 0.3875\n",
      "Epoch 15/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.4086 - val_loss: 0.3633\n",
      "Epoch 16/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.3857 - val_loss: 0.3409\n",
      "Epoch 17/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.3617 - val_loss: 0.3194\n",
      "Epoch 18/300\n",
      "269/269 [==============================] - 0s 126us/sample - loss: 0.3366 - val_loss: 0.2946\n",
      "Epoch 19/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.3118 - val_loss: 0.2766\n",
      "Epoch 20/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.2952 - val_loss: 0.2471\n",
      "Epoch 21/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.2641 - val_loss: 0.2320\n",
      "Epoch 22/300\n",
      "269/269 [==============================] - 0s 125us/sample - loss: 0.2472 - val_loss: 0.2236\n",
      "Epoch 23/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.2265 - val_loss: 0.1986\n",
      "Epoch 24/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.2084 - val_loss: 0.1832\n",
      "Epoch 25/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.2081 - val_loss: 0.1992\n",
      "Epoch 26/300\n",
      "269/269 [==============================] - 0s 134us/sample - loss: 0.2031 - val_loss: 0.1971\n",
      "Epoch 27/300\n",
      "269/269 [==============================] - 0s 128us/sample - loss: 0.1936 - val_loss: 0.1681\n",
      "Epoch 28/300\n",
      "269/269 [==============================] - 0s 125us/sample - loss: 0.1777 - val_loss: 0.1708\n",
      "Epoch 29/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.1779 - val_loss: 0.1599\n",
      "Epoch 30/300\n",
      "269/269 [==============================] - 0s 125us/sample - loss: 0.1646 - val_loss: 0.1626\n",
      "Epoch 31/300\n",
      "269/269 [==============================] - 0s 138us/sample - loss: 0.1668 - val_loss: 0.1678\n",
      "Epoch 32/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.1733 - val_loss: 0.1993\n",
      "Epoch 33/300\n",
      "269/269 [==============================] - 0s 126us/sample - loss: 0.1689 - val_loss: 0.1421\n",
      "Epoch 34/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.1390 - val_loss: 0.1165\n",
      "Epoch 35/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.1208 - val_loss: 0.1082\n",
      "Epoch 36/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.1102 - val_loss: 0.0944\n",
      "Epoch 37/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0983 - val_loss: 0.0839\n",
      "Epoch 38/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0910 - val_loss: 0.0949\n",
      "Epoch 39/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0867 - val_loss: 0.0735\n",
      "Epoch 40/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0785 - val_loss: 0.0575\n",
      "Epoch 41/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0605 - val_loss: 0.0471\n",
      "Epoch 42/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0588 - val_loss: 0.0449\n",
      "Epoch 43/300\n",
      "269/269 [==============================] - 0s 116us/sample - loss: 0.0518 - val_loss: 0.0449\n",
      "Epoch 44/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0490 - val_loss: 0.0339\n",
      "Epoch 45/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0394 - val_loss: 0.0295\n",
      "Epoch 46/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0306 - val_loss: 0.0288\n",
      "Epoch 47/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0392 - val_loss: 0.0424\n",
      "Epoch 48/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0566 - val_loss: 0.0445\n",
      "Epoch 49/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0374 - val_loss: 0.0373\n",
      "Epoch 50/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0316 - val_loss: 0.0342\n",
      "Epoch 51/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0301 - val_loss: 0.0284\n",
      "Epoch 52/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0330 - val_loss: 0.0351\n",
      "Epoch 53/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0325 - val_loss: 0.0385\n",
      "Epoch 54/300\n",
      "269/269 [==============================] - 0s 128us/sample - loss: 0.0324 - val_loss: 0.0268\n",
      "Epoch 55/300\n",
      "269/269 [==============================] - 0s 126us/sample - loss: 0.0323 - val_loss: 0.0271\n",
      "Epoch 56/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.0255 - val_loss: 0.0220\n",
      "Epoch 57/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0292 - val_loss: 0.0198\n",
      "Epoch 58/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0220 - val_loss: 0.0204\n",
      "Epoch 59/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0269 - val_loss: 0.0213\n",
      "Epoch 60/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0296 - val_loss: 0.0199\n",
      "Epoch 61/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0284 - val_loss: 0.0438\n",
      "Epoch 62/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0421 - val_loss: 0.0672\n",
      "Epoch 63/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0396 - val_loss: 0.0319\n",
      "Epoch 64/300\n",
      "269/269 [==============================] - 0s 125us/sample - loss: 0.0284 - val_loss: 0.0268\n",
      "Epoch 65/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0367 - val_loss: 0.0443\n",
      "Epoch 66/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0372 - val_loss: 0.0371\n",
      "Epoch 67/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0356 - val_loss: 0.0412\n",
      "Epoch 68/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0295 - val_loss: 0.0216\n",
      "Epoch 69/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0341 - val_loss: 0.0344\n",
      "Epoch 70/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0276 - val_loss: 0.0272\n",
      "Epoch 71/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0287 - val_loss: 0.0386\n",
      "Epoch 72/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0381 - val_loss: 0.0204\n",
      "Epoch 73/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0308 - val_loss: 0.0259\n",
      "Epoch 74/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0306 - val_loss: 0.0568\n",
      "Epoch 75/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0432 - val_loss: 0.0328\n",
      "Epoch 76/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0244 - val_loss: 0.0214\n",
      "Epoch 77/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0245 - val_loss: 0.0276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0263 - val_loss: 0.0323\n",
      "Epoch 79/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 80/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0203 - val_loss: 0.0301\n",
      "Epoch 81/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0232 - val_loss: 0.0166\n",
      "Epoch 82/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0190 - val_loss: 0.0257\n",
      "Epoch 83/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0274 - val_loss: 0.0208\n",
      "Epoch 84/300\n",
      "269/269 [==============================] - 0s 101us/sample - loss: 0.0262 - val_loss: 0.0208\n",
      "Epoch 85/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0415 - val_loss: 0.0457\n",
      "Epoch 86/300\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.043 - 0s 102us/sample - loss: 0.0291 - val_loss: 0.0365\n",
      "Epoch 87/300\n",
      "269/269 [==============================] - 0s 125us/sample - loss: 0.0309 - val_loss: 0.0405\n",
      "Epoch 88/300\n",
      "269/269 [==============================] - 0s 128us/sample - loss: 0.0407 - val_loss: 0.0185\n",
      "Epoch 89/300\n",
      "269/269 [==============================] - 0s 134us/sample - loss: 0.0200 - val_loss: 0.0163\n",
      "Epoch 90/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0173 - val_loss: 0.0209\n",
      "Epoch 91/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0243 - val_loss: 0.0372\n",
      "Epoch 92/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0384 - val_loss: 0.0287\n",
      "Epoch 93/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0400 - val_loss: 0.0239\n",
      "Epoch 94/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0205 - val_loss: 0.0319\n",
      "Epoch 95/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0280 - val_loss: 0.0363\n",
      "Epoch 96/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0398 - val_loss: 0.0196\n",
      "Epoch 97/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0233 - val_loss: 0.0157\n",
      "Epoch 98/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0191 - val_loss: 0.0239\n",
      "Epoch 99/300\n",
      "269/269 [==============================] - 0s 132us/sample - loss: 0.0235 - val_loss: 0.0197\n",
      "Epoch 100/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0168 - val_loss: 0.0124\n",
      "Epoch 101/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0178 - val_loss: 0.0268\n",
      "Epoch 102/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0219 - val_loss: 0.0198\n",
      "Epoch 103/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0224 - val_loss: 0.0354\n",
      "Epoch 104/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0281 - val_loss: 0.0221\n",
      "Epoch 105/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0362 - val_loss: 0.0328\n",
      "Epoch 106/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0255 - val_loss: 0.0424\n",
      "Epoch 107/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0317 - val_loss: 0.0245\n",
      "Epoch 108/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0183 - val_loss: 0.0141\n",
      "Epoch 109/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0153 - val_loss: 0.0169\n",
      "Epoch 110/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0191 - val_loss: 0.0156\n",
      "Epoch 111/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0187 - val_loss: 0.0300\n",
      "Epoch 112/300\n",
      "269/269 [==============================] - 0s 134us/sample - loss: 0.0210 - val_loss: 0.0216\n",
      "Epoch 113/300\n",
      "269/269 [==============================] - 0s 143us/sample - loss: 0.0172 - val_loss: 0.0243\n",
      "Epoch 114/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0235 - val_loss: 0.0201\n",
      "Epoch 115/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0219 - val_loss: 0.0197\n",
      "Epoch 116/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0208 - val_loss: 0.0168\n",
      "Epoch 117/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.0137 - val_loss: 0.0102\n",
      "Epoch 118/300\n",
      "269/269 [==============================] - 0s 149us/sample - loss: 0.0141 - val_loss: 0.0189\n",
      "Epoch 119/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0177 - val_loss: 0.0156\n",
      "Epoch 120/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0139 - val_loss: 0.0161\n",
      "Epoch 121/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0148 - val_loss: 0.0152\n",
      "Epoch 122/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0139 - val_loss: 0.0125\n",
      "Epoch 123/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0150 - val_loss: 0.0142\n",
      "Epoch 124/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0167 - val_loss: 0.0161\n",
      "Epoch 125/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0172 - val_loss: 0.0208\n",
      "Epoch 126/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0176 - val_loss: 0.0175\n",
      "Epoch 127/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0215 - val_loss: 0.0259\n",
      "Epoch 128/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0235 - val_loss: 0.0192\n",
      "Epoch 129/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0208 - val_loss: 0.0222\n",
      "Epoch 130/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0231 - val_loss: 0.0178\n",
      "Epoch 131/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0206 - val_loss: 0.0257\n",
      "Epoch 132/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0237 - val_loss: 0.0194\n",
      "Epoch 133/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0168 - val_loss: 0.0157\n",
      "Epoch 134/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0185 - val_loss: 0.0236\n",
      "Epoch 135/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 136/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0138 - val_loss: 0.0124\n",
      "Epoch 137/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0137 - val_loss: 0.0148\n",
      "Epoch 138/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0146 - val_loss: 0.0144\n",
      "Epoch 139/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0157 - val_loss: 0.0255\n",
      "Epoch 140/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0218 - val_loss: 0.0149\n",
      "Epoch 141/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0152 - val_loss: 0.0178\n",
      "Epoch 142/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0178 - val_loss: 0.0253\n",
      "Epoch 143/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0213 - val_loss: 0.0114\n",
      "Epoch 144/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.0215 - val_loss: 0.0180\n",
      "Epoch 145/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0247 - val_loss: 0.0218\n",
      "Epoch 146/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0222 - val_loss: 0.0131\n",
      "Epoch 147/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0178 - val_loss: 0.0197\n",
      "Epoch 148/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0197 - val_loss: 0.0207\n",
      "Epoch 149/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0219 - val_loss: 0.0233\n",
      "Epoch 150/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0166 - val_loss: 0.0107\n",
      "Epoch 151/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0134 - val_loss: 0.0103\n",
      "Epoch 152/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0135 - val_loss: 0.0172\n",
      "Epoch 153/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0144 - val_loss: 0.0113\n",
      "Epoch 154/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0123 - val_loss: 0.0132\n",
      "Epoch 155/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0145 - val_loss: 0.0216\n",
      "Epoch 156/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0167 - val_loss: 0.0154\n",
      "Epoch 157/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0131 - val_loss: 0.0112\n",
      "Epoch 158/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0139 - val_loss: 0.0137\n",
      "Epoch 159/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0134 - val_loss: 0.0105\n",
      "Epoch 160/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0124 - val_loss: 0.0274\n",
      "Epoch 161/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0253 - val_loss: 0.0303\n",
      "Epoch 162/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0203 - val_loss: 0.0225\n",
      "Epoch 163/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0191 - val_loss: 0.0255\n",
      "Epoch 164/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0246 - val_loss: 0.0213\n",
      "Epoch 165/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0159 - val_loss: 0.0177\n",
      "Epoch 166/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0153 - val_loss: 0.0136\n",
      "Epoch 167/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0155 - val_loss: 0.0197\n",
      "Epoch 168/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0163 - val_loss: 0.0163\n",
      "Epoch 169/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0175 - val_loss: 0.0218\n",
      "Epoch 170/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0209 - val_loss: 0.0135\n",
      "Epoch 171/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0193 - val_loss: 0.0148\n",
      "Epoch 172/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0245 - val_loss: 0.0197\n",
      "Epoch 173/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0179 - val_loss: 0.0146\n",
      "Epoch 174/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0187 - val_loss: 0.0137\n",
      "Epoch 175/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0168 - val_loss: 0.0157\n",
      "Epoch 176/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0149 - val_loss: 0.0126\n",
      "Epoch 177/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0146 - val_loss: 0.0174\n",
      "Epoch 178/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0192 - val_loss: 0.0193\n",
      "Epoch 179/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0214 - val_loss: 0.0304\n",
      "Epoch 180/300\n",
      "269/269 [==============================] - 0s 109us/sample - loss: 0.0236 - val_loss: 0.0254\n",
      "Epoch 181/300\n",
      "269/269 [==============================] - 0s 125us/sample - loss: 0.0276 - val_loss: 0.0112\n",
      "Epoch 182/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0216 - val_loss: 0.0166\n",
      "Epoch 183/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0145 - val_loss: 0.0126\n",
      "Epoch 184/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0182 - val_loss: 0.0161\n",
      "Epoch 185/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0196 - val_loss: 0.0188\n",
      "Epoch 186/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0209 - val_loss: 0.0171\n",
      "Epoch 187/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0223 - val_loss: 0.0204\n",
      "Epoch 188/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0174 - val_loss: 0.0196\n",
      "Epoch 189/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0185 - val_loss: 0.0151\n",
      "Epoch 190/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0230 - val_loss: 0.0154\n",
      "Epoch 191/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0175 - val_loss: 0.0109\n",
      "Epoch 192/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0181 - val_loss: 0.0125\n",
      "Epoch 193/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0183 - val_loss: 0.0122\n",
      "Epoch 194/300\n",
      "269/269 [==============================] - 0s 132us/sample - loss: 0.0148 - val_loss: 0.0134\n",
      "Epoch 195/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0133 - val_loss: 0.0148\n",
      "Epoch 196/300\n",
      "269/269 [==============================] - 0s 130us/sample - loss: 0.0136 - val_loss: 0.0114\n",
      "Epoch 197/300\n",
      "269/269 [==============================] - 0s 132us/sample - loss: 0.0150 - val_loss: 0.0241\n",
      "Epoch 198/300\n",
      "269/269 [==============================] - 0s 130us/sample - loss: 0.0216 - val_loss: 0.0119\n",
      "Epoch 199/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0145 - val_loss: 0.0104\n",
      "Epoch 200/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0145 - val_loss: 0.0159\n",
      "Epoch 201/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0158 - val_loss: 0.0221\n",
      "Epoch 202/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0204 - val_loss: 0.0254\n",
      "Epoch 203/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0234 - val_loss: 0.0189\n",
      "Epoch 204/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0156 - val_loss: 0.0152\n",
      "Epoch 205/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0137 - val_loss: 0.0119\n",
      "Epoch 206/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0137 - val_loss: 0.0186\n",
      "Epoch 207/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0151 - val_loss: 0.0162\n",
      "Epoch 208/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0176 - val_loss: 0.0185\n",
      "Epoch 209/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0161 - val_loss: 0.0218\n",
      "Epoch 210/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0164 - val_loss: 0.0167\n",
      "Epoch 211/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0164 - val_loss: 0.0141\n",
      "Epoch 212/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0202 - val_loss: 0.0239\n",
      "Epoch 213/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0200 - val_loss: 0.0122\n",
      "Epoch 214/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0127 - val_loss: 0.0143\n",
      "Epoch 215/300\n",
      "269/269 [==============================] - 0s 125us/sample - loss: 0.0131 - val_loss: 0.0107\n",
      "Epoch 216/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0133 - val_loss: 0.0127\n",
      "Epoch 217/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0151 - val_loss: 0.0197\n",
      "Epoch 218/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0181 - val_loss: 0.0203\n",
      "Epoch 219/300\n",
      "269/269 [==============================] - 0s 126us/sample - loss: 0.0181 - val_loss: 0.0158\n",
      "Epoch 220/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0167 - val_loss: 0.0252\n",
      "Epoch 221/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0180 - val_loss: 0.0151\n",
      "Epoch 222/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0176 - val_loss: 0.0135\n",
      "Epoch 223/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0153 - val_loss: 0.0148\n",
      "Epoch 224/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0161 - val_loss: 0.0202\n",
      "Epoch 225/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0188 - val_loss: 0.0260\n",
      "Epoch 226/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0225 - val_loss: 0.0208\n",
      "Epoch 227/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0218 - val_loss: 0.0250\n",
      "Epoch 228/300\n",
      "269/269 [==============================] - 0s 126us/sample - loss: 0.0169 - val_loss: 0.0133\n",
      "Epoch 229/300\n",
      "269/269 [==============================] - 0s 136us/sample - loss: 0.0168 - val_loss: 0.0180\n",
      "Epoch 230/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0151 - val_loss: 0.0136\n",
      "Epoch 231/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0167 - val_loss: 0.0172\n",
      "Epoch 232/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0143 - val_loss: 0.0146\n",
      "Epoch 233/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0124 - val_loss: 0.0111\n",
      "Epoch 234/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0126 - val_loss: 0.0199\n",
      "Epoch 235/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0220 - val_loss: 0.0207\n",
      "Epoch 236/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0143 - val_loss: 0.0137\n",
      "Epoch 237/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0172 - val_loss: 0.0183\n",
      "Epoch 238/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0199 - val_loss: 0.0189\n",
      "Epoch 239/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0228 - val_loss: 0.0326\n",
      "Epoch 240/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0268 - val_loss: 0.0382\n",
      "Epoch 241/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0378 - val_loss: 0.0225\n",
      "Epoch 242/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0222 - val_loss: 0.0150\n",
      "Epoch 243/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.0215 - val_loss: 0.0369\n",
      "Epoch 244/300\n",
      "269/269 [==============================] - 0s 125us/sample - loss: 0.0311 - val_loss: 0.0155\n",
      "Epoch 245/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.0213 - val_loss: 0.0270\n",
      "Epoch 246/300\n",
      "269/269 [==============================] - 0s 126us/sample - loss: 0.0216 - val_loss: 0.0226\n",
      "Epoch 247/300\n",
      "269/269 [==============================] - 0s 105us/sample - loss: 0.0214 - val_loss: 0.0213\n",
      "Epoch 248/300\n",
      "269/269 [==============================] - 0s 126us/sample - loss: 0.0184 - val_loss: 0.0203\n",
      "Epoch 249/300\n",
      "269/269 [==============================] - 0s 141us/sample - loss: 0.0184 - val_loss: 0.0194\n",
      "Epoch 250/300\n",
      "269/269 [==============================] - 0s 152us/sample - loss: 0.0139 - val_loss: 0.0102\n",
      "Epoch 251/300\n",
      "269/269 [==============================] - 0s 141us/sample - loss: 0.0126 - val_loss: 0.0151\n",
      "Epoch 252/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0163 - val_loss: 0.0220\n",
      "Epoch 253/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0197 - val_loss: 0.0174\n",
      "Epoch 254/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0163 - val_loss: 0.0134\n",
      "Epoch 255/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0149 - val_loss: 0.0124\n",
      "Epoch 256/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0131 - val_loss: 0.0121\n",
      "Epoch 257/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0130 - val_loss: 0.0162\n",
      "Epoch 258/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0143 - val_loss: 0.0161\n",
      "Epoch 259/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0141 - val_loss: 0.0234\n",
      "Epoch 260/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0235 - val_loss: 0.0238\n",
      "Epoch 261/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0202 - val_loss: 0.0168\n",
      "Epoch 262/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0160 - val_loss: 0.0152\n",
      "Epoch 263/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0169 - val_loss: 0.0153\n",
      "Epoch 264/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0155 - val_loss: 0.0146\n",
      "Epoch 265/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0149 - val_loss: 0.0132\n",
      "Epoch 266/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0165 - val_loss: 0.0136\n",
      "Epoch 267/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0139 - val_loss: 0.0128\n",
      "Epoch 268/300\n",
      "269/269 [==============================] - 0s 126us/sample - loss: 0.0138 - val_loss: 0.0129\n",
      "Epoch 269/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0170 - val_loss: 0.0113\n",
      "Epoch 270/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0139 - val_loss: 0.0149\n",
      "Epoch 271/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0151 - val_loss: 0.0133\n",
      "Epoch 272/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0144 - val_loss: 0.0133\n",
      "Epoch 273/300\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.014 - 0s 106us/sample - loss: 0.0128 - val_loss: 0.0118\n",
      "Epoch 274/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0128 - val_loss: 0.0120\n",
      "Epoch 275/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0120 - val_loss: 0.0127\n",
      "Epoch 276/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.0138 - val_loss: 0.0115\n",
      "Epoch 277/300\n",
      "269/269 [==============================] - 0s 143us/sample - loss: 0.0147 - val_loss: 0.0152\n",
      "Epoch 278/300\n",
      "269/269 [==============================] - 0s 145us/sample - loss: 0.0177 - val_loss: 0.0150\n",
      "Epoch 279/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0147 - val_loss: 0.0155\n",
      "Epoch 280/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0135 - val_loss: 0.0163\n",
      "Epoch 281/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0168 - val_loss: 0.0127\n",
      "Epoch 282/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0155 - val_loss: 0.0173\n",
      "Epoch 283/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0157 - val_loss: 0.0228\n",
      "Epoch 284/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0224 - val_loss: 0.0124\n",
      "Epoch 285/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0185 - val_loss: 0.0190\n",
      "Epoch 286/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0162 - val_loss: 0.0143\n",
      "Epoch 287/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0170 - val_loss: 0.0139\n",
      "Epoch 288/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0162 - val_loss: 0.0134\n",
      "Epoch 289/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0163 - val_loss: 0.0136\n",
      "Epoch 290/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0158 - val_loss: 0.0215\n",
      "Epoch 291/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0187 - val_loss: 0.0107\n",
      "Epoch 292/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0139 - val_loss: 0.0239\n",
      "Epoch 293/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0194 - val_loss: 0.0171\n",
      "Epoch 294/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0219 - val_loss: 0.0161\n",
      "Epoch 295/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0147 - val_loss: 0.0179\n",
      "Epoch 296/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0168 - val_loss: 0.0202\n",
      "Epoch 297/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0146 - val_loss: 0.0147\n",
      "Epoch 298/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0155 - val_loss: 0.0174\n",
      "Epoch 299/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0152 - val_loss: 0.0177\n",
      "Epoch 300/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0148 - val_loss: 0.0168\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=300,\n",
    "                    \n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4114417 , 0.48735207, 0.61332107, 0.6514602 , 0.7352706 ,\n",
       "        0.82840574, 0.8609055 , 0.9938454 , 1.0638919 ],\n",
       "       [0.8470335 , 1.0021744 , 1.2437438 , 1.3201789 , 1.4834784 ,\n",
       "        1.6747763 , 1.7289132 , 2.0042746 , 2.1307094 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrite to multi-output for each quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes the following order of statistical distribution parameters for\n",
    "# the neurons in the layer before this custom layer:\n",
    "# x[0] = mu\n",
    "# x[1] = sigma\n",
    "# x[2] = skewness\n",
    "# x[3] = kurtosis\n",
    "def DistributionLayer(quantile_index,x):\n",
    "    mu = x[0]\n",
    "    sigma = x[1] \n",
    "    skewness = None\n",
    "    kurtosis = None\n",
    "    \n",
    "    # absorb extra statistical distribution parameters if present\n",
    "    if (len(x.get_shape().as_list())>2):\n",
    "        skewness= x[2]\n",
    "    if (len(x.get_shape().as_list())>3):\n",
    "        kurtosis = x[3]\n",
    "        \n",
    "    if (skewness==None):\n",
    "        if (kurtosis==None):\n",
    "            # Source of Z-scores: https://www.wolframalpha.com/input/?i=percentiles+of+a+normal+distribution\n",
    "             return {\n",
    "                0.005: mu-2.57583*sigma, # https://www.wolframalpha.com/input/?i=0.5+percentiles+of+a+normal+distribution\n",
    "                0.025: mu-1.95996*sigma, # https://www.wolframalpha.com/input/?i=2.5+percentiles+of+a+normal+distribution\n",
    "                0.165: mu-0.974114*sigma, # https://www.wolframalpha.com/input/?i=16.5+percentiles+of+a+normal+distribution\n",
    "                0.25: mu-0.674*sigma, # https://www.wolframalpha.com/input/?i=25+percentiles+of+a+normal+distribution\n",
    "                0.5: mu, # https://www.wolframalpha.com/input/?i=50+percentiles+of+a+normal+distribution\n",
    "                0.75: mu+0.674*sigma, # https://www.wolframalpha.com/input/?i=75+percentiles+of+a+normal+distribution\n",
    "                0.835: mu+0.9741114*sigma, #https://www.wolframalpha.com/input/?i=83.5+percentiles+of+a+normal+distribution\n",
    "                0.975: mu+1.95996*sigma, #https://www.wolframalpha.com/input/?i=97.5+percentiles+of+a+normal+distribution\n",
    "                0.995: mu+2.57583*sigma, #https://www.wolframalpha.com/input/?i=99.5+percentiles+of+a+normal+distribution\n",
    "            }[quantile]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(inp_shape, quantiles):\n",
    "    # clear previous sessions\n",
    "    K.clear_session()\n",
    "\n",
    "    inp = Input(inp_shape, name=\"input\")\n",
    "    x = inp\n",
    "    x = Dense(16)(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = Dense(2)(x)  # represents mu, sigma\n",
    "    \n",
    "    #out_q0 = Dense(1, name=\"q0\")(x)  # DistributionLayer(quantile=quantiles[0])(x)\n",
    "    out_q0 = DistributionLayer(quantiles[0],x)\n",
    "    out_q1 = DistributionLayer(quantiles[1],x)\n",
    "    out_q2 = DistributionLayer(quantiles[2],x)\n",
    "    out_q3 = DistributionLayer(quantiles[3],x)\n",
    "    out_q4 = DistributionLayer(quantiles[4],x)\n",
    "    out_q5 = DistributionLayer(quantiles[5],x)\n",
    "    out_q6 = DistributionLayer(quantiles[6],x)\n",
    "    out_q7 = DistributionLayer(quantiles[7],x)\n",
    "    out_q8 = DistributionLayer(quantiles[8],x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=[out_q0, out_q1, out_q2, out_q3, out_q4, out_q5, out_q6, out_q7, out_q8])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           176         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           544         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           2112        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            130         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(2,)]               0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [(2,)]               0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [(2,)]               0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [(2,)]               0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [(2,)]               0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [(2,)]               0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [(2,)]               0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [(2,)]               0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [(2,)]               0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(2,)]               0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul (TensorFlowOpLa [(2,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(2,)]               0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_8 (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(2,)]               0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_16 (TensorFlowO [(2,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(2,)]               0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_24 (TensorFlowO [(2,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(2,)]               0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_32 (TensorFlowO [(2,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(2,)]               0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_40 (TensorFlowO [(2,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(2,)]               0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_48 (TensorFlowO [(2,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(2,)]               0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_56 (TensorFlowO [(2,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(2,)]               0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_64 (TensorFlowO [(2,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub (TensorFlowOpLa [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_mul[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_4 (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_mul_8[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_8 (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_mul_16[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_12 (TensorFlowO [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_mul_24[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_16 (TensorFlowO [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_mul_32[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_20 (TensorFlowO [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_mul_40[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_24 (TensorFlowO [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_mul_48[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_28 (TensorFlowO [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_mul_56[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_32 (TensorFlowO [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_mul_64[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 2,962\n",
      "Trainable params: 2,962\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(inp_shape=(train_df.columns.size,), quantiles=quantiles)\n",
    "model.compile(optimizer=\"adam\", loss=\"MAE\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q0': array([0.77976871, 0.4255634 , 0.41740025, 0.42903284, 0.82754025,\n",
       "        0.4064456 , 0.40616237, 0.45264247, 0.41096554, 0.41689168,\n",
       "        0.38520678, 0.41300217, 0.40747019, 0.86864485, 0.43559714,\n",
       "        0.42313449, 0.41549744, 0.43160425, 0.4241328 , 0.39397524,\n",
       "        0.45516087, 0.92357664, 0.41138681, 0.4042906 , 0.84447597,\n",
       "        0.42392267, 0.7718635 , 0.44973176, 0.43213789, 0.84063285,\n",
       "        0.82372458, 0.78483486, 0.41210167, 0.43546165, 0.92956534,\n",
       "        0.44674431, 0.40117869, 0.44582427, 0.83591121, 0.87259832,\n",
       "        0.90353443, 0.42208748, 0.38899553, 0.45153601, 0.39837333,\n",
       "        0.41525051, 0.42364229, 0.86586664, 0.40607733, 0.41816087,\n",
       "        0.38255025, 0.89064516, 0.42218558, 0.42119283, 0.40201799,\n",
       "        0.90192745, 0.40566299, 0.42338319, 0.41504695, 0.43416054,\n",
       "        0.40994688, 0.41956467, 0.86222354, 0.40953107, 0.87499344,\n",
       "        0.43268808, 0.40363463, 0.43650302, 0.41236164, 0.91755915,\n",
       "        0.84431159, 0.79940419, 0.41597326, 0.41392266, 0.42782747,\n",
       "        0.39393664, 0.89695079, 0.86224126, 0.43617093, 0.85140217,\n",
       "        0.42278976, 0.39106672, 0.41890434, 0.39874521, 0.41975423,\n",
       "        0.89906273, 0.45000559, 0.40947942, 0.44329988, 0.42845406,\n",
       "        0.432556  , 0.39379119, 0.40501631, 0.41589453, 0.78193413,\n",
       "        0.42418178, 0.40707033, 0.41339733, 0.43810726, 0.41006597,\n",
       "        0.43851828, 0.85018333, 0.42148472, 0.43332995, 0.83816669,\n",
       "        0.4207673 , 0.90264225, 0.44033269, 0.80576037, 0.84932577,\n",
       "        0.43348768, 0.83992371, 0.39006657, 0.88018906, 0.43101839,\n",
       "        0.44261483, 0.83254959, 0.40761605, 0.42398747, 0.39113956,\n",
       "        0.79373536, 0.4129318 , 0.90506873, 0.84718589, 0.85837079,\n",
       "        0.83941267, 0.88721045, 0.4195636 , 0.42338891, 0.44482469,\n",
       "        0.44546948, 0.43602464, 0.44134388, 0.89297545, 0.83983143,\n",
       "        0.4293607 , 0.43347221, 0.41334255, 0.84876859, 0.43941201,\n",
       "        0.44940611, 0.41955864, 0.85634571, 0.42355538, 0.43945227,\n",
       "        0.41727819, 0.42498001, 0.42266657, 0.42266616, 0.41148532,\n",
       "        0.88900356, 0.83327412, 0.85083557, 0.36450726, 0.42908377,\n",
       "        0.8729209 , 0.84048098, 0.40453602, 0.81843599, 0.85376998,\n",
       "        0.41641431, 0.41676266, 0.87021769, 0.41394461, 0.42832182,\n",
       "        0.41310186, 0.41093871, 0.85118556, 0.45711055, 0.41131593,\n",
       "        0.42535109, 0.42494805, 0.42291034, 0.44941366, 0.39662125,\n",
       "        0.41934535, 0.43803166, 0.42816385, 0.44481512, 0.40592902,\n",
       "        0.44510568, 0.87551584, 0.43882955, 0.40980793, 0.39726162,\n",
       "        0.83865186, 0.82391532, 0.43606232, 0.903867  , 0.42598936,\n",
       "        0.39335341, 0.87932989, 0.88167455, 0.92069727, 0.88504133,\n",
       "        0.41160328, 0.82383574, 0.41669152, 0.8902058 , 0.86226759,\n",
       "        0.42570417, 0.42333129, 0.41146715, 0.40853873, 0.87497931,\n",
       "        0.41888769, 0.4174933 , 0.40137973, 0.41868108, 0.41480139,\n",
       "        0.41762532, 0.43417065, 0.83195903, 0.42012424, 0.88049122,\n",
       "        0.43808452, 0.43475285, 0.37609209, 0.83488053, 0.39741986,\n",
       "        0.45581282, 0.87479406, 0.41141341, 0.41079624, 0.43100557,\n",
       "        0.37878448, 0.40052723, 0.41471736, 0.41684838, 0.43305646,\n",
       "        0.37894753, 0.4310462 , 0.88510673, 0.91283571, 0.90124721,\n",
       "        0.39263573, 0.41654587, 0.8974081 , 0.42264731, 0.42741047,\n",
       "        0.42512897, 0.42810603, 0.81919692, 0.84746617, 0.42715475,\n",
       "        0.39471582, 0.41054117, 0.41730352, 0.41887935, 0.41252063,\n",
       "        0.39940721, 0.90601885, 0.44086474, 0.42635516, 0.41635982,\n",
       "        0.89206673, 0.4321597 , 0.38922181, 0.81370253, 0.41180009,\n",
       "        0.42916062, 0.8504008 , 0.43751243, 0.45314441, 0.43264675,\n",
       "        0.81895532, 0.83909076, 0.3941779 , 0.41131961]),\n",
       " 'q1': array([0.98694101, 0.49009461, 0.49216885, 0.49557415, 1.01162224,\n",
       "        0.50957932, 0.49744197, 0.51033935, 0.48771643, 0.49897617,\n",
       "        0.48736895, 0.48874831, 0.49512264, 1.01446794, 0.49360654,\n",
       "        0.5011236 , 0.49018645, 0.48991859, 0.50036596, 0.49762297,\n",
       "        0.49253734, 1.05807856, 0.50273478, 0.48129021, 0.99869867,\n",
       "        0.49927749, 1.00492484, 0.50894352, 0.50124206, 1.00702438,\n",
       "        1.0056577 , 0.98704648, 0.47556742, 0.50901135, 1.03540212,\n",
       "        0.49528869, 0.49726181, 0.50423386, 1.01871513, 1.00822702,\n",
       "        1.00746122, 0.48079287, 0.48451008, 0.51382279, 0.50449866,\n",
       "        0.49952134, 0.4781263 , 1.02966442, 0.47676185, 0.50073502,\n",
       "        0.48975022, 1.02514877, 0.47741895, 0.50557659, 0.47679472,\n",
       "        1.02220408, 0.48937788, 0.51426022, 0.49339485, 0.50352995,\n",
       "        0.47363033, 0.48989425, 1.00843135, 0.50756214, 1.02545345,\n",
       "        0.51216801, 0.48720753, 0.50620415, 0.49156689, 1.02284072,\n",
       "        1.00312264, 0.98599412, 0.49582001, 0.48670795, 0.49725651,\n",
       "        0.51115274, 1.0135584 , 1.00080876, 0.51531796, 1.03367328,\n",
       "        0.49577904, 0.47843734, 0.48073988, 0.48792381, 0.47876208,\n",
       "        1.05745382, 0.52552364, 0.49381206, 0.50296196, 0.5092961 ,\n",
       "        0.49874424, 0.49395363, 0.50255466, 0.50275193, 0.96459871,\n",
       "        0.48562709, 0.47127788, 0.49053995, 0.49197004, 0.48347861,\n",
       "        0.51034104, 0.9515071 , 0.49285103, 0.50164632, 0.9912556 ,\n",
       "        0.51201887, 1.02047253, 0.50266385, 1.00514551, 1.02315231,\n",
       "        0.50567869, 0.98625531, 0.48620697, 0.98859376, 0.49709315,\n",
       "        0.49514335, 0.9951382 , 0.49400333, 0.50803919, 0.45791431,\n",
       "        0.99710541, 0.50437562, 1.00353863, 1.01851988, 1.01320094,\n",
       "        1.03688206, 1.01064562, 0.4876326 , 0.49487272, 0.49730388,\n",
       "        0.49577792, 0.50022169, 0.50017166, 1.02817656, 1.01071715,\n",
       "        0.4923242 , 0.50834872, 0.50602337, 1.01779662, 0.51028976,\n",
       "        0.50552597, 0.4913838 , 0.99181463, 0.48212265, 0.5041746 ,\n",
       "        0.50426376, 0.49784764, 0.50485703, 0.50799094, 0.48854629,\n",
       "        1.00389055, 1.02390523, 0.99702205, 0.49905009, 0.50058671,\n",
       "        0.988371  , 0.98308459, 0.49724383, 0.9584036 , 1.04166762,\n",
       "        0.49458875, 0.4937687 , 1.01964101, 0.49731282, 0.50025734,\n",
       "        0.49637799, 0.48975453, 1.01972863, 0.49900831, 0.51659863,\n",
       "        0.49381212, 0.51276073, 0.49783404, 0.49254733, 0.48678729,\n",
       "        0.48216814, 0.49518275, 0.50301859, 0.50641068, 0.48711808,\n",
       "        0.50619097, 1.00885207, 0.51158332, 0.493986  , 0.49348383,\n",
       "        0.99602502, 0.99890815, 0.5113605 , 1.02705827, 0.49046472,\n",
       "        0.48983167, 1.04348895, 1.0209626 , 1.04413318, 1.01637382,\n",
       "        0.49396519, 1.00992583, 0.48080134, 1.03971773, 1.0195359 ,\n",
       "        0.50181136, 0.50765927, 0.48838422, 0.48219274, 1.03215824,\n",
       "        0.50128541, 0.50672073, 0.48209268, 0.49500284, 0.50020777,\n",
       "        0.50038114, 0.50654676, 1.00557768, 0.49480572, 0.98793313,\n",
       "        0.50006588, 0.49145004, 0.4907908 , 1.03259482, 0.50191482,\n",
       "        0.50186779, 1.00173852, 0.49176462, 0.49286449, 0.49103959,\n",
       "        0.454028  , 0.48626906, 0.50566403, 0.49470059, 0.49883381,\n",
       "        0.48859335, 0.49235676, 0.99662842, 1.00860885, 1.01120466,\n",
       "        0.50197417, 0.47646561, 1.00163566, 0.49193431, 0.4881685 ,\n",
       "        0.48211356, 0.51494388, 1.00079741, 1.03201598, 0.50704743,\n",
       "        0.48413271, 0.4891214 , 0.50328069, 0.50908513, 0.49309843,\n",
       "        0.50374785, 1.01776805, 0.50391482, 0.4974368 , 0.48559929,\n",
       "        1.01745078, 0.49532064, 0.49530214, 1.00560392, 0.48604815,\n",
       "        0.5079594 , 1.03177231, 0.4954174 , 0.50915667, 0.49112741,\n",
       "        0.98531047, 1.030373  , 0.49810812, 0.49329876]),\n",
       " 'q2': array([1.23415944, 0.62403202, 0.62149103, 0.62300947, 1.26217082,\n",
       "        0.62762769, 0.62297613, 0.62618969, 0.61528915, 0.60759722,\n",
       "        0.62185884, 0.61635795, 0.61774114, 1.24545958, 0.62086249,\n",
       "        0.63097009, 0.62455689, 0.62501272, 0.62425812, 0.61958445,\n",
       "        0.62107551, 1.25297968, 0.63520761, 0.61876652, 1.24592338,\n",
       "        0.62841592, 1.26300544, 0.6191478 , 0.62087474, 1.24998631,\n",
       "        1.24402391, 1.25977008, 0.63122105, 0.62802461, 1.27008318,\n",
       "        0.63156619, 0.62309866, 0.61808352, 1.26380532, 1.25014126,\n",
       "        1.25052403, 0.61524934, 0.62261617, 0.62218547, 0.62446789,\n",
       "        0.62792731, 0.61535662, 1.26161977, 0.63268795, 0.62502217,\n",
       "        0.61358015, 1.25408356, 0.61417772, 0.6187807 , 0.62082149,\n",
       "        1.25556961, 0.62059392, 0.63016314, 0.61566804, 0.62620342,\n",
       "        0.62485769, 0.61284626, 1.25019815, 0.62844213, 1.24936877,\n",
       "        0.62590049, 0.61753697, 0.62757603, 0.62059209, 1.25260924,\n",
       "        1.25127986, 1.25972421, 0.61954638, 0.6219585 , 0.6251437 ,\n",
       "        0.61962757, 1.25075908, 1.26367542, 0.63655936, 1.26564917,\n",
       "        0.61661823, 0.61760989, 0.61921294, 0.62597898, 0.63110533,\n",
       "        1.28091955, 0.62643384, 0.61875223, 0.62327298, 0.63437493,\n",
       "        0.61764525, 0.62440675, 0.62818242, 0.61168596, 1.25598814,\n",
       "        0.62785221, 0.63263717, 0.6362003 , 0.62902647, 0.62689884,\n",
       "        0.63049301, 1.23807505, 0.62119081, 0.62525419, 1.24636912,\n",
       "        0.61818913, 1.25492035, 0.62320071, 1.24944229, 1.25072259,\n",
       "        0.63032966, 1.23606224, 0.6225696 , 1.24354047, 0.62096936,\n",
       "        0.61900264, 1.25995944, 0.62769148, 0.62463672, 0.61376801,\n",
       "        1.24298732, 0.62407361, 1.24711397, 1.26567477, 1.25588174,\n",
       "        1.25179153, 1.2545534 , 0.62665707, 0.63109202, 0.6241268 ,\n",
       "        0.61305931, 0.63568395, 0.61551026, 1.25921581, 1.24369283,\n",
       "        0.63489319, 0.63133156, 0.62466468, 1.2522283 , 0.62784165,\n",
       "        0.62538753, 0.6224903 , 1.24460801, 0.61645724, 0.6207517 ,\n",
       "        0.61960804, 0.61947945, 0.62512051, 0.62509865, 0.62387299,\n",
       "        1.24593917, 1.25183447, 1.25118721, 0.62087067, 0.62848196,\n",
       "        1.25838881, 1.24631658, 0.62873451, 1.23836776, 1.28015594,\n",
       "        0.62710721, 0.62954164, 1.2469327 , 0.62730045, 0.62263382,\n",
       "        0.62645677, 0.62458252, 1.25597435, 0.62666851, 0.62180764,\n",
       "        0.6237753 , 0.63216437, 0.63072   , 0.62137434, 0.61358608,\n",
       "        0.62738811, 0.62285963, 0.6182879 , 0.62145093, 0.61782083,\n",
       "        0.63708837, 1.24535217, 0.63060131, 0.61901192, 0.63018767,\n",
       "        1.25103069, 1.24506989, 0.63763215, 1.24530069, 0.62061272,\n",
       "        0.62626489, 1.26568293, 1.25222736, 1.27266938, 1.27394426,\n",
       "        0.62286346, 1.24133757, 0.62443488, 1.26655064, 1.24946391,\n",
       "        0.62046132, 0.63028102, 0.60989389, 0.62220374, 1.27361788,\n",
       "        0.62416329, 0.62252152, 0.61191324, 0.62704227, 0.62203599,\n",
       "        0.61010957, 0.62473649, 1.26372783, 0.62091589, 1.25416445,\n",
       "        0.62243683, 0.62823039, 0.62325411, 1.26540977, 0.63103205,\n",
       "        0.61854779, 1.25410241, 0.61453691, 0.61191762, 0.61421155,\n",
       "        0.62780954, 0.62538423, 0.63184065, 0.62216598, 0.63340651,\n",
       "        0.62276536, 0.63135018, 1.25964817, 1.25504678, 1.25336523,\n",
       "        0.6208399 , 0.62564658, 1.27201883, 0.62142732, 0.61582923,\n",
       "        0.62132782, 0.62427135, 1.24520786, 1.27288314, 0.63639293,\n",
       "        0.6201414 , 0.61925565, 0.63652999, 0.62532106, 0.61005924,\n",
       "        0.62440197, 1.26007469, 0.62045531, 0.62473529, 0.62820411,\n",
       "        1.25797305, 0.61559212, 0.62824833, 1.25897925, 0.6230901 ,\n",
       "        0.62864541, 1.25387655, 0.62733589, 0.62673391, 0.61007945,\n",
       "        1.23435803, 1.2576428 , 0.62595282, 0.63103304]),\n",
       " 'q3': array([1.31723581, 0.66033059, 0.6592753 , 0.66133424, 1.3320228 ,\n",
       "        0.66096545, 0.66778587, 0.66731533, 0.65420575, 0.65171159,\n",
       "        0.66240246, 0.65164507, 0.65922365, 1.32561766, 0.6677954 ,\n",
       "        0.66781545, 0.66213692, 0.66217674, 0.66500155, 0.66005076,\n",
       "        0.662469  , 1.32945328, 0.67289112, 0.65365688, 1.3248756 ,\n",
       "        0.66823926, 1.33438623, 0.66103369, 0.66224226, 1.32742574,\n",
       "        1.31016559, 1.33532641, 0.66370602, 0.66152799, 1.33202984,\n",
       "        0.66714988, 0.66181973, 0.65787594, 1.33929344, 1.32268346,\n",
       "        1.32921383, 0.66191331, 0.66761227, 0.66658659, 0.66963971,\n",
       "        0.66364068, 0.65155188, 1.33274491, 0.66466645, 0.66832897,\n",
       "        0.65979445, 1.33403622, 0.64902849, 0.65692324, 0.66415054,\n",
       "        1.32694221, 0.65699836, 0.66900737, 0.65501408, 0.6667991 ,\n",
       "        0.66460612, 0.65276886, 1.33644125, 0.66673362, 1.31697297,\n",
       "        0.6638489 , 0.65916382, 0.66533093, 0.66289015, 1.32582536,\n",
       "        1.3309913 , 1.32398213, 0.65848908, 0.65736769, 0.66822425,\n",
       "        0.65469048, 1.33712331, 1.34322155, 0.66833558, 1.34623098,\n",
       "        0.65422535, 0.65770452, 0.65885716, 0.66361448, 0.6726797 ,\n",
       "        1.34852505, 0.67024972, 0.66091273, 0.66475046, 0.66330258,\n",
       "        0.65851398, 0.66524878, 0.66746875, 0.65518494, 1.3307843 ,\n",
       "        0.66485312, 0.66801475, 0.67620529, 0.66060127, 0.66544112,\n",
       "        0.66273628, 1.32585813, 0.6576778 , 0.66715572, 1.32948766,\n",
       "        0.65810468, 1.3366208 , 0.66305498, 1.32771181, 1.32233661,\n",
       "        0.67460366, 1.31917846, 0.6563776 , 1.33086716, 0.66804441,\n",
       "        0.65868687, 1.33316089, 0.66264201, 0.66013637, 0.6545152 ,\n",
       "        1.30599382, 0.66380807, 1.33689595, 1.33920802, 1.33262933,\n",
       "        1.3330205 , 1.34104905, 0.66017325, 0.66974601, 0.65960514,\n",
       "        0.65511992, 0.66711553, 0.66115331, 1.34198528, 1.31341776,\n",
       "        0.66610277, 0.67507959, 0.66490458, 1.31449508, 0.66491295,\n",
       "        0.65974554, 0.66519467, 1.31906406, 0.65585645, 0.6584035 ,\n",
       "        0.66022654, 0.65510033, 0.66430143, 0.6632202 , 0.66658849,\n",
       "        1.31791209, 1.33595034, 1.34569084, 0.65889827, 0.66207737,\n",
       "        1.3341927 , 1.32643953, 0.66569416, 1.31515611, 1.34801759,\n",
       "        0.66101623, 0.66520537, 1.31971292, 0.66417882, 0.65952361,\n",
       "        0.66037153, 0.66578517, 1.34019329, 0.66350399, 0.66137706,\n",
       "        0.66806438, 0.6658318 , 0.67077618, 0.6613522 , 0.6546362 ,\n",
       "        0.67384116, 0.65473279, 0.65900636, 0.65504968, 0.66114336,\n",
       "        0.66902905, 1.32166958, 0.66940348, 0.65616147, 0.67037101,\n",
       "        1.32138349, 1.3215389 , 0.67487097, 1.32076563, 0.65212862,\n",
       "        0.66742075, 1.33618606, 1.32276201, 1.35912042, 1.35240622,\n",
       "        0.66038175, 1.31726581, 0.6686082 , 1.3206293 , 1.33427645,\n",
       "        0.65293161, 0.6672563 , 0.6546146 , 0.65407474, 1.34048064,\n",
       "        0.67135263, 0.66006081, 0.66097128, 0.66393153, 0.65635722,\n",
       "        0.65295342, 0.66215016, 1.33118645, 0.65886816, 1.31912821,\n",
       "        0.65940915, 0.66337067, 0.66303018, 1.34356549, 0.67124209,\n",
       "        0.65403181, 1.34054726, 0.65593613, 0.6507664 , 0.65117838,\n",
       "        0.67030755, 0.66736413, 0.67614335, 0.65581685, 0.66430211,\n",
       "        0.66369168, 0.66810155, 1.33207219, 1.32588064, 1.33476591,\n",
       "        0.66382865, 0.6610979 , 1.34380321, 0.66258363, 0.6487662 ,\n",
       "        0.66337552, 0.66072435, 1.31717271, 1.34536928, 0.66856589,\n",
       "        0.66457383, 0.66321706, 0.67634305, 0.6615422 , 0.64971732,\n",
       "        0.66153683, 1.34106452, 0.66314342, 0.66124236, 0.66651766,\n",
       "        1.34624637, 0.6603903 , 0.66258055, 1.34134286, 0.66270542,\n",
       "        0.66795638, 1.31890679, 0.66617373, 0.66740451, 0.64821642,\n",
       "        1.30644805, 1.3262954 , 0.66118063, 0.67123138]),\n",
       " 'q4': array([1.5100656 , 0.74822681, 0.74982045, 0.75453434, 1.48676496,\n",
       "        0.74195801, 0.74741724, 0.75372374, 0.73928988, 0.74428014,\n",
       "        0.74841454, 0.7402475 , 0.75118736, 1.51373431, 0.75832049,\n",
       "        0.75932027, 0.74863458, 0.74617607, 0.7476841 , 0.74723838,\n",
       "        0.74832949, 1.49270971, 0.75674059, 0.74724952, 1.48864857,\n",
       "        0.74385411, 1.5126433 , 0.75057122, 0.74357343, 1.49620184,\n",
       "        1.49606645, 1.49800857, 0.75009822, 0.74819103, 1.50686467,\n",
       "        0.76178339, 0.7480796 , 0.73696697, 1.49544974, 1.49892692,\n",
       "        1.49940888, 0.74818176, 0.75018603, 0.75238629, 0.75024151,\n",
       "        0.74326155, 0.73681029, 1.50080102, 0.75455704, 0.74996131,\n",
       "        0.74648846, 1.50887967, 0.7460205 , 0.74765216, 0.7569674 ,\n",
       "        1.48781365, 0.74770215, 0.75297981, 0.74505495, 0.75490098,\n",
       "        0.74925689, 0.74441207, 1.51619391, 0.75105289, 1.49471673,\n",
       "        0.7533792 , 0.74630323, 0.75198586, 0.7493437 , 1.49026591,\n",
       "        1.51197556, 1.49152473, 0.74839996, 0.74647735, 0.7568787 ,\n",
       "        0.75023516, 1.51007932, 1.4965232 , 0.75587967, 1.51019802,\n",
       "        0.73570152, 0.74849736, 0.74856966, 0.73730822, 0.75691439,\n",
       "        1.50906947, 0.74903076, 0.75224535, 0.75254082, 0.74963723,\n",
       "        0.74670426, 0.75051351, 0.75115476, 0.74403247, 1.49298817,\n",
       "        0.74607242, 0.75038284, 0.75572511, 0.74872104, 0.75540265,\n",
       "        0.75343217, 1.49260006, 0.74874655, 0.75217636, 1.50857724,\n",
       "        0.74479666, 1.51092051, 0.74942527, 1.4988225 , 1.50428838,\n",
       "        0.76124171, 1.50560036, 0.74469012, 1.49965766, 0.74756264,\n",
       "        0.74311585, 1.50517282, 0.74774004, 0.74923508, 0.74408483,\n",
       "        1.47423323, 0.74874573, 1.50559785, 1.49419114, 1.51098044,\n",
       "        1.50134111, 1.50331097, 0.75413766, 0.76075745, 0.74762835,\n",
       "        0.74711808, 0.75437177, 0.75171942, 1.50968405, 1.48936262,\n",
       "        0.75215321, 0.76664192, 0.74177192, 1.48187877, 0.75107274,\n",
       "        0.74778019, 0.74837335, 1.50432296, 0.74563251, 0.74562415,\n",
       "        0.74002898, 0.74752009, 0.7493414 , 0.7485994 , 0.75502846,\n",
       "        1.49271667, 1.50673622, 1.51788811, 0.74385506, 0.74322669,\n",
       "        1.50363757, 1.49836031, 0.75259109, 1.49110143, 1.50246998,\n",
       "        0.74908291, 0.74870296, 1.49136378, 0.75196171, 0.75640562,\n",
       "        0.75171093, 0.74799322, 1.49889604, 0.74832334, 0.74511182,\n",
       "        0.75463159, 0.75173232, 0.75582459, 0.74311302, 0.7496083 ,\n",
       "        0.76199825, 0.74690747, 0.74902564, 0.73911585, 0.75627819,\n",
       "        0.75291877, 1.50538424, 0.75408822, 0.7510456 , 0.74941204,\n",
       "        1.48848485, 1.49148319, 0.76275596, 1.47873013, 0.75139125,\n",
       "        0.74568479, 1.50660153, 1.49483216, 1.51510901, 1.51044308,\n",
       "        0.74153067, 1.47742769, 0.75512174, 1.49839525, 1.51368102,\n",
       "        0.74921602, 0.75062455, 0.74936644, 0.747576  , 1.50401196,\n",
       "        0.76040851, 0.74518887, 0.74769309, 0.74842496, 0.7510373 ,\n",
       "        0.75083463, 0.75335919, 1.48708747, 0.74897388, 1.49382568,\n",
       "        0.74548741, 0.74822118, 0.74118576, 1.50327553, 0.75722457,\n",
       "        0.74586819, 1.51626112, 0.7461525 , 0.74149261, 0.7422184 ,\n",
       "        0.74813799, 0.75064743, 0.74831977, 0.74938803, 0.75255957,\n",
       "        0.75662137, 0.76090278, 1.49971652, 1.50748914, 1.48000432,\n",
       "        0.74478527, 0.76033179, 1.50659412, 0.74943245, 0.7410154 ,\n",
       "        0.75659307, 0.74394757, 1.49128421, 1.51573975, 0.74914677,\n",
       "        0.75643219, 0.7465252 , 0.75736845, 0.74899783, 0.73108062,\n",
       "        0.74868915, 1.49972606, 0.74659186, 0.75286189, 0.75147106,\n",
       "        1.50323227, 0.74059542, 0.74878979, 1.50291545, 0.74695471,\n",
       "        0.74733893, 1.49416699, 0.75304884, 0.75517107, 0.74023395,\n",
       "        1.4892553 , 1.49989782, 0.75330607, 0.75011839]),\n",
       " 'q5': array([1.67374622, 0.84016758, 0.84377537, 0.84576592, 1.65148878,\n",
       "        0.83085102, 0.83258688, 0.83810715, 0.83428072, 0.83026302,\n",
       "        0.84116677, 0.83240735, 0.83514995, 1.68687976, 0.84331779,\n",
       "        0.83916233, 0.83815162, 0.82966707, 0.8359668 , 0.83993458,\n",
       "        0.84155963, 1.67383697, 0.84273922, 0.83808839, 1.67856803,\n",
       "        0.83554821, 1.68606898, 0.83570981, 0.83211353, 1.66278769,\n",
       "        1.66414264, 1.66801022, 0.84494573, 0.83894649, 1.66732913,\n",
       "        0.84215705, 0.83536403, 0.83355667, 1.66494027, 1.65810229,\n",
       "        1.68371541, 0.83776667, 0.83457828, 0.83782458, 0.84414499,\n",
       "        0.83475972, 0.82915522, 1.67647544, 0.83958574, 0.84059994,\n",
       "        0.83626011, 1.68541776, 0.84027952, 0.83723327, 0.84303092,\n",
       "        1.66660701, 0.83869018, 0.83932062, 0.82420387, 0.83892582,\n",
       "        0.82908067, 0.83180874, 1.67326323, 0.82707661, 1.6718176 ,\n",
       "        0.83465635, 0.83419748, 0.845422  , 0.84185962, 1.67027819,\n",
       "        1.67074884, 1.66004558, 0.83759065, 0.83138418, 0.83997861,\n",
       "        0.84001279, 1.68315213, 1.65842053, 0.8409922 , 1.67429627,\n",
       "        0.8312459 , 0.84042881, 0.83332312, 0.82870832, 0.84266563,\n",
       "        1.69137551, 0.85123817, 0.83740963, 0.83204024, 0.82660811,\n",
       "        0.83344016, 0.84483664, 0.83510273, 0.82912734, 1.66916166,\n",
       "        0.83499132, 0.83327394, 0.84671232, 0.82934602, 0.84010137,\n",
       "        0.83818508, 1.65302078, 0.8374185 , 0.84323036, 1.68069173,\n",
       "        0.8360008 , 1.67653258, 0.83389361, 1.65905407, 1.6676716 ,\n",
       "        0.84285312, 1.66716817, 0.82878886, 1.66260041, 0.84127541,\n",
       "        0.82956244, 1.68200664, 0.83943353, 0.82913265, 0.83468833,\n",
       "        1.63521292, 0.83913974, 1.67074601, 1.66700145, 1.6711549 ,\n",
       "        1.67154105, 1.67200287, 0.8433452 , 0.84874213, 0.83266817,\n",
       "        0.83285057, 0.83787423, 0.84039823, 1.67897274, 1.65874168,\n",
       "        0.84012594, 0.84689386, 0.83817443, 1.66587953, 0.83355136,\n",
       "        0.83440044, 0.83297253, 1.66101161, 0.82740477, 0.8381581 ,\n",
       "        0.83065399, 0.82971237, 0.84531651, 0.83523624, 0.83877731,\n",
       "        1.66205052, 1.67436915, 1.67596083, 0.83050899, 0.82452839,\n",
       "        1.67232324, 1.66768601, 0.84127404, 1.65717636, 1.67587677,\n",
       "        0.83453713, 0.83805277, 1.64246721, 0.83458854, 0.84563534,\n",
       "        0.84364949, 0.8266728 , 1.67107827, 0.83460236, 0.83040486,\n",
       "        0.84810799, 0.8376577 , 0.84464552, 0.83263149, 0.83477493,\n",
       "        0.85160644, 0.83166874, 0.84011747, 0.82815917, 0.84080626,\n",
       "        0.8396329 , 1.67709702, 0.84272726, 0.83485546, 0.83085369,\n",
       "        1.65952495, 1.67130351, 0.84549281, 1.63869719, 0.84532734,\n",
       "        0.84008748, 1.66917613, 1.6737096 , 1.67477079, 1.68758859,\n",
       "        0.83269288, 1.65345508, 0.84084529, 1.66959166, 1.67930212,\n",
       "        0.8365798 , 0.83668538, 0.83633476, 0.83429344, 1.65746148,\n",
       "        0.84134752, 0.83633899, 0.8283425 , 0.83903473, 0.84169868,\n",
       "        0.83640679, 0.84298699, 1.66495946, 0.83922594, 1.65045166,\n",
       "        0.83106007, 0.83726643, 0.83399012, 1.66074016, 0.83445624,\n",
       "        0.83591717, 1.66180131, 0.83586845, 0.8304199 , 0.8332543 ,\n",
       "        0.84021033, 0.83521673, 0.83700425, 0.83969305, 0.83293521,\n",
       "        0.84182247, 0.83598728, 1.66389834, 1.66536661, 1.65993064,\n",
       "        0.83782695, 0.85042064, 1.66969934, 0.83486962, 0.83235181,\n",
       "        0.85324856, 0.82959682, 1.65473583, 1.68408985, 0.82728989,\n",
       "        0.84095402, 0.83102794, 0.83964186, 0.83906084, 0.82575629,\n",
       "        0.83305459, 1.67743285, 0.84136574, 0.83739237, 0.84053846,\n",
       "        1.67126051, 0.82637456, 0.83912328, 1.67078518, 0.82639824,\n",
       "        0.83283376, 1.64965077, 0.84007524, 0.83684134, 0.82973529,\n",
       "        1.67283916, 1.66277595, 0.84290698, 0.83246499]),\n",
       " 'q6': array([1.74195185, 0.88105827, 0.88393236, 0.88144774, 1.72881422,\n",
       "        0.86958931, 0.87737013, 0.870177  , 0.87716683, 0.86648274,\n",
       "        0.87378011, 0.87740338, 0.87238128, 1.77059489, 0.88325817,\n",
       "        0.88135373, 0.87979495, 0.87489069, 0.87667536, 0.87364856,\n",
       "        0.87717292, 1.75138063, 0.87329689, 0.87293133, 1.75641213,\n",
       "        0.87624299, 1.75453076, 0.87162619, 0.87120566, 1.73896149,\n",
       "        1.74867578, 1.73046465, 0.8823514 , 0.88385679, 1.74852877,\n",
       "        0.87932942, 0.87561409, 0.86991776, 1.7422114 , 1.7323857 ,\n",
       "        1.76625405, 0.87609063, 0.87450039, 0.87549699, 0.88268638,\n",
       "        0.87070362, 0.87056279, 1.75262846, 0.87387565, 0.8758311 ,\n",
       "        0.87575085, 1.74383564, 0.87529135, 0.87792338, 0.88458725,\n",
       "        1.73977302, 0.88608564, 0.88149378, 0.86273461, 0.87469878,\n",
       "        0.86697847, 0.86850916, 1.74672578, 0.86706362, 1.74122136,\n",
       "        0.87310568, 0.87709287, 0.8860738 , 0.88246532, 1.74825171,\n",
       "        1.74443526, 1.73941911, 0.87858733, 0.87265869, 0.884272  ,\n",
       "        0.87299567, 1.76070122, 1.73471598, 0.88139814, 1.76830212,\n",
       "        0.86370436, 0.87679355, 0.86868839, 0.85991599, 0.88296048,\n",
       "        1.75789671, 0.88745414, 0.87517068, 0.87221801, 0.87139807,\n",
       "        0.87416334, 0.89259652, 0.86780013, 0.86965747, 1.74024412,\n",
       "        0.87673533, 0.87467894, 0.88066115, 0.87040804, 0.873279  ,\n",
       "        0.88021086, 1.73112744, 0.87133195, 0.87959003, 1.76377954,\n",
       "        0.87732488, 1.74691904, 0.87635708, 1.7453755 , 1.73834449,\n",
       "        0.88511977, 1.73854134, 0.86458425, 1.74433023, 0.88428226,\n",
       "        0.87252446, 1.75597381, 0.88239143, 0.87389175, 0.87875318,\n",
       "        1.73650723, 0.87422482, 1.74130868, 1.7345566 , 1.74717189,\n",
       "        1.74694905, 1.7565437 , 0.88051642, 0.88989865, 0.87105356,\n",
       "        0.87243723, 0.8739277 , 0.86669883, 1.74121565, 1.73233355,\n",
       "        0.88047019, 0.88365046, 0.88361439, 1.75095915, 0.87591348,\n",
       "        0.87175319, 0.86876639, 1.73799466, 0.86834474, 0.87681762,\n",
       "        0.87604765, 0.87320623, 0.88804147, 0.86757118, 0.87129337,\n",
       "        1.73633199, 1.74450832, 1.76497507, 0.88245147, 0.8614993 ,\n",
       "        1.736073  , 1.75066113, 0.88150412, 1.7313193 , 1.76217367,\n",
       "        0.87284392, 0.87994989, 1.73512721, 0.87462341, 0.88075811,\n",
       "        0.883489  , 0.86412674, 1.73720331, 0.86711142, 0.86002466,\n",
       "        0.88521444, 0.87538368, 0.88204357, 0.87229161, 0.87643746,\n",
       "        0.89244296, 0.87278923, 0.88014383, 0.86624509, 0.88197603,\n",
       "        0.87744073, 1.7466918 , 0.88378493, 0.88146789, 0.86500443,\n",
       "        1.73381806, 1.75569769, 0.88224558, 1.72377405, 0.87783526,\n",
       "        0.87726799, 1.74667657, 1.7367824 , 1.74830866, 1.74945167,\n",
       "        0.87528341, 1.72815448, 0.88328333, 1.74885424, 1.7468889 ,\n",
       "        0.87787745, 0.88058709, 0.87861279, 0.87095332, 1.74547462,\n",
       "        0.87364571, 0.87141496, 0.87110652, 0.88287184, 0.87867683,\n",
       "        0.874668  , 0.87776895, 1.73824319, 0.88298947, 1.72427147,\n",
       "        0.87402945, 0.87931899, 0.87255763, 1.72817853, 0.87575707,\n",
       "        0.87478053, 1.72736494, 0.87124377, 0.87271375, 0.87186566,\n",
       "        0.87607336, 0.87648262, 0.87796611, 0.87728218, 0.87624096,\n",
       "        0.88444005, 0.86838682, 1.73022123, 1.74146895, 1.73570596,\n",
       "        0.8807363 , 0.89093439, 1.7451223 , 0.87028341, 0.87088157,\n",
       "        0.88810889, 0.86836669, 1.72742244, 1.75308032, 0.87144793,\n",
       "        0.87977031, 0.87632772, 0.87352375, 0.87707365, 0.86950892,\n",
       "        0.88156983, 1.76149332, 0.87634989, 0.8789147 , 0.87284646,\n",
       "        1.73647405, 0.8733107 , 0.87477594, 1.73445427, 0.86678364,\n",
       "        0.87807068, 1.7210894 , 0.88180324, 0.86804214, 0.86498486,\n",
       "        1.74007212, 1.74106075, 0.88017806, 0.86469267]),\n",
       " 'q7': array([1.99980573, 1.01760656, 0.99053578, 0.99063677, 1.97885326,\n",
       "        1.00036719, 0.99631661, 0.9881006 , 0.99976005, 0.99902039,\n",
       "        1.00020853, 1.00764962, 0.99494261, 2.02823373, 0.98969791,\n",
       "        1.01234816, 0.99759951, 1.00903969, 1.01042375, 1.02117746,\n",
       "        1.00737723, 1.99068011, 0.97794512, 1.00224587, 2.00341201,\n",
       "        1.00160016, 1.99595481, 0.98498625, 0.98749433, 1.95822202,\n",
       "        1.95582568, 1.98084066, 1.01484992, 1.04367993, 1.98530635,\n",
       "        0.99938745, 1.00251671, 0.98134009, 1.97255996, 1.98526355,\n",
       "        2.01553436, 1.01672904, 1.00505446, 0.99876932, 0.99423887,\n",
       "        0.99582509, 1.00235845, 1.98470284, 1.00032018, 1.01571048,\n",
       "        0.99166418, 1.99012148, 1.01011105, 1.00589492, 1.00938668,\n",
       "        1.98759298, 1.00285863, 1.00817647, 0.98536608, 0.98850621,\n",
       "        0.99488704, 1.00669485, 1.98319699, 0.9977021 , 2.01359506,\n",
       "        1.00737551, 0.98897697, 0.99855032, 0.99038376, 1.96275629,\n",
       "        2.00901592, 1.95943738, 1.01209558, 1.00684425, 0.9925567 ,\n",
       "        1.00179211, 2.02184581, 1.97245536, 1.02008887, 2.02428412,\n",
       "        0.99205341, 0.98943681, 0.99180687, 0.99132538, 1.01878925,\n",
       "        2.00026576, 1.00713015, 1.00776277, 1.00191591, 1.00665869,\n",
       "        0.98574312, 1.00568525, 1.0136539 , 1.00226848, 1.95937298,\n",
       "        0.9992244 , 1.00800398, 1.01819503, 0.99770593, 1.01424949,\n",
       "        1.00207821, 1.99804417, 0.99210187, 1.00018871, 1.97613727,\n",
       "        0.99886212, 1.96794637, 1.01497239, 1.96532769, 1.96144898,\n",
       "        1.01407895, 1.99984596, 0.99083132, 2.00555769, 1.00870465,\n",
       "        1.00318143, 1.99170158, 1.00587332, 0.98734066, 1.00622121,\n",
       "        1.99426088, 1.00436466, 1.98588995, 1.94015586, 1.97283879,\n",
       "        1.99634705, 2.02636528, 1.01230443, 1.03311478, 1.00439846,\n",
       "        0.99255841, 0.99753594, 1.01011367, 2.00387077, 2.01019966,\n",
       "        0.98963825, 1.0020149 , 0.99790967, 1.96300849, 0.99818323,\n",
       "        0.99777196, 1.01319306, 1.9657046 , 0.99913134, 0.99401599,\n",
       "        1.01618564, 1.01966632, 1.00718065, 1.00024225, 1.00656151,\n",
       "        1.94179118, 1.97622269, 2.05694329, 1.01496445, 0.98961883,\n",
       "        1.97136704, 2.00111467, 1.02232651, 1.9878484 , 2.00021758,\n",
       "        0.98609303, 1.01142634, 2.00699137, 0.98911583, 0.99478366,\n",
       "        1.00987451, 0.99086484, 1.98964829, 1.01019716, 1.00190405,\n",
       "        1.02006446, 0.99540859, 1.02320276, 0.99633265, 1.01110598,\n",
       "        1.0191145 , 1.01733917, 0.99202398, 0.99435899, 1.02325424,\n",
       "        1.01662977, 1.98244338, 1.00787904, 1.00248475, 0.97972498,\n",
       "        1.96386133, 1.98377741, 0.99686956, 1.96267363, 1.00048684,\n",
       "        0.99039057, 2.04802845, 1.99057895, 2.02497607, 2.00365735,\n",
       "        0.9950518 , 2.01151557, 1.01498635, 2.00716659, 2.01081131,\n",
       "        1.00529962, 1.009514  , 1.00980838, 0.99760429, 2.02379052,\n",
       "        0.98735475, 0.98675214, 0.98907356, 1.00344388, 1.00586365,\n",
       "        0.99041192, 1.01954518, 1.99434188, 0.99389246, 2.00003118,\n",
       "        1.00865857, 1.00206268, 1.00260117, 2.01438601, 1.00939584,\n",
       "        1.00217101, 1.98078664, 0.99725272, 0.99192542, 0.9987111 ,\n",
       "        1.00870627, 1.012167  , 1.00158482, 1.00445355, 0.99890722,\n",
       "        1.01620254, 0.99014044, 2.02633474, 2.01020908, 1.98321154,\n",
       "        1.02574556, 1.01337612, 1.97206497, 0.99690538, 0.99409559,\n",
       "        1.0250761 , 0.99217708, 1.97185091, 1.99918504, 1.00196785,\n",
       "        1.01236041, 1.00727739, 1.02206626, 1.00973149, 1.01200497,\n",
       "        1.01198597, 2.02039766, 0.98619504, 1.00423432, 1.00704223,\n",
       "        1.96651259, 0.99978361, 1.00559699, 1.9935524 , 0.98312721,\n",
       "        1.00242942, 1.94024548, 1.01911256, 0.97996944, 1.00123463,\n",
       "        2.00354662, 1.9720583 , 0.99523263, 1.00401376]),\n",
       " 'q8': array([2.22013777, 1.08509588, 1.06001875, 1.04958425, 2.10919476,\n",
       "        1.06647006, 1.08536509, 1.04237012, 1.05456584, 1.1046199 ,\n",
       "        1.07523211, 1.07383787, 1.07007969, 2.17073842, 1.07768251,\n",
       "        1.10543154, 1.06262427, 1.07801412, 1.07470267, 1.07658743,\n",
       "        1.09501919, 2.15454445, 1.04801219, 1.06780623, 2.21256323,\n",
       "        1.09990158, 2.1083167 , 1.04010703, 1.06877979, 2.15024784,\n",
       "        2.08415371, 2.13644475, 1.08347247, 1.16309747, 2.14992404,\n",
       "        1.11904061, 1.07312518, 1.0633484 , 2.05794993, 2.13603831,\n",
       "        2.15035549, 1.07507664, 1.07342144, 1.07536214, 1.04634921,\n",
       "        1.04795609, 1.07015535, 2.07647416, 1.12594741, 1.09101412,\n",
       "        1.04925619, 2.17895969, 1.06610128, 1.06528978, 1.07477708,\n",
       "        2.16999656, 1.07253261, 1.13360942, 1.06101216, 1.04465696,\n",
       "        1.07003646, 1.09974758, 2.05965129, 1.07873883, 2.21957439,\n",
       "        1.13477446, 1.06209612, 1.08257485, 1.03429683, 2.1312346 ,\n",
       "        2.16804508, 2.09130952, 1.07535202, 1.08423033, 1.04632201,\n",
       "        1.10146131, 2.07748237, 2.08667213, 1.11316515, 2.1019435 ,\n",
       "        1.0556157 , 1.05445063, 1.06060432, 1.05640261, 1.10639073,\n",
       "        2.14468085, 1.07729569, 1.09067246, 1.11187747, 1.06617799,\n",
       "        1.04974193, 1.05758079, 1.07562555, 1.08892457, 2.15526177,\n",
       "        1.10805444, 1.07755487, 1.08237594, 1.08141843, 1.0606405 ,\n",
       "        1.10524903, 2.09810708, 1.05730481, 1.08254506, 2.16739353,\n",
       "        1.0759497 , 2.09843032, 1.07234503, 2.10145226, 2.09477432,\n",
       "        1.09549732, 2.12767884, 1.08049272, 2.16355013, 1.06869506,\n",
       "        1.07046426, 2.15124199, 1.08406736, 1.08282744, 1.06903779,\n",
       "        2.11129925, 1.06035961, 2.10613675, 2.06813423, 2.15383599,\n",
       "        2.15181924, 2.16117407, 1.1101216 , 1.11196208, 1.07639126,\n",
       "        1.08136889, 1.07647809, 1.10052029, 2.1606462 , 2.15141392,\n",
       "        1.04320058, 1.09846669, 1.07404156, 2.06506051, 1.07365976,\n",
       "        1.06578632, 1.08645382, 2.12225002, 1.08890938, 1.0818338 ,\n",
       "        1.08342925, 1.08531825, 1.0825517 , 1.064838  , 1.0866444 ,\n",
       "        2.1509083 , 2.07087945, 2.24578071, 1.08049834, 1.08606653,\n",
       "        2.13921618, 2.13029631, 1.08636236, 2.11605951, 2.14700442,\n",
       "        1.08287845, 1.07361228, 2.15380115, 1.05453775, 1.0681494 ,\n",
       "        1.06330795, 1.04708872, 2.17129213, 1.0895648 , 1.08577397,\n",
       "        1.09787535, 1.0745957 , 1.11044974, 1.06023056, 1.10467807,\n",
       "        1.09818018, 1.07999014, 1.06313236, 1.05774682, 1.09393765,\n",
       "        1.07518992, 2.11337304, 1.08453693, 1.08532703, 1.04307017,\n",
       "        2.10072166, 2.16505371, 1.04878313, 2.14201867, 1.05639909,\n",
       "        1.05368461, 2.17514641, 2.11282479, 2.12074514, 2.15249924,\n",
       "        1.05873959, 2.15853976, 1.10572942, 2.16042065, 2.15037019,\n",
       "        1.0810845 , 1.07153549, 1.10753028, 1.06345844, 2.17555564,\n",
       "        1.06129531, 1.06257148, 1.10083134, 1.06899118, 1.10739951,\n",
       "        1.07678303, 1.10331112, 2.10300452, 1.07675014, 2.14518399,\n",
       "        1.0939737 , 1.05409456, 1.06317029, 2.14358696, 1.05051671,\n",
       "        1.09919978, 2.08193704, 1.09803279, 1.08899608, 1.0649629 ,\n",
       "        1.09186546, 1.09024406, 1.1228653 , 1.07348174, 1.08093119,\n",
       "        1.07575071, 1.09140113, 2.13859601, 2.12310023, 2.0717006 ,\n",
       "        1.10984644, 1.11205763, 2.09049262, 1.06204147, 1.07200161,\n",
       "        1.10076801, 1.12774468, 2.09141926, 2.11319916, 1.06642113,\n",
       "        1.08257229, 1.079482  , 1.11075938, 1.08785691, 1.08811072,\n",
       "        1.09867689, 2.13963383, 1.07509933, 1.08762683, 1.07883011,\n",
       "        2.0874249 , 1.08439228, 1.05816573, 2.12866131, 1.0524235 ,\n",
       "        1.13614032, 2.04301432, 1.0813418 , 1.05227582, 1.05784615,\n",
       "        2.12049193, 2.1058704 , 1.06354564, 1.09404334])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_mo = {'q'+str(i): y_train[:, i] for i in range(len(quantiles))}\n",
    "y_val_mo = {'q'+str(i): y_val[:, i] for i in range(len(quantiles))}\n",
    "y_train_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No data provided for \"tf_op_layer_sub\". Need data for each key in: ['tf_op_layer_sub', 'tf_op_layer_sub_4', 'tf_op_layer_sub_8', 'tf_op_layer_sub_12', 'tf_op_layer_sub_16', 'tf_op_layer_sub_20', 'tf_op_layer_sub_24', 'tf_op_layer_sub_28', 'tf_op_layer_sub_32']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    505\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m       ]\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    505\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m       ]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tf_op_layer_sub'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-b60b72d9b648>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m history = model.fit(X_train, y_train_mo, epochs=300,\n\u001b[1;32m----> 2\u001b[1;33m                     validation_data=(X_val, y_val_mo))\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[1;32m--> 646\u001b[1;33m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[0;32m    647\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2467\u001b[0m           \u001b[0mshapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2468\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2469\u001b[1;33m           exception_prefix='target')\n\u001b[0m\u001b[0;32m   2470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m       \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m       raise ValueError('No data provided for \"' + e.args[0] + '\". Need data '\n\u001b[1;32m--> 510\u001b[1;33m                        'for each key in: ' + str(names))\n\u001b[0m\u001b[0;32m    511\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No data provided for \"tf_op_layer_sub\". Need data for each key in: ['tf_op_layer_sub', 'tf_op_layer_sub_4', 'tf_op_layer_sub_8', 'tf_op_layer_sub_12', 'tf_op_layer_sub_16', 'tf_op_layer_sub_20', 'tf_op_layer_sub_24', 'tf_op_layer_sub_28', 'tf_op_layer_sub_32']"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_mo, epochs=300,\n",
    "                    validation_data=(X_val, y_val_mo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# revert multi output format to (n_samples, quantiles)\n",
    "y_pred = np.array(list(zip(*y_pred))).squeeze()\n",
    "y_pred[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: training seems slower! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employ pinball loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Daniel Sch., at:\n",
    "# https://stackoverflow.com/questions/43151694/define-pinball-loss-function-in-keras-with-tensorflow-backend\n",
    "def create_pinball_loss(tau=0.5):\n",
    "    def pinball_loss(y_true, y_pred):\n",
    "        err = y_true - y_pred\n",
    "        return K.mean(K.maximum(tau * err, (tau - 1) * err), axis=-1)\n",
    "    return pinball_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {'q'+str(i): create_pinball_loss(tau=q) for (i, q) in enumerate(quantiles)}\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(inp_shape=(train_df.columns.size,), quantiles=quantiles)\n",
    "model.compile(optimizer=\"adam\", loss=losses)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train_mo, epochs=300,\n",
    "                    validation_data=(X_val, y_val_mo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict validation set\n",
    "y_pred = model.predict(X_val)\n",
    "# revert multi output format to (n_samples, quantiles)\n",
    "y_pred = np.array(list(zip(*y_pred))).squeeze()\n",
    "y_pred[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the distribution of predictions is wider than when trained with the MAE. This is in line with what we would expect: over-predicting the lower quantiles is punished much harder than before, and the same for under-predicting the higher quantiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pinball loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/keras-team/keras/pull/8033/files\n",
    "def test_pinball_loss():\n",
    "    y_pred = K.variable(np.array([0.3, 0.6, 0.1]))\n",
    "    y_true = K.variable(np.array([0.3, 0.4, 0.5]))\n",
    "    quantile = 0.25\n",
    "    loss_fcn = create_pinball_loss(tau=quantile)#losses.PinballLoss(quantile)\n",
    "    expected_loss = (quantile * 0.4 + (1 - quantile) * 0.2) / 3\n",
    "    loss = K.eval(loss_fcn(y_true, y_pred))\n",
    "    assert np.isclose(expected_loss, loss)\n",
    "\n",
    "test_pinball_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"custom_layer/features2.csv\", index_col=0)\n",
    "target_df = pd.read_csv(\"custom_layer/targets2.csv\", index_col=0, header=None, names=['target'])\n",
    "quantiles = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Daniel Sch., at:\n",
    "# https://stackoverflow.com/questions/43151694/define-pinball-loss-function-in-keras-with-tensorflow-backend\n",
    "def create_pinball_loss(tau=0.5):\n",
    "    def pinball_loss(y_true, y_pred):\n",
    "        err = y_true - y_pred\n",
    "        return K.mean(K.maximum(tau * err, (tau - 1) * err), axis=-1)\n",
    "    return pinball_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.values\n",
    "y = target_df.values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train_mo = {'q'+str(i): y_train for i in range(len(quantiles))}\n",
    "y_val_mo = {'q'+str(i): y_val for i in range(len(quantiles))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(inp_shape, quantiles):\n",
    "    # clear previous sessions\n",
    "    K.clear_session()\n",
    "\n",
    "    inp = Input(inp_shape, name=\"input\")\n",
    "    x = inp\n",
    "    x = Dense(16)(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = Dense(2)(x)  # represents mu, sigma\n",
    "    \n",
    "    out_q0 = Dense(1, name=\"q0\")(x)  # DistributionLayer(quantile=quantiles[0])(x)\n",
    "    out_q1 = Dense(1, name=\"q1\")(x)  # DistributionLayer(quantile=quantiles[1])(x)\n",
    "    out_q2 = Dense(1, name=\"q2\")(x)  # ...\n",
    "    out_q3 = Dense(1, name=\"q3\")(x)\n",
    "    out_q4 = Dense(1, name=\"q4\")(x)\n",
    "    out_q5 = Dense(1, name=\"q5\")(x)\n",
    "    out_q6 = Dense(1, name=\"q6\")(x)\n",
    "    out_q7 = Dense(1, name=\"q7\")(x)\n",
    "    out_q8 = Dense(1, name=\"q8\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=[out_q0, out_q1, out_q2, out_q3, out_q4, out_q5, out_q6, out_q7, out_q8])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {'q'+str(i): create_pinball_loss(tau=q) for (i, q) in enumerate(quantiles)}\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(inp_shape=(X_train.shape[1],), quantiles=quantiles)\n",
    "model.compile(optimizer=\"adam\", loss=losses)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train_mo, epochs=300,\n",
    "                    validation_data=(X_val, y_val_mo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict validation set\n",
    "y_pred = model.predict(X_val)\n",
    "# revert multi output format to (n_samples, quantiles)\n",
    "y_pred = np.array(list(zip(*y_pred))).squeeze()\n",
    "y_pred[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"true\" quantiles\n",
    "pd.Series(np.random.normal(0.75, 0.13, size=100000)).quantile(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"true\" quantiles\n",
    "pd.Series(np.random.normal(1.5, 0.25, size=100000)).quantile(quantiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**: The toy dataset contains two simple distributions, with either $\\mu=0.75, \\sigma=0.13$ or $\\mu=1.5, \\sigma=0.25$, depending on whether it is a weekday or weekend. The observed 'demand' are samples distributed as such. For these simple distributions, the Pinball Loss is able to (approximately) retrieve the correct quantiles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01324b84642f4e1fafc49cf89f9ff391": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2573316ddee1409d897edfcfbd87e8ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5712bde02e284e70bc5e9c1d7367502a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_01324b84642f4e1fafc49cf89f9ff391",
       "placeholder": "",
       "style": "IPY_MODEL_860311c3b0564afa891a4c6e24da2d3a",
       "value": " 42840/42840 [00:49&lt;00:00, 860.84it/s]"
      }
     },
     "5f64a626e4d342ccaa955ea1c68afacd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6314c6fca7a845618625e72ac301f6a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2573316ddee1409d897edfcfbd87e8ba",
       "max": 42840,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7370a0011b714f6dbba43bf3f3725de8",
       "value": 42840
      }
     },
     "7370a0011b714f6dbba43bf3f3725de8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "860311c3b0564afa891a4c6e24da2d3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8c18e2818bf648e08be0bc8e855fcb3f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6314c6fca7a845618625e72ac301f6a7",
        "IPY_MODEL_5712bde02e284e70bc5e9c1d7367502a"
       ],
       "layout": "IPY_MODEL_5f64a626e4d342ccaa955ea1c68afacd"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
