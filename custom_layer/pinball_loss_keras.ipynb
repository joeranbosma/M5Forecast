{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\18-09-19 Document structure\\business\\Study\\Master\\Cognitive Computing\\P3\\Machine learning in practice\\git\\Private\\M5Forecast\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "E:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os, gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, LeakyReLU\n",
    "from tensorflow.keras.layers import Flatten, Input, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, concatenate, Reshape, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "os.environ['DATA_DIR'] = 'data/'\n",
    "os.environ['SUB_DIR'] = 'submissions_uncertainty/'\n",
    "for dirname, _, filenames in os.walk(os.environ['DATA_DIR']):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Hardcode requested quantiles\n",
    "quantiles = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    f, ax = plt.subplots(1, 1, figsize=(18, 6))\n",
    "    ax.plot(history.history['loss'], label='Train')\n",
    "    ax.plot(history.history['val_loss'], label='Validation')\n",
    "    ax.set_ylim(0)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinball Loss function for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"custom_layer/features.csv\", index_col=0)\n",
    "target_df = pd.read_csv(\"custom_layer/targets.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "      <th>w_3</th>\n",
       "      <th>w_4</th>\n",
       "      <th>w_5</th>\n",
       "      <th>w_6</th>\n",
       "      <th>w_7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-30</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-31</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-02-01</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-02-02</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            wday  month  snap_CA  w_1  w_2  w_3  w_4  w_5  w_6  w_7\n",
       "date                                                               \n",
       "2011-01-29     1      1        0    1    0    0    0    0    0    0\n",
       "2011-01-30     2      1        0    0    1    0    0    0    0    0\n",
       "2011-01-31     3      1        0    0    0    1    0    0    0    0\n",
       "2011-02-01     4      2        1    0    0    0    1    0    0    0\n",
       "2011-02-02     5      2        1    0    0    0    0    1    0    0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.005</th>\n",
       "      <th>0.025</th>\n",
       "      <th>0.165</th>\n",
       "      <th>0.25</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.75</th>\n",
       "      <th>0.835</th>\n",
       "      <th>0.975</th>\n",
       "      <th>0.995</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>0.429084</td>\n",
       "      <td>0.500587</td>\n",
       "      <td>0.628482</td>\n",
       "      <td>0.662077</td>\n",
       "      <td>0.743227</td>\n",
       "      <td>0.824528</td>\n",
       "      <td>0.861499</td>\n",
       "      <td>0.989619</td>\n",
       "      <td>1.086067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-30</th>\n",
       "      <td>0.834881</td>\n",
       "      <td>1.032595</td>\n",
       "      <td>1.265410</td>\n",
       "      <td>1.343565</td>\n",
       "      <td>1.503276</td>\n",
       "      <td>1.660740</td>\n",
       "      <td>1.728179</td>\n",
       "      <td>2.014386</td>\n",
       "      <td>2.143587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-31</th>\n",
       "      <td>0.838167</td>\n",
       "      <td>0.991256</td>\n",
       "      <td>1.246369</td>\n",
       "      <td>1.329488</td>\n",
       "      <td>1.508577</td>\n",
       "      <td>1.680692</td>\n",
       "      <td>1.763780</td>\n",
       "      <td>1.976137</td>\n",
       "      <td>2.167394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-02-01</th>\n",
       "      <td>0.424697</td>\n",
       "      <td>0.501162</td>\n",
       "      <td>0.629065</td>\n",
       "      <td>0.668797</td>\n",
       "      <td>0.758173</td>\n",
       "      <td>0.842714</td>\n",
       "      <td>0.878296</td>\n",
       "      <td>1.015888</td>\n",
       "      <td>1.090475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-02-02</th>\n",
       "      <td>0.438830</td>\n",
       "      <td>0.511583</td>\n",
       "      <td>0.630601</td>\n",
       "      <td>0.669403</td>\n",
       "      <td>0.754088</td>\n",
       "      <td>0.842727</td>\n",
       "      <td>0.883785</td>\n",
       "      <td>1.007879</td>\n",
       "      <td>1.084537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0.005     0.025     0.165      0.25       0.5      0.75  \\\n",
       "date                                                                     \n",
       "2011-01-29  0.429084  0.500587  0.628482  0.662077  0.743227  0.824528   \n",
       "2011-01-30  0.834881  1.032595  1.265410  1.343565  1.503276  1.660740   \n",
       "2011-01-31  0.838167  0.991256  1.246369  1.329488  1.508577  1.680692   \n",
       "2011-02-01  0.424697  0.501162  0.629065  0.668797  0.758173  0.842714   \n",
       "2011-02-02  0.438830  0.511583  0.630601  0.669403  0.754088  0.842727   \n",
       "\n",
       "               0.835     0.975     0.995  \n",
       "date                                      \n",
       "2011-01-29  0.861499  0.989619  1.086067  \n",
       "2011-01-30  1.728179  2.014386  2.143587  \n",
       "2011-01-31  1.763780  1.976137  2.167394  \n",
       "2011-02-01  0.878296  1.015888  1.090475  \n",
       "2011-02-02  0.883785  1.007879  1.084537  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(inp_shape, quantiles):\n",
    "    # clear previous sessions\n",
    "    K.clear_session()\n",
    "\n",
    "    inp = Input(inp_shape, name=\"input\")\n",
    "    x = inp\n",
    "    x = Dense(16)(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = Dense(2)(x)  # represents mu, sigma\n",
    "    x = Dense(len(quantiles))(x)  # returns 9 points, one for each quantile\n",
    "    out = x\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 9)                 27        \n",
      "=================================================================\n",
      "Total params: 2,989\n",
      "Trainable params: 2,989\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "quantiles = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]\n",
    "model = get_model(inp_shape=(train_df.columns.size,), quantiles=quantiles)\n",
    "model.compile(optimizer=\"adam\", loss=\"MAE\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.values\n",
    "y = target_df.values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 269 samples, validate on 68 samples\n",
      "Epoch 1/300\n",
      "269/269 [==============================] - 2s 9ms/sample - loss: 1.1917 - val_loss: 0.9042\n",
      "Epoch 2/300\n",
      "269/269 [==============================] - 0s 749us/sample - loss: 0.8217 - val_loss: 0.7791\n",
      "Epoch 3/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.7792 - val_loss: 0.6893\n",
      "Epoch 4/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.7038 - val_loss: 0.6442\n",
      "Epoch 5/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.6833 - val_loss: 0.6131\n",
      "Epoch 6/300\n",
      "269/269 [==============================] - 0s 130us/sample - loss: 0.6547 - val_loss: 0.5905\n",
      "Epoch 7/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.6272 - val_loss: 0.5632\n",
      "Epoch 8/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.5996 - val_loss: 0.5338\n",
      "Epoch 9/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.5675 - val_loss: 0.5071\n",
      "Epoch 10/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.5388 - val_loss: 0.4832\n",
      "Epoch 11/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.5117 - val_loss: 0.4658\n",
      "Epoch 12/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.4850 - val_loss: 0.4283\n",
      "Epoch 13/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.4563 - val_loss: 0.4109\n",
      "Epoch 14/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.4329 - val_loss: 0.3875\n",
      "Epoch 15/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.4086 - val_loss: 0.3633\n",
      "Epoch 16/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.3857 - val_loss: 0.3409\n",
      "Epoch 17/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.3617 - val_loss: 0.3194\n",
      "Epoch 18/300\n",
      "269/269 [==============================] - 0s 126us/sample - loss: 0.3366 - val_loss: 0.2946\n",
      "Epoch 19/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.3118 - val_loss: 0.2766\n",
      "Epoch 20/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.2952 - val_loss: 0.2471\n",
      "Epoch 21/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.2641 - val_loss: 0.2320\n",
      "Epoch 22/300\n",
      "269/269 [==============================] - 0s 125us/sample - loss: 0.2472 - val_loss: 0.2236\n",
      "Epoch 23/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.2265 - val_loss: 0.1986\n",
      "Epoch 24/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.2084 - val_loss: 0.1832\n",
      "Epoch 25/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.2081 - val_loss: 0.1992\n",
      "Epoch 26/300\n",
      "269/269 [==============================] - 0s 134us/sample - loss: 0.2031 - val_loss: 0.1971\n",
      "Epoch 27/300\n",
      "269/269 [==============================] - 0s 128us/sample - loss: 0.1936 - val_loss: 0.1681\n",
      "Epoch 28/300\n",
      "269/269 [==============================] - 0s 125us/sample - loss: 0.1777 - val_loss: 0.1708\n",
      "Epoch 29/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.1779 - val_loss: 0.1599\n",
      "Epoch 30/300\n",
      "269/269 [==============================] - 0s 125us/sample - loss: 0.1646 - val_loss: 0.1626\n",
      "Epoch 31/300\n",
      "269/269 [==============================] - 0s 138us/sample - loss: 0.1668 - val_loss: 0.1678\n",
      "Epoch 32/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.1733 - val_loss: 0.1993\n",
      "Epoch 33/300\n",
      "269/269 [==============================] - 0s 126us/sample - loss: 0.1689 - val_loss: 0.1421\n",
      "Epoch 34/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.1390 - val_loss: 0.1165\n",
      "Epoch 35/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.1208 - val_loss: 0.1082\n",
      "Epoch 36/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.1102 - val_loss: 0.0944\n",
      "Epoch 37/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0983 - val_loss: 0.0839\n",
      "Epoch 38/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0910 - val_loss: 0.0949\n",
      "Epoch 39/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0867 - val_loss: 0.0735\n",
      "Epoch 40/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0785 - val_loss: 0.0575\n",
      "Epoch 41/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0605 - val_loss: 0.0471\n",
      "Epoch 42/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0588 - val_loss: 0.0449\n",
      "Epoch 43/300\n",
      "269/269 [==============================] - 0s 116us/sample - loss: 0.0518 - val_loss: 0.0449\n",
      "Epoch 44/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0490 - val_loss: 0.0339\n",
      "Epoch 45/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0394 - val_loss: 0.0295\n",
      "Epoch 46/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0306 - val_loss: 0.0288\n",
      "Epoch 47/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0392 - val_loss: 0.0424\n",
      "Epoch 48/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0566 - val_loss: 0.0445\n",
      "Epoch 49/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0374 - val_loss: 0.0373\n",
      "Epoch 50/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0316 - val_loss: 0.0342\n",
      "Epoch 51/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0301 - val_loss: 0.0284\n",
      "Epoch 52/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0330 - val_loss: 0.0351\n",
      "Epoch 53/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0325 - val_loss: 0.0385\n",
      "Epoch 54/300\n",
      "269/269 [==============================] - 0s 128us/sample - loss: 0.0324 - val_loss: 0.0268\n",
      "Epoch 55/300\n",
      "269/269 [==============================] - 0s 126us/sample - loss: 0.0323 - val_loss: 0.0271\n",
      "Epoch 56/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.0255 - val_loss: 0.0220\n",
      "Epoch 57/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0292 - val_loss: 0.0198\n",
      "Epoch 58/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0220 - val_loss: 0.0204\n",
      "Epoch 59/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0269 - val_loss: 0.0213\n",
      "Epoch 60/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0296 - val_loss: 0.0199\n",
      "Epoch 61/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0284 - val_loss: 0.0438\n",
      "Epoch 62/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0421 - val_loss: 0.0672\n",
      "Epoch 63/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0396 - val_loss: 0.0319\n",
      "Epoch 64/300\n",
      "269/269 [==============================] - 0s 125us/sample - loss: 0.0284 - val_loss: 0.0268\n",
      "Epoch 65/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0367 - val_loss: 0.0443\n",
      "Epoch 66/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0372 - val_loss: 0.0371\n",
      "Epoch 67/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0356 - val_loss: 0.0412\n",
      "Epoch 68/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0295 - val_loss: 0.0216\n",
      "Epoch 69/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0341 - val_loss: 0.0344\n",
      "Epoch 70/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0276 - val_loss: 0.0272\n",
      "Epoch 71/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0287 - val_loss: 0.0386\n",
      "Epoch 72/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0381 - val_loss: 0.0204\n",
      "Epoch 73/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0308 - val_loss: 0.0259\n",
      "Epoch 74/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0306 - val_loss: 0.0568\n",
      "Epoch 75/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0432 - val_loss: 0.0328\n",
      "Epoch 76/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0244 - val_loss: 0.0214\n",
      "Epoch 77/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0245 - val_loss: 0.0276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0263 - val_loss: 0.0323\n",
      "Epoch 79/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0273 - val_loss: 0.0194\n",
      "Epoch 80/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0203 - val_loss: 0.0301\n",
      "Epoch 81/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0232 - val_loss: 0.0166\n",
      "Epoch 82/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0190 - val_loss: 0.0257\n",
      "Epoch 83/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0274 - val_loss: 0.0208\n",
      "Epoch 84/300\n",
      "269/269 [==============================] - 0s 101us/sample - loss: 0.0262 - val_loss: 0.0208\n",
      "Epoch 85/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0415 - val_loss: 0.0457\n",
      "Epoch 86/300\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.043 - 0s 102us/sample - loss: 0.0291 - val_loss: 0.0365\n",
      "Epoch 87/300\n",
      "269/269 [==============================] - 0s 125us/sample - loss: 0.0309 - val_loss: 0.0405\n",
      "Epoch 88/300\n",
      "269/269 [==============================] - 0s 128us/sample - loss: 0.0407 - val_loss: 0.0185\n",
      "Epoch 89/300\n",
      "269/269 [==============================] - 0s 134us/sample - loss: 0.0200 - val_loss: 0.0163\n",
      "Epoch 90/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0173 - val_loss: 0.0209\n",
      "Epoch 91/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0243 - val_loss: 0.0372\n",
      "Epoch 92/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0384 - val_loss: 0.0287\n",
      "Epoch 93/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0400 - val_loss: 0.0239\n",
      "Epoch 94/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0205 - val_loss: 0.0319\n",
      "Epoch 95/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0280 - val_loss: 0.0363\n",
      "Epoch 96/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0398 - val_loss: 0.0196\n",
      "Epoch 97/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0233 - val_loss: 0.0157\n",
      "Epoch 98/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0191 - val_loss: 0.0239\n",
      "Epoch 99/300\n",
      "269/269 [==============================] - 0s 132us/sample - loss: 0.0235 - val_loss: 0.0197\n",
      "Epoch 100/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0168 - val_loss: 0.0124\n",
      "Epoch 101/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0178 - val_loss: 0.0268\n",
      "Epoch 102/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0219 - val_loss: 0.0198\n",
      "Epoch 103/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0224 - val_loss: 0.0354\n",
      "Epoch 104/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0281 - val_loss: 0.0221\n",
      "Epoch 105/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0362 - val_loss: 0.0328\n",
      "Epoch 106/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0255 - val_loss: 0.0424\n",
      "Epoch 107/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0317 - val_loss: 0.0245\n",
      "Epoch 108/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0183 - val_loss: 0.0141\n",
      "Epoch 109/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0153 - val_loss: 0.0169\n",
      "Epoch 110/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0191 - val_loss: 0.0156\n",
      "Epoch 111/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0187 - val_loss: 0.0300\n",
      "Epoch 112/300\n",
      "269/269 [==============================] - 0s 134us/sample - loss: 0.0210 - val_loss: 0.0216\n",
      "Epoch 113/300\n",
      "269/269 [==============================] - 0s 143us/sample - loss: 0.0172 - val_loss: 0.0243\n",
      "Epoch 114/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0235 - val_loss: 0.0201\n",
      "Epoch 115/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0219 - val_loss: 0.0197\n",
      "Epoch 116/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0208 - val_loss: 0.0168\n",
      "Epoch 117/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.0137 - val_loss: 0.0102\n",
      "Epoch 118/300\n",
      "269/269 [==============================] - 0s 149us/sample - loss: 0.0141 - val_loss: 0.0189\n",
      "Epoch 119/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0177 - val_loss: 0.0156\n",
      "Epoch 120/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0139 - val_loss: 0.0161\n",
      "Epoch 121/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0148 - val_loss: 0.0152\n",
      "Epoch 122/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0139 - val_loss: 0.0125\n",
      "Epoch 123/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0150 - val_loss: 0.0142\n",
      "Epoch 124/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0167 - val_loss: 0.0161\n",
      "Epoch 125/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0172 - val_loss: 0.0208\n",
      "Epoch 126/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0176 - val_loss: 0.0175\n",
      "Epoch 127/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0215 - val_loss: 0.0259\n",
      "Epoch 128/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0235 - val_loss: 0.0192\n",
      "Epoch 129/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0208 - val_loss: 0.0222\n",
      "Epoch 130/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0231 - val_loss: 0.0178\n",
      "Epoch 131/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0206 - val_loss: 0.0257\n",
      "Epoch 132/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0237 - val_loss: 0.0194\n",
      "Epoch 133/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0168 - val_loss: 0.0157\n",
      "Epoch 134/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0185 - val_loss: 0.0236\n",
      "Epoch 135/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 136/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0138 - val_loss: 0.0124\n",
      "Epoch 137/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0137 - val_loss: 0.0148\n",
      "Epoch 138/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0146 - val_loss: 0.0144\n",
      "Epoch 139/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0157 - val_loss: 0.0255\n",
      "Epoch 140/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0218 - val_loss: 0.0149\n",
      "Epoch 141/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0152 - val_loss: 0.0178\n",
      "Epoch 142/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0178 - val_loss: 0.0253\n",
      "Epoch 143/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0213 - val_loss: 0.0114\n",
      "Epoch 144/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.0215 - val_loss: 0.0180\n",
      "Epoch 145/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0247 - val_loss: 0.0218\n",
      "Epoch 146/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0222 - val_loss: 0.0131\n",
      "Epoch 147/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0178 - val_loss: 0.0197\n",
      "Epoch 148/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0197 - val_loss: 0.0207\n",
      "Epoch 149/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0219 - val_loss: 0.0233\n",
      "Epoch 150/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0166 - val_loss: 0.0107\n",
      "Epoch 151/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0134 - val_loss: 0.0103\n",
      "Epoch 152/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0135 - val_loss: 0.0172\n",
      "Epoch 153/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0144 - val_loss: 0.0113\n",
      "Epoch 154/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0123 - val_loss: 0.0132\n",
      "Epoch 155/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0145 - val_loss: 0.0216\n",
      "Epoch 156/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0167 - val_loss: 0.0154\n",
      "Epoch 157/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0131 - val_loss: 0.0112\n",
      "Epoch 158/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0139 - val_loss: 0.0137\n",
      "Epoch 159/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0134 - val_loss: 0.0105\n",
      "Epoch 160/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0124 - val_loss: 0.0274\n",
      "Epoch 161/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0253 - val_loss: 0.0303\n",
      "Epoch 162/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0203 - val_loss: 0.0225\n",
      "Epoch 163/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0191 - val_loss: 0.0255\n",
      "Epoch 164/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0246 - val_loss: 0.0213\n",
      "Epoch 165/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0159 - val_loss: 0.0177\n",
      "Epoch 166/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0153 - val_loss: 0.0136\n",
      "Epoch 167/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0155 - val_loss: 0.0197\n",
      "Epoch 168/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0163 - val_loss: 0.0163\n",
      "Epoch 169/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0175 - val_loss: 0.0218\n",
      "Epoch 170/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0209 - val_loss: 0.0135\n",
      "Epoch 171/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0193 - val_loss: 0.0148\n",
      "Epoch 172/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0245 - val_loss: 0.0197\n",
      "Epoch 173/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0179 - val_loss: 0.0146\n",
      "Epoch 174/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0187 - val_loss: 0.0137\n",
      "Epoch 175/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0168 - val_loss: 0.0157\n",
      "Epoch 176/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0149 - val_loss: 0.0126\n",
      "Epoch 177/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0146 - val_loss: 0.0174\n",
      "Epoch 178/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0192 - val_loss: 0.0193\n",
      "Epoch 179/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0214 - val_loss: 0.0304\n",
      "Epoch 180/300\n",
      "269/269 [==============================] - 0s 109us/sample - loss: 0.0236 - val_loss: 0.0254\n",
      "Epoch 181/300\n",
      "269/269 [==============================] - 0s 125us/sample - loss: 0.0276 - val_loss: 0.0112\n",
      "Epoch 182/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0216 - val_loss: 0.0166\n",
      "Epoch 183/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0145 - val_loss: 0.0126\n",
      "Epoch 184/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0182 - val_loss: 0.0161\n",
      "Epoch 185/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0196 - val_loss: 0.0188\n",
      "Epoch 186/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0209 - val_loss: 0.0171\n",
      "Epoch 187/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0223 - val_loss: 0.0204\n",
      "Epoch 188/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0174 - val_loss: 0.0196\n",
      "Epoch 189/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0185 - val_loss: 0.0151\n",
      "Epoch 190/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0230 - val_loss: 0.0154\n",
      "Epoch 191/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0175 - val_loss: 0.0109\n",
      "Epoch 192/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0181 - val_loss: 0.0125\n",
      "Epoch 193/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0183 - val_loss: 0.0122\n",
      "Epoch 194/300\n",
      "269/269 [==============================] - 0s 132us/sample - loss: 0.0148 - val_loss: 0.0134\n",
      "Epoch 195/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0133 - val_loss: 0.0148\n",
      "Epoch 196/300\n",
      "269/269 [==============================] - 0s 130us/sample - loss: 0.0136 - val_loss: 0.0114\n",
      "Epoch 197/300\n",
      "269/269 [==============================] - 0s 132us/sample - loss: 0.0150 - val_loss: 0.0241\n",
      "Epoch 198/300\n",
      "269/269 [==============================] - 0s 130us/sample - loss: 0.0216 - val_loss: 0.0119\n",
      "Epoch 199/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0145 - val_loss: 0.0104\n",
      "Epoch 200/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0145 - val_loss: 0.0159\n",
      "Epoch 201/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0158 - val_loss: 0.0221\n",
      "Epoch 202/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0204 - val_loss: 0.0254\n",
      "Epoch 203/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0234 - val_loss: 0.0189\n",
      "Epoch 204/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0156 - val_loss: 0.0152\n",
      "Epoch 205/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0137 - val_loss: 0.0119\n",
      "Epoch 206/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0137 - val_loss: 0.0186\n",
      "Epoch 207/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0151 - val_loss: 0.0162\n",
      "Epoch 208/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0176 - val_loss: 0.0185\n",
      "Epoch 209/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0161 - val_loss: 0.0218\n",
      "Epoch 210/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0164 - val_loss: 0.0167\n",
      "Epoch 211/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0164 - val_loss: 0.0141\n",
      "Epoch 212/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0202 - val_loss: 0.0239\n",
      "Epoch 213/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0200 - val_loss: 0.0122\n",
      "Epoch 214/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0127 - val_loss: 0.0143\n",
      "Epoch 215/300\n",
      "269/269 [==============================] - 0s 125us/sample - loss: 0.0131 - val_loss: 0.0107\n",
      "Epoch 216/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0133 - val_loss: 0.0127\n",
      "Epoch 217/300\n",
      "269/269 [==============================] - 0s 121us/sample - loss: 0.0151 - val_loss: 0.0197\n",
      "Epoch 218/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0181 - val_loss: 0.0203\n",
      "Epoch 219/300\n",
      "269/269 [==============================] - 0s 126us/sample - loss: 0.0181 - val_loss: 0.0158\n",
      "Epoch 220/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0167 - val_loss: 0.0252\n",
      "Epoch 221/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0180 - val_loss: 0.0151\n",
      "Epoch 222/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0176 - val_loss: 0.0135\n",
      "Epoch 223/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0153 - val_loss: 0.0148\n",
      "Epoch 224/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0161 - val_loss: 0.0202\n",
      "Epoch 225/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0188 - val_loss: 0.0260\n",
      "Epoch 226/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0225 - val_loss: 0.0208\n",
      "Epoch 227/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0218 - val_loss: 0.0250\n",
      "Epoch 228/300\n",
      "269/269 [==============================] - 0s 126us/sample - loss: 0.0169 - val_loss: 0.0133\n",
      "Epoch 229/300\n",
      "269/269 [==============================] - 0s 136us/sample - loss: 0.0168 - val_loss: 0.0180\n",
      "Epoch 230/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0151 - val_loss: 0.0136\n",
      "Epoch 231/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0167 - val_loss: 0.0172\n",
      "Epoch 232/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0143 - val_loss: 0.0146\n",
      "Epoch 233/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0124 - val_loss: 0.0111\n",
      "Epoch 234/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0126 - val_loss: 0.0199\n",
      "Epoch 235/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0220 - val_loss: 0.0207\n",
      "Epoch 236/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0143 - val_loss: 0.0137\n",
      "Epoch 237/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0172 - val_loss: 0.0183\n",
      "Epoch 238/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0199 - val_loss: 0.0189\n",
      "Epoch 239/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0228 - val_loss: 0.0326\n",
      "Epoch 240/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0268 - val_loss: 0.0382\n",
      "Epoch 241/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0378 - val_loss: 0.0225\n",
      "Epoch 242/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0222 - val_loss: 0.0150\n",
      "Epoch 243/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.0215 - val_loss: 0.0369\n",
      "Epoch 244/300\n",
      "269/269 [==============================] - 0s 125us/sample - loss: 0.0311 - val_loss: 0.0155\n",
      "Epoch 245/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.0213 - val_loss: 0.0270\n",
      "Epoch 246/300\n",
      "269/269 [==============================] - 0s 126us/sample - loss: 0.0216 - val_loss: 0.0226\n",
      "Epoch 247/300\n",
      "269/269 [==============================] - 0s 105us/sample - loss: 0.0214 - val_loss: 0.0213\n",
      "Epoch 248/300\n",
      "269/269 [==============================] - 0s 126us/sample - loss: 0.0184 - val_loss: 0.0203\n",
      "Epoch 249/300\n",
      "269/269 [==============================] - 0s 141us/sample - loss: 0.0184 - val_loss: 0.0194\n",
      "Epoch 250/300\n",
      "269/269 [==============================] - 0s 152us/sample - loss: 0.0139 - val_loss: 0.0102\n",
      "Epoch 251/300\n",
      "269/269 [==============================] - 0s 141us/sample - loss: 0.0126 - val_loss: 0.0151\n",
      "Epoch 252/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0163 - val_loss: 0.0220\n",
      "Epoch 253/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0197 - val_loss: 0.0174\n",
      "Epoch 254/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0163 - val_loss: 0.0134\n",
      "Epoch 255/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0149 - val_loss: 0.0124\n",
      "Epoch 256/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0131 - val_loss: 0.0121\n",
      "Epoch 257/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0130 - val_loss: 0.0162\n",
      "Epoch 258/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0143 - val_loss: 0.0161\n",
      "Epoch 259/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0141 - val_loss: 0.0234\n",
      "Epoch 260/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0235 - val_loss: 0.0238\n",
      "Epoch 261/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0202 - val_loss: 0.0168\n",
      "Epoch 262/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0160 - val_loss: 0.0152\n",
      "Epoch 263/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0169 - val_loss: 0.0153\n",
      "Epoch 264/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0155 - val_loss: 0.0146\n",
      "Epoch 265/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0149 - val_loss: 0.0132\n",
      "Epoch 266/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0165 - val_loss: 0.0136\n",
      "Epoch 267/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0139 - val_loss: 0.0128\n",
      "Epoch 268/300\n",
      "269/269 [==============================] - 0s 126us/sample - loss: 0.0138 - val_loss: 0.0129\n",
      "Epoch 269/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0170 - val_loss: 0.0113\n",
      "Epoch 270/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0139 - val_loss: 0.0149\n",
      "Epoch 271/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0151 - val_loss: 0.0133\n",
      "Epoch 272/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0144 - val_loss: 0.0133\n",
      "Epoch 273/300\n",
      "269/269 [==============================] - ETA: 0s - loss: 0.014 - 0s 106us/sample - loss: 0.0128 - val_loss: 0.0118\n",
      "Epoch 274/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0128 - val_loss: 0.0120\n",
      "Epoch 275/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0120 - val_loss: 0.0127\n",
      "Epoch 276/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.0138 - val_loss: 0.0115\n",
      "Epoch 277/300\n",
      "269/269 [==============================] - 0s 143us/sample - loss: 0.0147 - val_loss: 0.0152\n",
      "Epoch 278/300\n",
      "269/269 [==============================] - 0s 145us/sample - loss: 0.0177 - val_loss: 0.0150\n",
      "Epoch 279/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0147 - val_loss: 0.0155\n",
      "Epoch 280/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0135 - val_loss: 0.0163\n",
      "Epoch 281/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0168 - val_loss: 0.0127\n",
      "Epoch 282/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0155 - val_loss: 0.0173\n",
      "Epoch 283/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0157 - val_loss: 0.0228\n",
      "Epoch 284/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0224 - val_loss: 0.0124\n",
      "Epoch 285/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0185 - val_loss: 0.0190\n",
      "Epoch 286/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0162 - val_loss: 0.0143\n",
      "Epoch 287/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0170 - val_loss: 0.0139\n",
      "Epoch 288/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0162 - val_loss: 0.0134\n",
      "Epoch 289/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0163 - val_loss: 0.0136\n",
      "Epoch 290/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0158 - val_loss: 0.0215\n",
      "Epoch 291/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0187 - val_loss: 0.0107\n",
      "Epoch 292/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0139 - val_loss: 0.0239\n",
      "Epoch 293/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0194 - val_loss: 0.0171\n",
      "Epoch 294/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0219 - val_loss: 0.0161\n",
      "Epoch 295/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.0147 - val_loss: 0.0179\n",
      "Epoch 296/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0168 - val_loss: 0.0202\n",
      "Epoch 297/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0146 - val_loss: 0.0147\n",
      "Epoch 298/300\n",
      "269/269 [==============================] - 0s 115us/sample - loss: 0.0155 - val_loss: 0.0174\n",
      "Epoch 299/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0152 - val_loss: 0.0177\n",
      "Epoch 300/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0148 - val_loss: 0.0168\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=300,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4114417 , 0.48735207, 0.61332107, 0.6514602 , 0.7352706 ,\n",
       "        0.82840574, 0.8609055 , 0.9938454 , 1.0638919 ],\n",
       "       [0.8470335 , 1.0021744 , 1.2437438 , 1.3201789 , 1.4834784 ,\n",
       "        1.6747763 , 1.7289132 , 2.0042746 , 2.1307094 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrite to multi-output for each quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DistributionLayer(quantile_index,x):\n",
    "    mu = x[0]\n",
    "    sigma = x[1] \n",
    "    skewness = None\n",
    "    kurtosis = None\n",
    "    if (len(x)>2):\n",
    "        skewness= x[2]\n",
    "    if (len(x)>3):\n",
    "        kurtosis = x[3]\n",
    "        \n",
    "    if (skewness==None):\n",
    "        if (kurtosis==None):\n",
    "            # Source of Z-scores: https://www.wolframalpha.com/input/?i=percentiles+of+a+normal+distribution\n",
    "             return {\n",
    "                0.005: mu-2.57583*sigma, # https://www.wolframalpha.com/input/?i=0.5+percentiles+of+a+normal+distribution\n",
    "                0.025: mu-1.95996*sigma, # https://www.wolframalpha.com/input/?i=2.5+percentiles+of+a+normal+distribution\n",
    "                0.165: mu-0.974114*sigma, # https://www.wolframalpha.com/input/?i=16.5+percentiles+of+a+normal+distribution\n",
    "                0.25: mu-0.674*sigma, # https://www.wolframalpha.com/input/?i=25+percentiles+of+a+normal+distribution\n",
    "                0.5: mu, # https://www.wolframalpha.com/input/?i=50+percentiles+of+a+normal+distribution\n",
    "                0.75: mu+0.674*sigma, # https://www.wolframalpha.com/input/?i=75+percentiles+of+a+normal+distribution\n",
    "                0.835: mu+0.9741114*sigma, #https://www.wolframalpha.com/input/?i=83.5+percentiles+of+a+normal+distribution\n",
    "                0.975: mu+1.95996*sigma, #https://www.wolframalpha.com/input/?i=97.5+percentiles+of+a+normal+distribution\n",
    "                0.995: mu+2.57583*sigma, #https://www.wolframalpha.com/input/?i=99.5+percentiles+of+a+normal+distribution\n",
    "            }[quantile]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y=-2.1516599999999997\n"
     ]
    }
   ],
   "source": [
    "x =[]\n",
    "mu = 3\n",
    "sigma = 2\n",
    "x.append(mu) \n",
    "x.append(sigma)\n",
    "quantile = 0.005\n",
    "val=DistributionLayer(quantile,x)\n",
    "print(f'y={val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(inp_shape, quantiles):\n",
    "    # clear previous sessions\n",
    "    K.clear_session()\n",
    "\n",
    "    inp = Input(inp_shape, name=\"input\")\n",
    "    x = inp\n",
    "    x = Dense(16)(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = Dense(2)(x)  # represents mu, sigma\n",
    "    \n",
    "    #out_q0 = Dense(1, name=\"q0\")(x)  # DistributionLayer(quantile=quantiles[0])(x)\n",
    "    out_q0 = DistributionLayer(quantiles[0],x)\n",
    "    print(f'out_q0={out_q0}')\n",
    "    out_q1 = Dense(1, name=\"q1\")(x)  # DistributionLayer(quantile=quantiles[1])(x)\n",
    "    out_q2 = Dense(1, name=\"q2\")(x)  # ...\n",
    "    out_q3 = Dense(1, name=\"q3\")(x)\n",
    "    out_q4 = Dense(1, name=\"q4\")(x)\n",
    "    out_q5 = Dense(1, name=\"q5\")(x)\n",
    "    out_q6 = Dense(1, name=\"q6\")(x)\n",
    "    out_q7 = Dense(1, name=\"q7\")(x)\n",
    "    out_q8 = Dense(1, name=\"q8\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=[out_q0, out_q1, out_q2, out_q3, out_q4, out_q5, out_q6, out_q7, out_q8])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(inp_shape=(train_df.columns.size,), quantiles=quantiles)\n",
    "model.compile(optimizer=\"adam\", loss=\"MAE\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_mo = {'q'+str(i): y_train[:, i] for i in range(len(quantiles))}\n",
    "y_val_mo = {'q'+str(i): y_val[:, i] for i in range(len(quantiles))}\n",
    "y_train_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train_mo, epochs=300,\n",
    "                    validation_data=(X_val, y_val_mo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# revert multi output format to (n_samples, quantiles)\n",
    "y_pred = np.array(list(zip(*y_pred))).squeeze()\n",
    "y_pred[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: training seems slower! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employ pinball loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Daniel Sch., at:\n",
    "# https://stackoverflow.com/questions/43151694/define-pinball-loss-function-in-keras-with-tensorflow-backend\n",
    "def create_pinball_loss(tau=0.5):\n",
    "    def pinball_loss(y_true, y_pred):\n",
    "        err = y_true - y_pred\n",
    "        return K.mean(K.maximum(tau * err, (tau - 1) * err), axis=-1)\n",
    "    return pinball_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {'q'+str(i): create_pinball_loss(tau=q) for (i, q) in enumerate(quantiles)}\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(inp_shape=(train_df.columns.size,), quantiles=quantiles)\n",
    "model.compile(optimizer=\"adam\", loss=losses)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train_mo, epochs=300,\n",
    "                    validation_data=(X_val, y_val_mo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict validation set\n",
    "y_pred = model.predict(X_val)\n",
    "# revert multi output format to (n_samples, quantiles)\n",
    "y_pred = np.array(list(zip(*y_pred))).squeeze()\n",
    "y_pred[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the distribution of predictions is wider than when trained with the MAE. This is in line with what we would expect: over-predicting the lower quantiles is punished much harder than before, and the same for under-predicting the higher quantiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pinball loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/keras-team/keras/pull/8033/files\n",
    "def test_pinball_loss():\n",
    "    y_pred = K.variable(np.array([0.3, 0.6, 0.1]))\n",
    "    y_true = K.variable(np.array([0.3, 0.4, 0.5]))\n",
    "    quantile = 0.25\n",
    "    loss_fcn = create_pinball_loss(tau=quantile)#losses.PinballLoss(quantile)\n",
    "    expected_loss = (quantile * 0.4 + (1 - quantile) * 0.2) / 3\n",
    "    loss = K.eval(loss_fcn(y_true, y_pred))\n",
    "    assert np.isclose(expected_loss, loss)\n",
    "\n",
    "test_pinball_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"custom_layer/features2.csv\", index_col=0)\n",
    "target_df = pd.read_csv(\"custom_layer/targets2.csv\", index_col=0, header=None, names=['target'])\n",
    "quantiles = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Daniel Sch., at:\n",
    "# https://stackoverflow.com/questions/43151694/define-pinball-loss-function-in-keras-with-tensorflow-backend\n",
    "def create_pinball_loss(tau=0.5):\n",
    "    def pinball_loss(y_true, y_pred):\n",
    "        err = y_true - y_pred\n",
    "        return K.mean(K.maximum(tau * err, (tau - 1) * err), axis=-1)\n",
    "    return pinball_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.values\n",
    "y = target_df.values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train_mo = {'q'+str(i): y_train for i in range(len(quantiles))}\n",
    "y_val_mo = {'q'+str(i): y_val for i in range(len(quantiles))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(inp_shape, quantiles):\n",
    "    # clear previous sessions\n",
    "    K.clear_session()\n",
    "\n",
    "    inp = Input(inp_shape, name=\"input\")\n",
    "    x = inp\n",
    "    x = Dense(16)(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = Dense(2)(x)  # represents mu, sigma\n",
    "    \n",
    "    out_q0 = Dense(1, name=\"q0\")(x)  # DistributionLayer(quantile=quantiles[0])(x)\n",
    "    out_q1 = Dense(1, name=\"q1\")(x)  # DistributionLayer(quantile=quantiles[1])(x)\n",
    "    out_q2 = Dense(1, name=\"q2\")(x)  # ...\n",
    "    out_q3 = Dense(1, name=\"q3\")(x)\n",
    "    out_q4 = Dense(1, name=\"q4\")(x)\n",
    "    out_q5 = Dense(1, name=\"q5\")(x)\n",
    "    out_q6 = Dense(1, name=\"q6\")(x)\n",
    "    out_q7 = Dense(1, name=\"q7\")(x)\n",
    "    out_q8 = Dense(1, name=\"q8\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=[out_q0, out_q1, out_q2, out_q3, out_q4, out_q5, out_q6, out_q7, out_q8])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {'q'+str(i): create_pinball_loss(tau=q) for (i, q) in enumerate(quantiles)}\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(inp_shape=(X_train.shape[1],), quantiles=quantiles)\n",
    "model.compile(optimizer=\"adam\", loss=losses)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train_mo, epochs=300,\n",
    "                    validation_data=(X_val, y_val_mo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict validation set\n",
    "y_pred = model.predict(X_val)\n",
    "# revert multi output format to (n_samples, quantiles)\n",
    "y_pred = np.array(list(zip(*y_pred))).squeeze()\n",
    "y_pred[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"true\" quantiles\n",
    "pd.Series(np.random.normal(0.75, 0.13, size=100000)).quantile(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"true\" quantiles\n",
    "pd.Series(np.random.normal(1.5, 0.25, size=100000)).quantile(quantiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**: The toy dataset contains two simple distributions, with either $\\mu=0.75, \\sigma=0.13$ or $\\mu=1.5, \\sigma=0.25$, depending on whether it is a weekday or weekend. The observed 'demand' are samples distributed as such. For these simple distributions, the Pinball Loss is able to (approximately) retrieve the correct quantiles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01324b84642f4e1fafc49cf89f9ff391": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2573316ddee1409d897edfcfbd87e8ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5712bde02e284e70bc5e9c1d7367502a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_01324b84642f4e1fafc49cf89f9ff391",
       "placeholder": "​",
       "style": "IPY_MODEL_860311c3b0564afa891a4c6e24da2d3a",
       "value": " 42840/42840 [00:49&lt;00:00, 860.84it/s]"
      }
     },
     "5f64a626e4d342ccaa955ea1c68afacd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6314c6fca7a845618625e72ac301f6a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2573316ddee1409d897edfcfbd87e8ba",
       "max": 42840,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7370a0011b714f6dbba43bf3f3725de8",
       "value": 42840
      }
     },
     "7370a0011b714f6dbba43bf3f3725de8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "860311c3b0564afa891a4c6e24da2d3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8c18e2818bf648e08be0bc8e855fcb3f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6314c6fca7a845618625e72ac301f6a7",
        "IPY_MODEL_5712bde02e284e70bc5e9c1d7367502a"
       ],
       "layout": "IPY_MODEL_5f64a626e4d342ccaa955ea1c68afacd"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
