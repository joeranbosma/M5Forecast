{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\18-09-19 Document structure\\business\\Study\\Master\\Cognitive Computing\\P3\\Machine learning in practice\\git\\Private\\M5Forecast\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "E:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os, gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, LeakyReLU\n",
    "from tensorflow.keras.layers import Flatten, Input, BatchNormalization, Lambda\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, concatenate, Reshape, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "os.environ['DATA_DIR'] = 'data/'\n",
    "os.environ['SUB_DIR'] = 'submissions_uncertainty/'\n",
    "for dirname, _, filenames in os.walk(os.environ['DATA_DIR']):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Hardcode requested quantiles\n",
    "quantiles = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    f, ax = plt.subplots(1, 1, figsize=(18, 6))\n",
    "    ax.plot(history.history['loss'], label='Train')\n",
    "    ax.plot(history.history['val_loss'], label='Validation')\n",
    "    ax.set_ylim(0)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinball Loss function for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"custom_layer/features.csv\", index_col=0)\n",
    "target_df = pd.read_csv(\"custom_layer/targets.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "      <th>w_3</th>\n",
       "      <th>w_4</th>\n",
       "      <th>w_5</th>\n",
       "      <th>w_6</th>\n",
       "      <th>w_7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-30</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-31</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-02-01</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-02-02</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            wday  month  snap_CA  w_1  w_2  w_3  w_4  w_5  w_6  w_7\n",
       "date                                                               \n",
       "2011-01-29     1      1        0    1    0    0    0    0    0    0\n",
       "2011-01-30     2      1        0    0    1    0    0    0    0    0\n",
       "2011-01-31     3      1        0    0    0    1    0    0    0    0\n",
       "2011-02-01     4      2        1    0    0    0    1    0    0    0\n",
       "2011-02-02     5      2        1    0    0    0    0    1    0    0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.005</th>\n",
       "      <th>0.025</th>\n",
       "      <th>0.165</th>\n",
       "      <th>0.25</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.75</th>\n",
       "      <th>0.835</th>\n",
       "      <th>0.975</th>\n",
       "      <th>0.995</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>0.429084</td>\n",
       "      <td>0.500587</td>\n",
       "      <td>0.628482</td>\n",
       "      <td>0.662077</td>\n",
       "      <td>0.743227</td>\n",
       "      <td>0.824528</td>\n",
       "      <td>0.861499</td>\n",
       "      <td>0.989619</td>\n",
       "      <td>1.086067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-30</th>\n",
       "      <td>0.834881</td>\n",
       "      <td>1.032595</td>\n",
       "      <td>1.265410</td>\n",
       "      <td>1.343565</td>\n",
       "      <td>1.503276</td>\n",
       "      <td>1.660740</td>\n",
       "      <td>1.728179</td>\n",
       "      <td>2.014386</td>\n",
       "      <td>2.143587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-31</th>\n",
       "      <td>0.838167</td>\n",
       "      <td>0.991256</td>\n",
       "      <td>1.246369</td>\n",
       "      <td>1.329488</td>\n",
       "      <td>1.508577</td>\n",
       "      <td>1.680692</td>\n",
       "      <td>1.763780</td>\n",
       "      <td>1.976137</td>\n",
       "      <td>2.167394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-02-01</th>\n",
       "      <td>0.424697</td>\n",
       "      <td>0.501162</td>\n",
       "      <td>0.629065</td>\n",
       "      <td>0.668797</td>\n",
       "      <td>0.758173</td>\n",
       "      <td>0.842714</td>\n",
       "      <td>0.878296</td>\n",
       "      <td>1.015888</td>\n",
       "      <td>1.090475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-02-02</th>\n",
       "      <td>0.438830</td>\n",
       "      <td>0.511583</td>\n",
       "      <td>0.630601</td>\n",
       "      <td>0.669403</td>\n",
       "      <td>0.754088</td>\n",
       "      <td>0.842727</td>\n",
       "      <td>0.883785</td>\n",
       "      <td>1.007879</td>\n",
       "      <td>1.084537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0.005     0.025     0.165      0.25       0.5      0.75  \\\n",
       "date                                                                     \n",
       "2011-01-29  0.429084  0.500587  0.628482  0.662077  0.743227  0.824528   \n",
       "2011-01-30  0.834881  1.032595  1.265410  1.343565  1.503276  1.660740   \n",
       "2011-01-31  0.838167  0.991256  1.246369  1.329488  1.508577  1.680692   \n",
       "2011-02-01  0.424697  0.501162  0.629065  0.668797  0.758173  0.842714   \n",
       "2011-02-02  0.438830  0.511583  0.630601  0.669403  0.754088  0.842727   \n",
       "\n",
       "               0.835     0.975     0.995  \n",
       "date                                      \n",
       "2011-01-29  0.861499  0.989619  1.086067  \n",
       "2011-01-30  1.728179  2.014386  2.143587  \n",
       "2011-01-31  1.763780  1.976137  2.167394  \n",
       "2011-02-01  0.878296  1.015888  1.090475  \n",
       "2011-02-02  0.883785  1.007879  1.084537  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(inp_shape, quantiles):\n",
    "    # clear previous sessions\n",
    "    K.clear_session()\n",
    "\n",
    "    inp = Input(inp_shape, name=\"input\")\n",
    "    x = inp\n",
    "    x = Dense(16)(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = Dense(2)(x)  # represents mu, sigma\n",
    "    x = Dense(len(quantiles))(x)  # returns 9 points, one for each quantile\n",
    "    out = x\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 9)                 27        \n",
      "=================================================================\n",
      "Total params: 2,989\n",
      "Trainable params: 2,989\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "quantiles = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]\n",
    "model = get_model(inp_shape=(train_df.columns.size,), quantiles=quantiles)\n",
    "model.compile(optimizer=\"adam\", loss=\"MAE\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.values\n",
    "y = target_df.values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 269 samples, validate on 68 samples\n",
      "Epoch 1/300\n",
      "269/269 [==============================] - 2s 9ms/sample - loss: 1.0705 - val_loss: 0.9371\n",
      "Epoch 2/300\n",
      "269/269 [==============================] - 0s 750us/sample - loss: 0.9392 - val_loss: 0.8732\n",
      "Epoch 3/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.9098 - val_loss: 0.8268\n",
      "Epoch 4/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.8804 - val_loss: 0.8054\n",
      "Epoch 5/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.8547 - val_loss: 0.7817\n",
      "Epoch 6/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.8295 - val_loss: 0.7608\n",
      "Epoch 7/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.8047 - val_loss: 0.7305\n",
      "Epoch 8/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.7758 - val_loss: 0.7091\n",
      "Epoch 9/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.7476 - val_loss: 0.6763\n",
      "Epoch 10/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.7197 - val_loss: 0.6506\n",
      "Epoch 11/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.6867 - val_loss: 0.6214\n",
      "Epoch 12/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.6542 - val_loss: 0.5944\n",
      "Epoch 13/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.6247 - val_loss: 0.5661\n",
      "Epoch 14/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.5997 - val_loss: 0.5435\n",
      "Epoch 15/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.5701 - val_loss: 0.5164\n",
      "Epoch 16/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.5428 - val_loss: 0.4857\n",
      "Epoch 17/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.5114 - val_loss: 0.4618\n",
      "Epoch 18/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.4914 - val_loss: 0.4449\n",
      "Epoch 19/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.4813 - val_loss: 0.4471\n",
      "Epoch 20/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.4602 - val_loss: 0.4079\n",
      "Epoch 21/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.4262 - val_loss: 0.3821\n",
      "Epoch 22/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.4063 - val_loss: 0.3682\n",
      "Epoch 23/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.3923 - val_loss: 0.3651\n",
      "Epoch 24/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.3925 - val_loss: 0.3680\n",
      "Epoch 25/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.3797 - val_loss: 0.3426\n",
      "Epoch 26/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.3558 - val_loss: 0.3177\n",
      "Epoch 27/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.3388 - val_loss: 0.3055\n",
      "Epoch 28/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.3282 - val_loss: 0.3040\n",
      "Epoch 29/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.3352 - val_loss: 0.2885\n",
      "Epoch 30/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.3146 - val_loss: 0.2726\n",
      "Epoch 31/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.2964 - val_loss: 0.2659\n",
      "Epoch 32/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.2802 - val_loss: 0.2489\n",
      "Epoch 33/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.2673 - val_loss: 0.2393\n",
      "Epoch 34/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.2560 - val_loss: 0.2334\n",
      "Epoch 35/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.2494 - val_loss: 0.2357\n",
      "Epoch 36/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.2497 - val_loss: 0.2075\n",
      "Epoch 37/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.2365 - val_loss: 0.1992\n",
      "Epoch 38/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.2171 - val_loss: 0.1934\n",
      "Epoch 39/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.2036 - val_loss: 0.1807\n",
      "Epoch 40/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.1950 - val_loss: 0.1748\n",
      "Epoch 41/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.1898 - val_loss: 0.1738\n",
      "Epoch 42/300\n",
      "269/269 [==============================] - 0s 113us/sample - loss: 0.1871 - val_loss: 0.1768\n",
      "Epoch 43/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.1855 - val_loss: 0.1580\n",
      "Epoch 44/300\n",
      "269/269 [==============================] - 0s 98us/sample - loss: 0.1645 - val_loss: 0.1505\n",
      "Epoch 45/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.1567 - val_loss: 0.1305\n",
      "Epoch 46/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.1436 - val_loss: 0.1251\n",
      "Epoch 47/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.1352 - val_loss: 0.1224\n",
      "Epoch 48/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.1296 - val_loss: 0.1094\n",
      "Epoch 49/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.1237 - val_loss: 0.1069\n",
      "Epoch 50/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.1151 - val_loss: 0.0988\n",
      "Epoch 51/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.1075 - val_loss: 0.0869\n",
      "Epoch 52/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0954 - val_loss: 0.0799\n",
      "Epoch 53/300\n",
      "269/269 [==============================] - 0s 98us/sample - loss: 0.0833 - val_loss: 0.0780\n",
      "Epoch 54/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0824 - val_loss: 0.0590\n",
      "Epoch 55/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0723 - val_loss: 0.0602\n",
      "Epoch 56/300\n",
      "269/269 [==============================] - 0s 101us/sample - loss: 0.0662 - val_loss: 0.0688\n",
      "Epoch 57/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0730 - val_loss: 0.0580\n",
      "Epoch 58/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0613 - val_loss: 0.0458\n",
      "Epoch 59/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0545 - val_loss: 0.0397\n",
      "Epoch 60/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0487 - val_loss: 0.0446\n",
      "Epoch 61/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0508 - val_loss: 0.0472\n",
      "Epoch 62/300\n",
      "269/269 [==============================] - 0s 98us/sample - loss: 0.0532 - val_loss: 0.0490\n",
      "Epoch 63/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0579 - val_loss: 0.0481\n",
      "Epoch 64/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0534 - val_loss: 0.0407\n",
      "Epoch 65/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0520 - val_loss: 0.0581\n",
      "Epoch 66/300\n",
      "269/269 [==============================] - 0s 97us/sample - loss: 0.0571 - val_loss: 0.0404\n",
      "Epoch 67/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0497 - val_loss: 0.0454\n",
      "Epoch 68/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0502 - val_loss: 0.0395\n",
      "Epoch 69/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0447 - val_loss: 0.0369\n",
      "Epoch 70/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0462 - val_loss: 0.0443\n",
      "Epoch 71/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0505 - val_loss: 0.0415\n",
      "Epoch 72/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0493 - val_loss: 0.0412\n",
      "Epoch 73/300\n",
      "269/269 [==============================] - 0s 98us/sample - loss: 0.0509 - val_loss: 0.0443\n",
      "Epoch 74/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0482 - val_loss: 0.0394\n",
      "Epoch 75/300\n",
      "269/269 [==============================] - 0s 98us/sample - loss: 0.0482 - val_loss: 0.0431\n",
      "Epoch 76/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0475 - val_loss: 0.0331\n",
      "Epoch 77/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0446 - val_loss: 0.0382\n",
      "Epoch 78/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 [==============================] - 0s 173us/sample - loss: 0.0454 - val_loss: 0.0358\n",
      "Epoch 79/300\n",
      "269/269 [==============================] - 0s 125us/sample - loss: 0.0441 - val_loss: 0.0406\n",
      "Epoch 80/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0440 - val_loss: 0.0339\n",
      "Epoch 81/300\n",
      "269/269 [==============================] - 0s 97us/sample - loss: 0.0409 - val_loss: 0.0332\n",
      "Epoch 82/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0408 - val_loss: 0.0332\n",
      "Epoch 83/300\n",
      "269/269 [==============================] - 0s 97us/sample - loss: 0.0403 - val_loss: 0.0322\n",
      "Epoch 84/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0406 - val_loss: 0.0334\n",
      "Epoch 85/300\n",
      "269/269 [==============================] - 0s 105us/sample - loss: 0.0399 - val_loss: 0.0335\n",
      "Epoch 86/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0407 - val_loss: 0.0350\n",
      "Epoch 87/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0398 - val_loss: 0.0301\n",
      "Epoch 88/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0390 - val_loss: 0.0349\n",
      "Epoch 89/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0394 - val_loss: 0.0308\n",
      "Epoch 90/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0379 - val_loss: 0.0362\n",
      "Epoch 91/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0372 - val_loss: 0.0295\n",
      "Epoch 92/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0371 - val_loss: 0.0401\n",
      "Epoch 93/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0418 - val_loss: 0.0315\n",
      "Epoch 94/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0366 - val_loss: 0.0345\n",
      "Epoch 95/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0361 - val_loss: 0.0296\n",
      "Epoch 96/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0362 - val_loss: 0.0323\n",
      "Epoch 97/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0329 - val_loss: 0.0263\n",
      "Epoch 98/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0298 - val_loss: 0.0249\n",
      "Epoch 99/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0295 - val_loss: 0.0288\n",
      "Epoch 100/300\n",
      "269/269 [==============================] - 0s 97us/sample - loss: 0.0341 - val_loss: 0.0231\n",
      "Epoch 101/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0328 - val_loss: 0.0294\n",
      "Epoch 102/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0347 - val_loss: 0.0289\n",
      "Epoch 103/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0306 - val_loss: 0.0274\n",
      "Epoch 104/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0290 - val_loss: 0.0272\n",
      "Epoch 105/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0294 - val_loss: 0.0256\n",
      "Epoch 106/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0271 - val_loss: 0.0231\n",
      "Epoch 107/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0287 - val_loss: 0.0228\n",
      "Epoch 108/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0243 - val_loss: 0.0221\n",
      "Epoch 109/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0270 - val_loss: 0.0387\n",
      "Epoch 110/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0349 - val_loss: 0.0281\n",
      "Epoch 111/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0273 - val_loss: 0.0210\n",
      "Epoch 112/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0219 - val_loss: 0.0207\n",
      "Epoch 113/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0211 - val_loss: 0.0181\n",
      "Epoch 114/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0214 - val_loss: 0.0229\n",
      "Epoch 115/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0243 - val_loss: 0.0219\n",
      "Epoch 116/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0266 - val_loss: 0.0188\n",
      "Epoch 117/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0205 - val_loss: 0.0164\n",
      "Epoch 118/300\n",
      "269/269 [==============================] - 0s 97us/sample - loss: 0.0191 - val_loss: 0.0216\n",
      "Epoch 119/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0220 - val_loss: 0.0166\n",
      "Epoch 120/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0192 - val_loss: 0.0166\n",
      "Epoch 121/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0211 - val_loss: 0.0198\n",
      "Epoch 122/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0215 - val_loss: 0.0215\n",
      "Epoch 123/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0187 - val_loss: 0.0205\n",
      "Epoch 124/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0224 - val_loss: 0.0253\n",
      "Epoch 125/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0244 - val_loss: 0.0338\n",
      "Epoch 126/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0274 - val_loss: 0.0215\n",
      "Epoch 127/300\n",
      "269/269 [==============================] - 0s 101us/sample - loss: 0.0179 - val_loss: 0.0167\n",
      "Epoch 128/300\n",
      "269/269 [==============================] - 0s 98us/sample - loss: 0.0185 - val_loss: 0.0247\n",
      "Epoch 129/300\n",
      "269/269 [==============================] - 0s 101us/sample - loss: 0.0247 - val_loss: 0.0220\n",
      "Epoch 130/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0218 - val_loss: 0.0246\n",
      "Epoch 131/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0233 - val_loss: 0.0280\n",
      "Epoch 132/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0189 - val_loss: 0.0195\n",
      "Epoch 133/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0193 - val_loss: 0.0155\n",
      "Epoch 134/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0159 - val_loss: 0.0154\n",
      "Epoch 135/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0161 - val_loss: 0.0152\n",
      "Epoch 136/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0145 - val_loss: 0.0129\n",
      "Epoch 137/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0139 - val_loss: 0.0120\n",
      "Epoch 138/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0129 - val_loss: 0.0102\n",
      "Epoch 139/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0123 - val_loss: 0.0122\n",
      "Epoch 140/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0157 - val_loss: 0.0193\n",
      "Epoch 141/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0192 - val_loss: 0.0103\n",
      "Epoch 142/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0144 - val_loss: 0.0191\n",
      "Epoch 143/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0174 - val_loss: 0.0135\n",
      "Epoch 144/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0168 - val_loss: 0.0165\n",
      "Epoch 145/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0160 - val_loss: 0.0127\n",
      "Epoch 146/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0157 - val_loss: 0.0160\n",
      "Epoch 147/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 148/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0250 - val_loss: 0.0205\n",
      "Epoch 149/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0212 - val_loss: 0.0266\n",
      "Epoch 150/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0255 - val_loss: 0.0299\n",
      "Epoch 151/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0184 - val_loss: 0.0238\n",
      "Epoch 152/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0182 - val_loss: 0.0124\n",
      "Epoch 153/300\n",
      "269/269 [==============================] - 0s 132us/sample - loss: 0.0163 - val_loss: 0.0179\n",
      "Epoch 154/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.0238 - val_loss: 0.0199\n",
      "Epoch 155/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0182 - val_loss: 0.0150\n",
      "Epoch 156/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0151 - val_loss: 0.0187\n",
      "Epoch 157/300\n",
      "269/269 [==============================] - 0s 117us/sample - loss: 0.0144 - val_loss: 0.0213\n",
      "Epoch 158/300\n",
      "269/269 [==============================] - 0s 95us/sample - loss: 0.0217 - val_loss: 0.0224\n",
      "Epoch 159/300\n",
      "269/269 [==============================] - 0s 97us/sample - loss: 0.0206 - val_loss: 0.0142\n",
      "Epoch 160/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0178 - val_loss: 0.0181\n",
      "Epoch 161/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0185 - val_loss: 0.0131\n",
      "Epoch 162/300\n",
      "269/269 [==============================] - 0s 111us/sample - loss: 0.0192 - val_loss: 0.0227\n",
      "Epoch 163/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0223 - val_loss: 0.0339\n",
      "Epoch 164/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0223 - val_loss: 0.0139\n",
      "Epoch 165/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0150 - val_loss: 0.0117\n",
      "Epoch 166/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0165 - val_loss: 0.0186\n",
      "Epoch 167/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0190 - val_loss: 0.0222\n",
      "Epoch 168/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0248 - val_loss: 0.0368\n",
      "Epoch 169/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0244 - val_loss: 0.0197\n",
      "Epoch 170/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0169 - val_loss: 0.0225\n",
      "Epoch 171/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0190 - val_loss: 0.0117\n",
      "Epoch 172/300\n",
      "269/269 [==============================] - 0s 105us/sample - loss: 0.0168 - val_loss: 0.0147\n",
      "Epoch 173/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0174 - val_loss: 0.0159\n",
      "Epoch 174/300\n",
      "269/269 [==============================] - 0s 108us/sample - loss: 0.0155 - val_loss: 0.0148\n",
      "Epoch 175/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0146 - val_loss: 0.0153\n",
      "Epoch 176/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0163 - val_loss: 0.0151\n",
      "Epoch 177/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0174 - val_loss: 0.0121\n",
      "Epoch 178/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0173 - val_loss: 0.0168\n",
      "Epoch 179/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0162 - val_loss: 0.0161\n",
      "Epoch 180/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0153 - val_loss: 0.0154\n",
      "Epoch 181/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0150 - val_loss: 0.0143\n",
      "Epoch 182/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0134 - val_loss: 0.0149\n",
      "Epoch 183/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0165 - val_loss: 0.0144\n",
      "Epoch 184/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0187 - val_loss: 0.0171\n",
      "Epoch 185/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0179 - val_loss: 0.0187\n",
      "Epoch 186/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0183 - val_loss: 0.0224\n",
      "Epoch 187/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0227 - val_loss: 0.0131\n",
      "Epoch 188/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0181 - val_loss: 0.0160\n",
      "Epoch 189/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0166 - val_loss: 0.0153\n",
      "Epoch 190/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0152 - val_loss: 0.0136\n",
      "Epoch 191/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0162 - val_loss: 0.0174\n",
      "Epoch 192/300\n",
      "269/269 [==============================] - 0s 119us/sample - loss: 0.0134 - val_loss: 0.0128\n",
      "Epoch 193/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 194/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0170 - val_loss: 0.0231\n",
      "Epoch 195/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0214 - val_loss: 0.0120\n",
      "Epoch 196/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0187 - val_loss: 0.0203\n",
      "Epoch 197/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0177 - val_loss: 0.0134\n",
      "Epoch 198/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0139 - val_loss: 0.0137\n",
      "Epoch 199/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0154 - val_loss: 0.0247\n",
      "Epoch 200/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0251 - val_loss: 0.0165\n",
      "Epoch 201/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0263 - val_loss: 0.0234\n",
      "Epoch 202/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0217 - val_loss: 0.0164\n",
      "Epoch 203/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0163 - val_loss: 0.0172\n",
      "Epoch 204/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0178 - val_loss: 0.0118\n",
      "Epoch 205/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0154 - val_loss: 0.0135\n",
      "Epoch 206/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0165 - val_loss: 0.0169\n",
      "Epoch 207/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0166 - val_loss: 0.0263\n",
      "Epoch 208/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0197 - val_loss: 0.0129\n",
      "Epoch 209/300\n",
      "269/269 [==============================] - 0s 112us/sample - loss: 0.0136 - val_loss: 0.0136\n",
      "Epoch 210/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0163 - val_loss: 0.0156\n",
      "Epoch 211/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0150 - val_loss: 0.0129\n",
      "Epoch 212/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0192 - val_loss: 0.0142\n",
      "Epoch 213/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0155 - val_loss: 0.0132\n",
      "Epoch 214/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0154 - val_loss: 0.0146\n",
      "Epoch 215/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0161 - val_loss: 0.0137\n",
      "Epoch 216/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0172 - val_loss: 0.0120\n",
      "Epoch 217/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0154 - val_loss: 0.0250\n",
      "Epoch 218/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0182 - val_loss: 0.0113\n",
      "Epoch 219/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0133 - val_loss: 0.0147\n",
      "Epoch 220/300\n",
      "269/269 [==============================] - 0s 101us/sample - loss: 0.0130 - val_loss: 0.0118\n",
      "Epoch 221/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0131 - val_loss: 0.0148\n",
      "Epoch 222/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0130 - val_loss: 0.0150\n",
      "Epoch 223/300\n",
      "269/269 [==============================] - 0s 101us/sample - loss: 0.0143 - val_loss: 0.0166\n",
      "Epoch 224/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0151 - val_loss: 0.0117\n",
      "Epoch 225/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0118 - val_loss: 0.0120\n",
      "Epoch 226/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0123 - val_loss: 0.0111\n",
      "Epoch 227/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0144 - val_loss: 0.0194\n",
      "Epoch 228/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0142 - val_loss: 0.0149\n",
      "Epoch 229/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0151 - val_loss: 0.0167\n",
      "Epoch 230/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0157 - val_loss: 0.0172\n",
      "Epoch 231/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0190 - val_loss: 0.0218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/300\n",
      "269/269 [==============================] - 0s 101us/sample - loss: 0.0191 - val_loss: 0.0165\n",
      "Epoch 233/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0164 - val_loss: 0.0183\n",
      "Epoch 234/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0160 - val_loss: 0.0145\n",
      "Epoch 235/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0140 - val_loss: 0.0145\n",
      "Epoch 236/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0126 - val_loss: 0.0111\n",
      "Epoch 237/300\n",
      "269/269 [==============================] - 0s 98us/sample - loss: 0.0128 - val_loss: 0.0147\n",
      "Epoch 238/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0151 - val_loss: 0.0126\n",
      "Epoch 239/300\n",
      "269/269 [==============================] - 0s 97us/sample - loss: 0.0140 - val_loss: 0.0116\n",
      "Epoch 240/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0116 - val_loss: 0.0108\n",
      "Epoch 241/300\n",
      "269/269 [==============================] - 0s 98us/sample - loss: 0.0129 - val_loss: 0.0192\n",
      "Epoch 242/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0157 - val_loss: 0.0150\n",
      "Epoch 243/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0159 - val_loss: 0.0192\n",
      "Epoch 244/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0190 - val_loss: 0.0170\n",
      "Epoch 245/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0188 - val_loss: 0.0205\n",
      "Epoch 246/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0222 - val_loss: 0.0170\n",
      "Epoch 247/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0189 - val_loss: 0.0134\n",
      "Epoch 248/300\n",
      "269/269 [==============================] - 0s 101us/sample - loss: 0.0151 - val_loss: 0.0137\n",
      "Epoch 249/300\n",
      "269/269 [==============================] - 0s 98us/sample - loss: 0.0132 - val_loss: 0.0140\n",
      "Epoch 250/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0140 - val_loss: 0.0136\n",
      "Epoch 251/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0152 - val_loss: 0.0205\n",
      "Epoch 252/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0204 - val_loss: 0.0211\n",
      "Epoch 253/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0182 - val_loss: 0.0132\n",
      "Epoch 254/300\n",
      "269/269 [==============================] - 0s 101us/sample - loss: 0.0128 - val_loss: 0.0166\n",
      "Epoch 255/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0135 - val_loss: 0.0116\n",
      "Epoch 256/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0143 - val_loss: 0.0133\n",
      "Epoch 257/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0141 - val_loss: 0.0139\n",
      "Epoch 258/300\n",
      "269/269 [==============================] - 0s 97us/sample - loss: 0.0143 - val_loss: 0.0128\n",
      "Epoch 259/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0136 - val_loss: 0.0126\n",
      "Epoch 260/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0149 - val_loss: 0.0169\n",
      "Epoch 261/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0140 - val_loss: 0.0103\n",
      "Epoch 262/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0131 - val_loss: 0.0107\n",
      "Epoch 263/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0144 - val_loss: 0.0199\n",
      "Epoch 264/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0155 - val_loss: 0.0195\n",
      "Epoch 265/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0162 - val_loss: 0.0120\n",
      "Epoch 266/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0148 - val_loss: 0.0149\n",
      "Epoch 267/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0181 - val_loss: 0.0132\n",
      "Epoch 268/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0177 - val_loss: 0.0154\n",
      "Epoch 269/300\n",
      "269/269 [==============================] - 0s 98us/sample - loss: 0.0148 - val_loss: 0.0121\n",
      "Epoch 270/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0174 - val_loss: 0.0158\n",
      "Epoch 271/300\n",
      "269/269 [==============================] - 0s 97us/sample - loss: 0.0172 - val_loss: 0.0180\n",
      "Epoch 272/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0153 - val_loss: 0.0166\n",
      "Epoch 273/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0168 - val_loss: 0.0163\n",
      "Epoch 274/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0165 - val_loss: 0.0138\n",
      "Epoch 275/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0140 - val_loss: 0.0123\n",
      "Epoch 276/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0118 - val_loss: 0.0120\n",
      "Epoch 277/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0114 - val_loss: 0.0139\n",
      "Epoch 278/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0135 - val_loss: 0.0144\n",
      "Epoch 279/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0159 - val_loss: 0.0153\n",
      "Epoch 280/300\n",
      "269/269 [==============================] - 0s 110us/sample - loss: 0.0150 - val_loss: 0.0118\n",
      "Epoch 281/300\n",
      "269/269 [==============================] - 0s 123us/sample - loss: 0.0132 - val_loss: 0.0163\n",
      "Epoch 282/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0151 - val_loss: 0.0116\n",
      "Epoch 283/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0141 - val_loss: 0.0124\n",
      "Epoch 284/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0127 - val_loss: 0.0118\n",
      "Epoch 285/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0129 - val_loss: 0.0139\n",
      "Epoch 286/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0125 - val_loss: 0.0113\n",
      "Epoch 287/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0122 - val_loss: 0.0106\n",
      "Epoch 288/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0108 - val_loss: 0.0116\n",
      "Epoch 289/300\n",
      "269/269 [==============================] - 0s 102us/sample - loss: 0.0119 - val_loss: 0.0135\n",
      "Epoch 290/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0120 - val_loss: 0.0111\n",
      "Epoch 291/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0121 - val_loss: 0.0119\n",
      "Epoch 292/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0125 - val_loss: 0.0144\n",
      "Epoch 293/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0155 - val_loss: 0.0149\n",
      "Epoch 294/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0158 - val_loss: 0.0180\n",
      "Epoch 295/300\n",
      "269/269 [==============================] - 0s 106us/sample - loss: 0.0145 - val_loss: 0.0213\n",
      "Epoch 296/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0155 - val_loss: 0.0119\n",
      "Epoch 297/300\n",
      "269/269 [==============================] - 0s 99us/sample - loss: 0.0135 - val_loss: 0.0134\n",
      "Epoch 298/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0147 - val_loss: 0.0166\n",
      "Epoch 299/300\n",
      "269/269 [==============================] - 0s 100us/sample - loss: 0.0176 - val_loss: 0.0136\n",
      "Epoch 300/300\n",
      "269/269 [==============================] - 0s 104us/sample - loss: 0.0157 - val_loss: 0.0214\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=300,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4101884 , 0.478746  , 0.59686995, 0.65506124, 0.72547054,\n",
       "        0.8169584 , 0.86430216, 0.97631246, 1.0609456 ],\n",
       "       [0.85098755, 0.9993737 , 1.2375164 , 1.3175391 , 1.4837236 ,\n",
       "        1.6506553 , 1.7293684 , 1.9692249 , 2.1158955 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrite to multi-output for each quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(inp_shape, quantiles):\n",
    "    # clear previous sessions\n",
    "    K.clear_session()\n",
    "\n",
    "    inp = Input(inp_shape, name=\"input\")\n",
    "    x = inp\n",
    "    x = Dense(16)(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = Dense(2)(x)  # represents mu, sigma\n",
    "    \n",
    "    #out_q0 = Dense(1, name=\"q0\")(x)  # DistributionLayer(quantile=quantiles[0])(x)\n",
    "    out_q0 = Dense(1, name=\"q0\")(x)  # DistributionLayer(quantile=quantiles[0])(x)\n",
    "    out_q1 = Dense(1, name=\"q1\")(x)  # DistributionLayer(quantile=quantiles[0])(x)\n",
    "    out_q2 = Dense(1, name=\"q2\")(x)  # DistributionLayer(quantile=quantiles[0])(x)\n",
    "    out_q3 = Dense(1, name=\"q3\")(x)  # DistributionLayer(quantile=quantiles[0])(x)\n",
    "    out_q4 = Dense(1, name=\"q4\")(x)  # DistributionLayer(quantile=quantiles[0])(x)\n",
    "    out_q5 = Dense(1, name=\"q5\")(x)  # DistributionLayer(quantile=quantiles[0])(x)\n",
    "    out_q6 = Dense(1, name=\"q6\")(x)  # DistributionLayer(quantile=quantiles[0])(x)\n",
    "    out_q7 = Dense(1, name=\"q7\")(x)  # DistributionLayer(quantile=quantiles[0])(x)\n",
    "    out_q8 = Dense(1, name=\"q8\")(x)  # DistributionLayer(quantile=quantiles[0])(x)\n",
    "    \n",
    "#     out_q0 = DistributionLayer(quantiles[0],x)\n",
    "#     out_q1 = DistributionLayer(quantiles[1],x)\n",
    "#     out_q2 = DistributionLayer(quantiles[2],x)\n",
    "#     out_q3 = DistributionLayer(quantiles[3],x)\n",
    "#     out_q4 = DistributionLayer(quantiles[4],x)\n",
    "#     out_q5 = DistributionLayer(quantiles[5],x)\n",
    "#     out_q6 = DistributionLayer(quantiles[6],x)\n",
    "#     out_q7 = DistributionLayer(quantiles[7],x)\n",
    "#     out_q8 = DistributionLayer(quantiles[8],x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=[out_q0, out_q1, out_q2, out_q3, out_q4, out_q5, out_q6, out_q7, out_q8])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           176         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           544         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           2112        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            130         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "q0 (Dense)                      (None, 1)            3           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "q1 (Dense)                      (None, 1)            3           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "q2 (Dense)                      (None, 1)            3           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "q3 (Dense)                      (None, 1)            3           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "q4 (Dense)                      (None, 1)            3           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "q5 (Dense)                      (None, 1)            3           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "q6 (Dense)                      (None, 1)            3           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "q7 (Dense)                      (None, 1)            3           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "q8 (Dense)                      (None, 1)            3           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,989\n",
      "Trainable params: 2,989\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(inp_shape=(train_df.columns.size,), quantiles=quantiles)\n",
    "model.compile(optimizer=\"adam\", loss=\"MAE\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q0': array([0.77976871, 0.4255634 , 0.41740025, 0.42903284, 0.82754025,\n",
       "        0.4064456 , 0.40616237, 0.45264247, 0.41096554, 0.41689168,\n",
       "        0.38520678, 0.41300217, 0.40747019, 0.86864485, 0.43559714,\n",
       "        0.42313449, 0.41549744, 0.43160425, 0.4241328 , 0.39397524,\n",
       "        0.45516087, 0.92357664, 0.41138681, 0.4042906 , 0.84447597,\n",
       "        0.42392267, 0.7718635 , 0.44973176, 0.43213789, 0.84063285,\n",
       "        0.82372458, 0.78483486, 0.41210167, 0.43546165, 0.92956534,\n",
       "        0.44674431, 0.40117869, 0.44582427, 0.83591121, 0.87259832,\n",
       "        0.90353443, 0.42208748, 0.38899553, 0.45153601, 0.39837333,\n",
       "        0.41525051, 0.42364229, 0.86586664, 0.40607733, 0.41816087,\n",
       "        0.38255025, 0.89064516, 0.42218558, 0.42119283, 0.40201799,\n",
       "        0.90192745, 0.40566299, 0.42338319, 0.41504695, 0.43416054,\n",
       "        0.40994688, 0.41956467, 0.86222354, 0.40953107, 0.87499344,\n",
       "        0.43268808, 0.40363463, 0.43650302, 0.41236164, 0.91755915,\n",
       "        0.84431159, 0.79940419, 0.41597326, 0.41392266, 0.42782747,\n",
       "        0.39393664, 0.89695079, 0.86224126, 0.43617093, 0.85140217,\n",
       "        0.42278976, 0.39106672, 0.41890434, 0.39874521, 0.41975423,\n",
       "        0.89906273, 0.45000559, 0.40947942, 0.44329988, 0.42845406,\n",
       "        0.432556  , 0.39379119, 0.40501631, 0.41589453, 0.78193413,\n",
       "        0.42418178, 0.40707033, 0.41339733, 0.43810726, 0.41006597,\n",
       "        0.43851828, 0.85018333, 0.42148472, 0.43332995, 0.83816669,\n",
       "        0.4207673 , 0.90264225, 0.44033269, 0.80576037, 0.84932577,\n",
       "        0.43348768, 0.83992371, 0.39006657, 0.88018906, 0.43101839,\n",
       "        0.44261483, 0.83254959, 0.40761605, 0.42398747, 0.39113956,\n",
       "        0.79373536, 0.4129318 , 0.90506873, 0.84718589, 0.85837079,\n",
       "        0.83941267, 0.88721045, 0.4195636 , 0.42338891, 0.44482469,\n",
       "        0.44546948, 0.43602464, 0.44134388, 0.89297545, 0.83983143,\n",
       "        0.4293607 , 0.43347221, 0.41334255, 0.84876859, 0.43941201,\n",
       "        0.44940611, 0.41955864, 0.85634571, 0.42355538, 0.43945227,\n",
       "        0.41727819, 0.42498001, 0.42266657, 0.42266616, 0.41148532,\n",
       "        0.88900356, 0.83327412, 0.85083557, 0.36450726, 0.42908377,\n",
       "        0.8729209 , 0.84048098, 0.40453602, 0.81843599, 0.85376998,\n",
       "        0.41641431, 0.41676266, 0.87021769, 0.41394461, 0.42832182,\n",
       "        0.41310186, 0.41093871, 0.85118556, 0.45711055, 0.41131593,\n",
       "        0.42535109, 0.42494805, 0.42291034, 0.44941366, 0.39662125,\n",
       "        0.41934535, 0.43803166, 0.42816385, 0.44481512, 0.40592902,\n",
       "        0.44510568, 0.87551584, 0.43882955, 0.40980793, 0.39726162,\n",
       "        0.83865186, 0.82391532, 0.43606232, 0.903867  , 0.42598936,\n",
       "        0.39335341, 0.87932989, 0.88167455, 0.92069727, 0.88504133,\n",
       "        0.41160328, 0.82383574, 0.41669152, 0.8902058 , 0.86226759,\n",
       "        0.42570417, 0.42333129, 0.41146715, 0.40853873, 0.87497931,\n",
       "        0.41888769, 0.4174933 , 0.40137973, 0.41868108, 0.41480139,\n",
       "        0.41762532, 0.43417065, 0.83195903, 0.42012424, 0.88049122,\n",
       "        0.43808452, 0.43475285, 0.37609209, 0.83488053, 0.39741986,\n",
       "        0.45581282, 0.87479406, 0.41141341, 0.41079624, 0.43100557,\n",
       "        0.37878448, 0.40052723, 0.41471736, 0.41684838, 0.43305646,\n",
       "        0.37894753, 0.4310462 , 0.88510673, 0.91283571, 0.90124721,\n",
       "        0.39263573, 0.41654587, 0.8974081 , 0.42264731, 0.42741047,\n",
       "        0.42512897, 0.42810603, 0.81919692, 0.84746617, 0.42715475,\n",
       "        0.39471582, 0.41054117, 0.41730352, 0.41887935, 0.41252063,\n",
       "        0.39940721, 0.90601885, 0.44086474, 0.42635516, 0.41635982,\n",
       "        0.89206673, 0.4321597 , 0.38922181, 0.81370253, 0.41180009,\n",
       "        0.42916062, 0.8504008 , 0.43751243, 0.45314441, 0.43264675,\n",
       "        0.81895532, 0.83909076, 0.3941779 , 0.41131961]),\n",
       " 'q1': array([0.98694101, 0.49009461, 0.49216885, 0.49557415, 1.01162224,\n",
       "        0.50957932, 0.49744197, 0.51033935, 0.48771643, 0.49897617,\n",
       "        0.48736895, 0.48874831, 0.49512264, 1.01446794, 0.49360654,\n",
       "        0.5011236 , 0.49018645, 0.48991859, 0.50036596, 0.49762297,\n",
       "        0.49253734, 1.05807856, 0.50273478, 0.48129021, 0.99869867,\n",
       "        0.49927749, 1.00492484, 0.50894352, 0.50124206, 1.00702438,\n",
       "        1.0056577 , 0.98704648, 0.47556742, 0.50901135, 1.03540212,\n",
       "        0.49528869, 0.49726181, 0.50423386, 1.01871513, 1.00822702,\n",
       "        1.00746122, 0.48079287, 0.48451008, 0.51382279, 0.50449866,\n",
       "        0.49952134, 0.4781263 , 1.02966442, 0.47676185, 0.50073502,\n",
       "        0.48975022, 1.02514877, 0.47741895, 0.50557659, 0.47679472,\n",
       "        1.02220408, 0.48937788, 0.51426022, 0.49339485, 0.50352995,\n",
       "        0.47363033, 0.48989425, 1.00843135, 0.50756214, 1.02545345,\n",
       "        0.51216801, 0.48720753, 0.50620415, 0.49156689, 1.02284072,\n",
       "        1.00312264, 0.98599412, 0.49582001, 0.48670795, 0.49725651,\n",
       "        0.51115274, 1.0135584 , 1.00080876, 0.51531796, 1.03367328,\n",
       "        0.49577904, 0.47843734, 0.48073988, 0.48792381, 0.47876208,\n",
       "        1.05745382, 0.52552364, 0.49381206, 0.50296196, 0.5092961 ,\n",
       "        0.49874424, 0.49395363, 0.50255466, 0.50275193, 0.96459871,\n",
       "        0.48562709, 0.47127788, 0.49053995, 0.49197004, 0.48347861,\n",
       "        0.51034104, 0.9515071 , 0.49285103, 0.50164632, 0.9912556 ,\n",
       "        0.51201887, 1.02047253, 0.50266385, 1.00514551, 1.02315231,\n",
       "        0.50567869, 0.98625531, 0.48620697, 0.98859376, 0.49709315,\n",
       "        0.49514335, 0.9951382 , 0.49400333, 0.50803919, 0.45791431,\n",
       "        0.99710541, 0.50437562, 1.00353863, 1.01851988, 1.01320094,\n",
       "        1.03688206, 1.01064562, 0.4876326 , 0.49487272, 0.49730388,\n",
       "        0.49577792, 0.50022169, 0.50017166, 1.02817656, 1.01071715,\n",
       "        0.4923242 , 0.50834872, 0.50602337, 1.01779662, 0.51028976,\n",
       "        0.50552597, 0.4913838 , 0.99181463, 0.48212265, 0.5041746 ,\n",
       "        0.50426376, 0.49784764, 0.50485703, 0.50799094, 0.48854629,\n",
       "        1.00389055, 1.02390523, 0.99702205, 0.49905009, 0.50058671,\n",
       "        0.988371  , 0.98308459, 0.49724383, 0.9584036 , 1.04166762,\n",
       "        0.49458875, 0.4937687 , 1.01964101, 0.49731282, 0.50025734,\n",
       "        0.49637799, 0.48975453, 1.01972863, 0.49900831, 0.51659863,\n",
       "        0.49381212, 0.51276073, 0.49783404, 0.49254733, 0.48678729,\n",
       "        0.48216814, 0.49518275, 0.50301859, 0.50641068, 0.48711808,\n",
       "        0.50619097, 1.00885207, 0.51158332, 0.493986  , 0.49348383,\n",
       "        0.99602502, 0.99890815, 0.5113605 , 1.02705827, 0.49046472,\n",
       "        0.48983167, 1.04348895, 1.0209626 , 1.04413318, 1.01637382,\n",
       "        0.49396519, 1.00992583, 0.48080134, 1.03971773, 1.0195359 ,\n",
       "        0.50181136, 0.50765927, 0.48838422, 0.48219274, 1.03215824,\n",
       "        0.50128541, 0.50672073, 0.48209268, 0.49500284, 0.50020777,\n",
       "        0.50038114, 0.50654676, 1.00557768, 0.49480572, 0.98793313,\n",
       "        0.50006588, 0.49145004, 0.4907908 , 1.03259482, 0.50191482,\n",
       "        0.50186779, 1.00173852, 0.49176462, 0.49286449, 0.49103959,\n",
       "        0.454028  , 0.48626906, 0.50566403, 0.49470059, 0.49883381,\n",
       "        0.48859335, 0.49235676, 0.99662842, 1.00860885, 1.01120466,\n",
       "        0.50197417, 0.47646561, 1.00163566, 0.49193431, 0.4881685 ,\n",
       "        0.48211356, 0.51494388, 1.00079741, 1.03201598, 0.50704743,\n",
       "        0.48413271, 0.4891214 , 0.50328069, 0.50908513, 0.49309843,\n",
       "        0.50374785, 1.01776805, 0.50391482, 0.4974368 , 0.48559929,\n",
       "        1.01745078, 0.49532064, 0.49530214, 1.00560392, 0.48604815,\n",
       "        0.5079594 , 1.03177231, 0.4954174 , 0.50915667, 0.49112741,\n",
       "        0.98531047, 1.030373  , 0.49810812, 0.49329876]),\n",
       " 'q2': array([1.23415944, 0.62403202, 0.62149103, 0.62300947, 1.26217082,\n",
       "        0.62762769, 0.62297613, 0.62618969, 0.61528915, 0.60759722,\n",
       "        0.62185884, 0.61635795, 0.61774114, 1.24545958, 0.62086249,\n",
       "        0.63097009, 0.62455689, 0.62501272, 0.62425812, 0.61958445,\n",
       "        0.62107551, 1.25297968, 0.63520761, 0.61876652, 1.24592338,\n",
       "        0.62841592, 1.26300544, 0.6191478 , 0.62087474, 1.24998631,\n",
       "        1.24402391, 1.25977008, 0.63122105, 0.62802461, 1.27008318,\n",
       "        0.63156619, 0.62309866, 0.61808352, 1.26380532, 1.25014126,\n",
       "        1.25052403, 0.61524934, 0.62261617, 0.62218547, 0.62446789,\n",
       "        0.62792731, 0.61535662, 1.26161977, 0.63268795, 0.62502217,\n",
       "        0.61358015, 1.25408356, 0.61417772, 0.6187807 , 0.62082149,\n",
       "        1.25556961, 0.62059392, 0.63016314, 0.61566804, 0.62620342,\n",
       "        0.62485769, 0.61284626, 1.25019815, 0.62844213, 1.24936877,\n",
       "        0.62590049, 0.61753697, 0.62757603, 0.62059209, 1.25260924,\n",
       "        1.25127986, 1.25972421, 0.61954638, 0.6219585 , 0.6251437 ,\n",
       "        0.61962757, 1.25075908, 1.26367542, 0.63655936, 1.26564917,\n",
       "        0.61661823, 0.61760989, 0.61921294, 0.62597898, 0.63110533,\n",
       "        1.28091955, 0.62643384, 0.61875223, 0.62327298, 0.63437493,\n",
       "        0.61764525, 0.62440675, 0.62818242, 0.61168596, 1.25598814,\n",
       "        0.62785221, 0.63263717, 0.6362003 , 0.62902647, 0.62689884,\n",
       "        0.63049301, 1.23807505, 0.62119081, 0.62525419, 1.24636912,\n",
       "        0.61818913, 1.25492035, 0.62320071, 1.24944229, 1.25072259,\n",
       "        0.63032966, 1.23606224, 0.6225696 , 1.24354047, 0.62096936,\n",
       "        0.61900264, 1.25995944, 0.62769148, 0.62463672, 0.61376801,\n",
       "        1.24298732, 0.62407361, 1.24711397, 1.26567477, 1.25588174,\n",
       "        1.25179153, 1.2545534 , 0.62665707, 0.63109202, 0.6241268 ,\n",
       "        0.61305931, 0.63568395, 0.61551026, 1.25921581, 1.24369283,\n",
       "        0.63489319, 0.63133156, 0.62466468, 1.2522283 , 0.62784165,\n",
       "        0.62538753, 0.6224903 , 1.24460801, 0.61645724, 0.6207517 ,\n",
       "        0.61960804, 0.61947945, 0.62512051, 0.62509865, 0.62387299,\n",
       "        1.24593917, 1.25183447, 1.25118721, 0.62087067, 0.62848196,\n",
       "        1.25838881, 1.24631658, 0.62873451, 1.23836776, 1.28015594,\n",
       "        0.62710721, 0.62954164, 1.2469327 , 0.62730045, 0.62263382,\n",
       "        0.62645677, 0.62458252, 1.25597435, 0.62666851, 0.62180764,\n",
       "        0.6237753 , 0.63216437, 0.63072   , 0.62137434, 0.61358608,\n",
       "        0.62738811, 0.62285963, 0.6182879 , 0.62145093, 0.61782083,\n",
       "        0.63708837, 1.24535217, 0.63060131, 0.61901192, 0.63018767,\n",
       "        1.25103069, 1.24506989, 0.63763215, 1.24530069, 0.62061272,\n",
       "        0.62626489, 1.26568293, 1.25222736, 1.27266938, 1.27394426,\n",
       "        0.62286346, 1.24133757, 0.62443488, 1.26655064, 1.24946391,\n",
       "        0.62046132, 0.63028102, 0.60989389, 0.62220374, 1.27361788,\n",
       "        0.62416329, 0.62252152, 0.61191324, 0.62704227, 0.62203599,\n",
       "        0.61010957, 0.62473649, 1.26372783, 0.62091589, 1.25416445,\n",
       "        0.62243683, 0.62823039, 0.62325411, 1.26540977, 0.63103205,\n",
       "        0.61854779, 1.25410241, 0.61453691, 0.61191762, 0.61421155,\n",
       "        0.62780954, 0.62538423, 0.63184065, 0.62216598, 0.63340651,\n",
       "        0.62276536, 0.63135018, 1.25964817, 1.25504678, 1.25336523,\n",
       "        0.6208399 , 0.62564658, 1.27201883, 0.62142732, 0.61582923,\n",
       "        0.62132782, 0.62427135, 1.24520786, 1.27288314, 0.63639293,\n",
       "        0.6201414 , 0.61925565, 0.63652999, 0.62532106, 0.61005924,\n",
       "        0.62440197, 1.26007469, 0.62045531, 0.62473529, 0.62820411,\n",
       "        1.25797305, 0.61559212, 0.62824833, 1.25897925, 0.6230901 ,\n",
       "        0.62864541, 1.25387655, 0.62733589, 0.62673391, 0.61007945,\n",
       "        1.23435803, 1.2576428 , 0.62595282, 0.63103304]),\n",
       " 'q3': array([1.31723581, 0.66033059, 0.6592753 , 0.66133424, 1.3320228 ,\n",
       "        0.66096545, 0.66778587, 0.66731533, 0.65420575, 0.65171159,\n",
       "        0.66240246, 0.65164507, 0.65922365, 1.32561766, 0.6677954 ,\n",
       "        0.66781545, 0.66213692, 0.66217674, 0.66500155, 0.66005076,\n",
       "        0.662469  , 1.32945328, 0.67289112, 0.65365688, 1.3248756 ,\n",
       "        0.66823926, 1.33438623, 0.66103369, 0.66224226, 1.32742574,\n",
       "        1.31016559, 1.33532641, 0.66370602, 0.66152799, 1.33202984,\n",
       "        0.66714988, 0.66181973, 0.65787594, 1.33929344, 1.32268346,\n",
       "        1.32921383, 0.66191331, 0.66761227, 0.66658659, 0.66963971,\n",
       "        0.66364068, 0.65155188, 1.33274491, 0.66466645, 0.66832897,\n",
       "        0.65979445, 1.33403622, 0.64902849, 0.65692324, 0.66415054,\n",
       "        1.32694221, 0.65699836, 0.66900737, 0.65501408, 0.6667991 ,\n",
       "        0.66460612, 0.65276886, 1.33644125, 0.66673362, 1.31697297,\n",
       "        0.6638489 , 0.65916382, 0.66533093, 0.66289015, 1.32582536,\n",
       "        1.3309913 , 1.32398213, 0.65848908, 0.65736769, 0.66822425,\n",
       "        0.65469048, 1.33712331, 1.34322155, 0.66833558, 1.34623098,\n",
       "        0.65422535, 0.65770452, 0.65885716, 0.66361448, 0.6726797 ,\n",
       "        1.34852505, 0.67024972, 0.66091273, 0.66475046, 0.66330258,\n",
       "        0.65851398, 0.66524878, 0.66746875, 0.65518494, 1.3307843 ,\n",
       "        0.66485312, 0.66801475, 0.67620529, 0.66060127, 0.66544112,\n",
       "        0.66273628, 1.32585813, 0.6576778 , 0.66715572, 1.32948766,\n",
       "        0.65810468, 1.3366208 , 0.66305498, 1.32771181, 1.32233661,\n",
       "        0.67460366, 1.31917846, 0.6563776 , 1.33086716, 0.66804441,\n",
       "        0.65868687, 1.33316089, 0.66264201, 0.66013637, 0.6545152 ,\n",
       "        1.30599382, 0.66380807, 1.33689595, 1.33920802, 1.33262933,\n",
       "        1.3330205 , 1.34104905, 0.66017325, 0.66974601, 0.65960514,\n",
       "        0.65511992, 0.66711553, 0.66115331, 1.34198528, 1.31341776,\n",
       "        0.66610277, 0.67507959, 0.66490458, 1.31449508, 0.66491295,\n",
       "        0.65974554, 0.66519467, 1.31906406, 0.65585645, 0.6584035 ,\n",
       "        0.66022654, 0.65510033, 0.66430143, 0.6632202 , 0.66658849,\n",
       "        1.31791209, 1.33595034, 1.34569084, 0.65889827, 0.66207737,\n",
       "        1.3341927 , 1.32643953, 0.66569416, 1.31515611, 1.34801759,\n",
       "        0.66101623, 0.66520537, 1.31971292, 0.66417882, 0.65952361,\n",
       "        0.66037153, 0.66578517, 1.34019329, 0.66350399, 0.66137706,\n",
       "        0.66806438, 0.6658318 , 0.67077618, 0.6613522 , 0.6546362 ,\n",
       "        0.67384116, 0.65473279, 0.65900636, 0.65504968, 0.66114336,\n",
       "        0.66902905, 1.32166958, 0.66940348, 0.65616147, 0.67037101,\n",
       "        1.32138349, 1.3215389 , 0.67487097, 1.32076563, 0.65212862,\n",
       "        0.66742075, 1.33618606, 1.32276201, 1.35912042, 1.35240622,\n",
       "        0.66038175, 1.31726581, 0.6686082 , 1.3206293 , 1.33427645,\n",
       "        0.65293161, 0.6672563 , 0.6546146 , 0.65407474, 1.34048064,\n",
       "        0.67135263, 0.66006081, 0.66097128, 0.66393153, 0.65635722,\n",
       "        0.65295342, 0.66215016, 1.33118645, 0.65886816, 1.31912821,\n",
       "        0.65940915, 0.66337067, 0.66303018, 1.34356549, 0.67124209,\n",
       "        0.65403181, 1.34054726, 0.65593613, 0.6507664 , 0.65117838,\n",
       "        0.67030755, 0.66736413, 0.67614335, 0.65581685, 0.66430211,\n",
       "        0.66369168, 0.66810155, 1.33207219, 1.32588064, 1.33476591,\n",
       "        0.66382865, 0.6610979 , 1.34380321, 0.66258363, 0.6487662 ,\n",
       "        0.66337552, 0.66072435, 1.31717271, 1.34536928, 0.66856589,\n",
       "        0.66457383, 0.66321706, 0.67634305, 0.6615422 , 0.64971732,\n",
       "        0.66153683, 1.34106452, 0.66314342, 0.66124236, 0.66651766,\n",
       "        1.34624637, 0.6603903 , 0.66258055, 1.34134286, 0.66270542,\n",
       "        0.66795638, 1.31890679, 0.66617373, 0.66740451, 0.64821642,\n",
       "        1.30644805, 1.3262954 , 0.66118063, 0.67123138]),\n",
       " 'q4': array([1.5100656 , 0.74822681, 0.74982045, 0.75453434, 1.48676496,\n",
       "        0.74195801, 0.74741724, 0.75372374, 0.73928988, 0.74428014,\n",
       "        0.74841454, 0.7402475 , 0.75118736, 1.51373431, 0.75832049,\n",
       "        0.75932027, 0.74863458, 0.74617607, 0.7476841 , 0.74723838,\n",
       "        0.74832949, 1.49270971, 0.75674059, 0.74724952, 1.48864857,\n",
       "        0.74385411, 1.5126433 , 0.75057122, 0.74357343, 1.49620184,\n",
       "        1.49606645, 1.49800857, 0.75009822, 0.74819103, 1.50686467,\n",
       "        0.76178339, 0.7480796 , 0.73696697, 1.49544974, 1.49892692,\n",
       "        1.49940888, 0.74818176, 0.75018603, 0.75238629, 0.75024151,\n",
       "        0.74326155, 0.73681029, 1.50080102, 0.75455704, 0.74996131,\n",
       "        0.74648846, 1.50887967, 0.7460205 , 0.74765216, 0.7569674 ,\n",
       "        1.48781365, 0.74770215, 0.75297981, 0.74505495, 0.75490098,\n",
       "        0.74925689, 0.74441207, 1.51619391, 0.75105289, 1.49471673,\n",
       "        0.7533792 , 0.74630323, 0.75198586, 0.7493437 , 1.49026591,\n",
       "        1.51197556, 1.49152473, 0.74839996, 0.74647735, 0.7568787 ,\n",
       "        0.75023516, 1.51007932, 1.4965232 , 0.75587967, 1.51019802,\n",
       "        0.73570152, 0.74849736, 0.74856966, 0.73730822, 0.75691439,\n",
       "        1.50906947, 0.74903076, 0.75224535, 0.75254082, 0.74963723,\n",
       "        0.74670426, 0.75051351, 0.75115476, 0.74403247, 1.49298817,\n",
       "        0.74607242, 0.75038284, 0.75572511, 0.74872104, 0.75540265,\n",
       "        0.75343217, 1.49260006, 0.74874655, 0.75217636, 1.50857724,\n",
       "        0.74479666, 1.51092051, 0.74942527, 1.4988225 , 1.50428838,\n",
       "        0.76124171, 1.50560036, 0.74469012, 1.49965766, 0.74756264,\n",
       "        0.74311585, 1.50517282, 0.74774004, 0.74923508, 0.74408483,\n",
       "        1.47423323, 0.74874573, 1.50559785, 1.49419114, 1.51098044,\n",
       "        1.50134111, 1.50331097, 0.75413766, 0.76075745, 0.74762835,\n",
       "        0.74711808, 0.75437177, 0.75171942, 1.50968405, 1.48936262,\n",
       "        0.75215321, 0.76664192, 0.74177192, 1.48187877, 0.75107274,\n",
       "        0.74778019, 0.74837335, 1.50432296, 0.74563251, 0.74562415,\n",
       "        0.74002898, 0.74752009, 0.7493414 , 0.7485994 , 0.75502846,\n",
       "        1.49271667, 1.50673622, 1.51788811, 0.74385506, 0.74322669,\n",
       "        1.50363757, 1.49836031, 0.75259109, 1.49110143, 1.50246998,\n",
       "        0.74908291, 0.74870296, 1.49136378, 0.75196171, 0.75640562,\n",
       "        0.75171093, 0.74799322, 1.49889604, 0.74832334, 0.74511182,\n",
       "        0.75463159, 0.75173232, 0.75582459, 0.74311302, 0.7496083 ,\n",
       "        0.76199825, 0.74690747, 0.74902564, 0.73911585, 0.75627819,\n",
       "        0.75291877, 1.50538424, 0.75408822, 0.7510456 , 0.74941204,\n",
       "        1.48848485, 1.49148319, 0.76275596, 1.47873013, 0.75139125,\n",
       "        0.74568479, 1.50660153, 1.49483216, 1.51510901, 1.51044308,\n",
       "        0.74153067, 1.47742769, 0.75512174, 1.49839525, 1.51368102,\n",
       "        0.74921602, 0.75062455, 0.74936644, 0.747576  , 1.50401196,\n",
       "        0.76040851, 0.74518887, 0.74769309, 0.74842496, 0.7510373 ,\n",
       "        0.75083463, 0.75335919, 1.48708747, 0.74897388, 1.49382568,\n",
       "        0.74548741, 0.74822118, 0.74118576, 1.50327553, 0.75722457,\n",
       "        0.74586819, 1.51626112, 0.7461525 , 0.74149261, 0.7422184 ,\n",
       "        0.74813799, 0.75064743, 0.74831977, 0.74938803, 0.75255957,\n",
       "        0.75662137, 0.76090278, 1.49971652, 1.50748914, 1.48000432,\n",
       "        0.74478527, 0.76033179, 1.50659412, 0.74943245, 0.7410154 ,\n",
       "        0.75659307, 0.74394757, 1.49128421, 1.51573975, 0.74914677,\n",
       "        0.75643219, 0.7465252 , 0.75736845, 0.74899783, 0.73108062,\n",
       "        0.74868915, 1.49972606, 0.74659186, 0.75286189, 0.75147106,\n",
       "        1.50323227, 0.74059542, 0.74878979, 1.50291545, 0.74695471,\n",
       "        0.74733893, 1.49416699, 0.75304884, 0.75517107, 0.74023395,\n",
       "        1.4892553 , 1.49989782, 0.75330607, 0.75011839]),\n",
       " 'q5': array([1.67374622, 0.84016758, 0.84377537, 0.84576592, 1.65148878,\n",
       "        0.83085102, 0.83258688, 0.83810715, 0.83428072, 0.83026302,\n",
       "        0.84116677, 0.83240735, 0.83514995, 1.68687976, 0.84331779,\n",
       "        0.83916233, 0.83815162, 0.82966707, 0.8359668 , 0.83993458,\n",
       "        0.84155963, 1.67383697, 0.84273922, 0.83808839, 1.67856803,\n",
       "        0.83554821, 1.68606898, 0.83570981, 0.83211353, 1.66278769,\n",
       "        1.66414264, 1.66801022, 0.84494573, 0.83894649, 1.66732913,\n",
       "        0.84215705, 0.83536403, 0.83355667, 1.66494027, 1.65810229,\n",
       "        1.68371541, 0.83776667, 0.83457828, 0.83782458, 0.84414499,\n",
       "        0.83475972, 0.82915522, 1.67647544, 0.83958574, 0.84059994,\n",
       "        0.83626011, 1.68541776, 0.84027952, 0.83723327, 0.84303092,\n",
       "        1.66660701, 0.83869018, 0.83932062, 0.82420387, 0.83892582,\n",
       "        0.82908067, 0.83180874, 1.67326323, 0.82707661, 1.6718176 ,\n",
       "        0.83465635, 0.83419748, 0.845422  , 0.84185962, 1.67027819,\n",
       "        1.67074884, 1.66004558, 0.83759065, 0.83138418, 0.83997861,\n",
       "        0.84001279, 1.68315213, 1.65842053, 0.8409922 , 1.67429627,\n",
       "        0.8312459 , 0.84042881, 0.83332312, 0.82870832, 0.84266563,\n",
       "        1.69137551, 0.85123817, 0.83740963, 0.83204024, 0.82660811,\n",
       "        0.83344016, 0.84483664, 0.83510273, 0.82912734, 1.66916166,\n",
       "        0.83499132, 0.83327394, 0.84671232, 0.82934602, 0.84010137,\n",
       "        0.83818508, 1.65302078, 0.8374185 , 0.84323036, 1.68069173,\n",
       "        0.8360008 , 1.67653258, 0.83389361, 1.65905407, 1.6676716 ,\n",
       "        0.84285312, 1.66716817, 0.82878886, 1.66260041, 0.84127541,\n",
       "        0.82956244, 1.68200664, 0.83943353, 0.82913265, 0.83468833,\n",
       "        1.63521292, 0.83913974, 1.67074601, 1.66700145, 1.6711549 ,\n",
       "        1.67154105, 1.67200287, 0.8433452 , 0.84874213, 0.83266817,\n",
       "        0.83285057, 0.83787423, 0.84039823, 1.67897274, 1.65874168,\n",
       "        0.84012594, 0.84689386, 0.83817443, 1.66587953, 0.83355136,\n",
       "        0.83440044, 0.83297253, 1.66101161, 0.82740477, 0.8381581 ,\n",
       "        0.83065399, 0.82971237, 0.84531651, 0.83523624, 0.83877731,\n",
       "        1.66205052, 1.67436915, 1.67596083, 0.83050899, 0.82452839,\n",
       "        1.67232324, 1.66768601, 0.84127404, 1.65717636, 1.67587677,\n",
       "        0.83453713, 0.83805277, 1.64246721, 0.83458854, 0.84563534,\n",
       "        0.84364949, 0.8266728 , 1.67107827, 0.83460236, 0.83040486,\n",
       "        0.84810799, 0.8376577 , 0.84464552, 0.83263149, 0.83477493,\n",
       "        0.85160644, 0.83166874, 0.84011747, 0.82815917, 0.84080626,\n",
       "        0.8396329 , 1.67709702, 0.84272726, 0.83485546, 0.83085369,\n",
       "        1.65952495, 1.67130351, 0.84549281, 1.63869719, 0.84532734,\n",
       "        0.84008748, 1.66917613, 1.6737096 , 1.67477079, 1.68758859,\n",
       "        0.83269288, 1.65345508, 0.84084529, 1.66959166, 1.67930212,\n",
       "        0.8365798 , 0.83668538, 0.83633476, 0.83429344, 1.65746148,\n",
       "        0.84134752, 0.83633899, 0.8283425 , 0.83903473, 0.84169868,\n",
       "        0.83640679, 0.84298699, 1.66495946, 0.83922594, 1.65045166,\n",
       "        0.83106007, 0.83726643, 0.83399012, 1.66074016, 0.83445624,\n",
       "        0.83591717, 1.66180131, 0.83586845, 0.8304199 , 0.8332543 ,\n",
       "        0.84021033, 0.83521673, 0.83700425, 0.83969305, 0.83293521,\n",
       "        0.84182247, 0.83598728, 1.66389834, 1.66536661, 1.65993064,\n",
       "        0.83782695, 0.85042064, 1.66969934, 0.83486962, 0.83235181,\n",
       "        0.85324856, 0.82959682, 1.65473583, 1.68408985, 0.82728989,\n",
       "        0.84095402, 0.83102794, 0.83964186, 0.83906084, 0.82575629,\n",
       "        0.83305459, 1.67743285, 0.84136574, 0.83739237, 0.84053846,\n",
       "        1.67126051, 0.82637456, 0.83912328, 1.67078518, 0.82639824,\n",
       "        0.83283376, 1.64965077, 0.84007524, 0.83684134, 0.82973529,\n",
       "        1.67283916, 1.66277595, 0.84290698, 0.83246499]),\n",
       " 'q6': array([1.74195185, 0.88105827, 0.88393236, 0.88144774, 1.72881422,\n",
       "        0.86958931, 0.87737013, 0.870177  , 0.87716683, 0.86648274,\n",
       "        0.87378011, 0.87740338, 0.87238128, 1.77059489, 0.88325817,\n",
       "        0.88135373, 0.87979495, 0.87489069, 0.87667536, 0.87364856,\n",
       "        0.87717292, 1.75138063, 0.87329689, 0.87293133, 1.75641213,\n",
       "        0.87624299, 1.75453076, 0.87162619, 0.87120566, 1.73896149,\n",
       "        1.74867578, 1.73046465, 0.8823514 , 0.88385679, 1.74852877,\n",
       "        0.87932942, 0.87561409, 0.86991776, 1.7422114 , 1.7323857 ,\n",
       "        1.76625405, 0.87609063, 0.87450039, 0.87549699, 0.88268638,\n",
       "        0.87070362, 0.87056279, 1.75262846, 0.87387565, 0.8758311 ,\n",
       "        0.87575085, 1.74383564, 0.87529135, 0.87792338, 0.88458725,\n",
       "        1.73977302, 0.88608564, 0.88149378, 0.86273461, 0.87469878,\n",
       "        0.86697847, 0.86850916, 1.74672578, 0.86706362, 1.74122136,\n",
       "        0.87310568, 0.87709287, 0.8860738 , 0.88246532, 1.74825171,\n",
       "        1.74443526, 1.73941911, 0.87858733, 0.87265869, 0.884272  ,\n",
       "        0.87299567, 1.76070122, 1.73471598, 0.88139814, 1.76830212,\n",
       "        0.86370436, 0.87679355, 0.86868839, 0.85991599, 0.88296048,\n",
       "        1.75789671, 0.88745414, 0.87517068, 0.87221801, 0.87139807,\n",
       "        0.87416334, 0.89259652, 0.86780013, 0.86965747, 1.74024412,\n",
       "        0.87673533, 0.87467894, 0.88066115, 0.87040804, 0.873279  ,\n",
       "        0.88021086, 1.73112744, 0.87133195, 0.87959003, 1.76377954,\n",
       "        0.87732488, 1.74691904, 0.87635708, 1.7453755 , 1.73834449,\n",
       "        0.88511977, 1.73854134, 0.86458425, 1.74433023, 0.88428226,\n",
       "        0.87252446, 1.75597381, 0.88239143, 0.87389175, 0.87875318,\n",
       "        1.73650723, 0.87422482, 1.74130868, 1.7345566 , 1.74717189,\n",
       "        1.74694905, 1.7565437 , 0.88051642, 0.88989865, 0.87105356,\n",
       "        0.87243723, 0.8739277 , 0.86669883, 1.74121565, 1.73233355,\n",
       "        0.88047019, 0.88365046, 0.88361439, 1.75095915, 0.87591348,\n",
       "        0.87175319, 0.86876639, 1.73799466, 0.86834474, 0.87681762,\n",
       "        0.87604765, 0.87320623, 0.88804147, 0.86757118, 0.87129337,\n",
       "        1.73633199, 1.74450832, 1.76497507, 0.88245147, 0.8614993 ,\n",
       "        1.736073  , 1.75066113, 0.88150412, 1.7313193 , 1.76217367,\n",
       "        0.87284392, 0.87994989, 1.73512721, 0.87462341, 0.88075811,\n",
       "        0.883489  , 0.86412674, 1.73720331, 0.86711142, 0.86002466,\n",
       "        0.88521444, 0.87538368, 0.88204357, 0.87229161, 0.87643746,\n",
       "        0.89244296, 0.87278923, 0.88014383, 0.86624509, 0.88197603,\n",
       "        0.87744073, 1.7466918 , 0.88378493, 0.88146789, 0.86500443,\n",
       "        1.73381806, 1.75569769, 0.88224558, 1.72377405, 0.87783526,\n",
       "        0.87726799, 1.74667657, 1.7367824 , 1.74830866, 1.74945167,\n",
       "        0.87528341, 1.72815448, 0.88328333, 1.74885424, 1.7468889 ,\n",
       "        0.87787745, 0.88058709, 0.87861279, 0.87095332, 1.74547462,\n",
       "        0.87364571, 0.87141496, 0.87110652, 0.88287184, 0.87867683,\n",
       "        0.874668  , 0.87776895, 1.73824319, 0.88298947, 1.72427147,\n",
       "        0.87402945, 0.87931899, 0.87255763, 1.72817853, 0.87575707,\n",
       "        0.87478053, 1.72736494, 0.87124377, 0.87271375, 0.87186566,\n",
       "        0.87607336, 0.87648262, 0.87796611, 0.87728218, 0.87624096,\n",
       "        0.88444005, 0.86838682, 1.73022123, 1.74146895, 1.73570596,\n",
       "        0.8807363 , 0.89093439, 1.7451223 , 0.87028341, 0.87088157,\n",
       "        0.88810889, 0.86836669, 1.72742244, 1.75308032, 0.87144793,\n",
       "        0.87977031, 0.87632772, 0.87352375, 0.87707365, 0.86950892,\n",
       "        0.88156983, 1.76149332, 0.87634989, 0.8789147 , 0.87284646,\n",
       "        1.73647405, 0.8733107 , 0.87477594, 1.73445427, 0.86678364,\n",
       "        0.87807068, 1.7210894 , 0.88180324, 0.86804214, 0.86498486,\n",
       "        1.74007212, 1.74106075, 0.88017806, 0.86469267]),\n",
       " 'q7': array([1.99980573, 1.01760656, 0.99053578, 0.99063677, 1.97885326,\n",
       "        1.00036719, 0.99631661, 0.9881006 , 0.99976005, 0.99902039,\n",
       "        1.00020853, 1.00764962, 0.99494261, 2.02823373, 0.98969791,\n",
       "        1.01234816, 0.99759951, 1.00903969, 1.01042375, 1.02117746,\n",
       "        1.00737723, 1.99068011, 0.97794512, 1.00224587, 2.00341201,\n",
       "        1.00160016, 1.99595481, 0.98498625, 0.98749433, 1.95822202,\n",
       "        1.95582568, 1.98084066, 1.01484992, 1.04367993, 1.98530635,\n",
       "        0.99938745, 1.00251671, 0.98134009, 1.97255996, 1.98526355,\n",
       "        2.01553436, 1.01672904, 1.00505446, 0.99876932, 0.99423887,\n",
       "        0.99582509, 1.00235845, 1.98470284, 1.00032018, 1.01571048,\n",
       "        0.99166418, 1.99012148, 1.01011105, 1.00589492, 1.00938668,\n",
       "        1.98759298, 1.00285863, 1.00817647, 0.98536608, 0.98850621,\n",
       "        0.99488704, 1.00669485, 1.98319699, 0.9977021 , 2.01359506,\n",
       "        1.00737551, 0.98897697, 0.99855032, 0.99038376, 1.96275629,\n",
       "        2.00901592, 1.95943738, 1.01209558, 1.00684425, 0.9925567 ,\n",
       "        1.00179211, 2.02184581, 1.97245536, 1.02008887, 2.02428412,\n",
       "        0.99205341, 0.98943681, 0.99180687, 0.99132538, 1.01878925,\n",
       "        2.00026576, 1.00713015, 1.00776277, 1.00191591, 1.00665869,\n",
       "        0.98574312, 1.00568525, 1.0136539 , 1.00226848, 1.95937298,\n",
       "        0.9992244 , 1.00800398, 1.01819503, 0.99770593, 1.01424949,\n",
       "        1.00207821, 1.99804417, 0.99210187, 1.00018871, 1.97613727,\n",
       "        0.99886212, 1.96794637, 1.01497239, 1.96532769, 1.96144898,\n",
       "        1.01407895, 1.99984596, 0.99083132, 2.00555769, 1.00870465,\n",
       "        1.00318143, 1.99170158, 1.00587332, 0.98734066, 1.00622121,\n",
       "        1.99426088, 1.00436466, 1.98588995, 1.94015586, 1.97283879,\n",
       "        1.99634705, 2.02636528, 1.01230443, 1.03311478, 1.00439846,\n",
       "        0.99255841, 0.99753594, 1.01011367, 2.00387077, 2.01019966,\n",
       "        0.98963825, 1.0020149 , 0.99790967, 1.96300849, 0.99818323,\n",
       "        0.99777196, 1.01319306, 1.9657046 , 0.99913134, 0.99401599,\n",
       "        1.01618564, 1.01966632, 1.00718065, 1.00024225, 1.00656151,\n",
       "        1.94179118, 1.97622269, 2.05694329, 1.01496445, 0.98961883,\n",
       "        1.97136704, 2.00111467, 1.02232651, 1.9878484 , 2.00021758,\n",
       "        0.98609303, 1.01142634, 2.00699137, 0.98911583, 0.99478366,\n",
       "        1.00987451, 0.99086484, 1.98964829, 1.01019716, 1.00190405,\n",
       "        1.02006446, 0.99540859, 1.02320276, 0.99633265, 1.01110598,\n",
       "        1.0191145 , 1.01733917, 0.99202398, 0.99435899, 1.02325424,\n",
       "        1.01662977, 1.98244338, 1.00787904, 1.00248475, 0.97972498,\n",
       "        1.96386133, 1.98377741, 0.99686956, 1.96267363, 1.00048684,\n",
       "        0.99039057, 2.04802845, 1.99057895, 2.02497607, 2.00365735,\n",
       "        0.9950518 , 2.01151557, 1.01498635, 2.00716659, 2.01081131,\n",
       "        1.00529962, 1.009514  , 1.00980838, 0.99760429, 2.02379052,\n",
       "        0.98735475, 0.98675214, 0.98907356, 1.00344388, 1.00586365,\n",
       "        0.99041192, 1.01954518, 1.99434188, 0.99389246, 2.00003118,\n",
       "        1.00865857, 1.00206268, 1.00260117, 2.01438601, 1.00939584,\n",
       "        1.00217101, 1.98078664, 0.99725272, 0.99192542, 0.9987111 ,\n",
       "        1.00870627, 1.012167  , 1.00158482, 1.00445355, 0.99890722,\n",
       "        1.01620254, 0.99014044, 2.02633474, 2.01020908, 1.98321154,\n",
       "        1.02574556, 1.01337612, 1.97206497, 0.99690538, 0.99409559,\n",
       "        1.0250761 , 0.99217708, 1.97185091, 1.99918504, 1.00196785,\n",
       "        1.01236041, 1.00727739, 1.02206626, 1.00973149, 1.01200497,\n",
       "        1.01198597, 2.02039766, 0.98619504, 1.00423432, 1.00704223,\n",
       "        1.96651259, 0.99978361, 1.00559699, 1.9935524 , 0.98312721,\n",
       "        1.00242942, 1.94024548, 1.01911256, 0.97996944, 1.00123463,\n",
       "        2.00354662, 1.9720583 , 0.99523263, 1.00401376]),\n",
       " 'q8': array([2.22013777, 1.08509588, 1.06001875, 1.04958425, 2.10919476,\n",
       "        1.06647006, 1.08536509, 1.04237012, 1.05456584, 1.1046199 ,\n",
       "        1.07523211, 1.07383787, 1.07007969, 2.17073842, 1.07768251,\n",
       "        1.10543154, 1.06262427, 1.07801412, 1.07470267, 1.07658743,\n",
       "        1.09501919, 2.15454445, 1.04801219, 1.06780623, 2.21256323,\n",
       "        1.09990158, 2.1083167 , 1.04010703, 1.06877979, 2.15024784,\n",
       "        2.08415371, 2.13644475, 1.08347247, 1.16309747, 2.14992404,\n",
       "        1.11904061, 1.07312518, 1.0633484 , 2.05794993, 2.13603831,\n",
       "        2.15035549, 1.07507664, 1.07342144, 1.07536214, 1.04634921,\n",
       "        1.04795609, 1.07015535, 2.07647416, 1.12594741, 1.09101412,\n",
       "        1.04925619, 2.17895969, 1.06610128, 1.06528978, 1.07477708,\n",
       "        2.16999656, 1.07253261, 1.13360942, 1.06101216, 1.04465696,\n",
       "        1.07003646, 1.09974758, 2.05965129, 1.07873883, 2.21957439,\n",
       "        1.13477446, 1.06209612, 1.08257485, 1.03429683, 2.1312346 ,\n",
       "        2.16804508, 2.09130952, 1.07535202, 1.08423033, 1.04632201,\n",
       "        1.10146131, 2.07748237, 2.08667213, 1.11316515, 2.1019435 ,\n",
       "        1.0556157 , 1.05445063, 1.06060432, 1.05640261, 1.10639073,\n",
       "        2.14468085, 1.07729569, 1.09067246, 1.11187747, 1.06617799,\n",
       "        1.04974193, 1.05758079, 1.07562555, 1.08892457, 2.15526177,\n",
       "        1.10805444, 1.07755487, 1.08237594, 1.08141843, 1.0606405 ,\n",
       "        1.10524903, 2.09810708, 1.05730481, 1.08254506, 2.16739353,\n",
       "        1.0759497 , 2.09843032, 1.07234503, 2.10145226, 2.09477432,\n",
       "        1.09549732, 2.12767884, 1.08049272, 2.16355013, 1.06869506,\n",
       "        1.07046426, 2.15124199, 1.08406736, 1.08282744, 1.06903779,\n",
       "        2.11129925, 1.06035961, 2.10613675, 2.06813423, 2.15383599,\n",
       "        2.15181924, 2.16117407, 1.1101216 , 1.11196208, 1.07639126,\n",
       "        1.08136889, 1.07647809, 1.10052029, 2.1606462 , 2.15141392,\n",
       "        1.04320058, 1.09846669, 1.07404156, 2.06506051, 1.07365976,\n",
       "        1.06578632, 1.08645382, 2.12225002, 1.08890938, 1.0818338 ,\n",
       "        1.08342925, 1.08531825, 1.0825517 , 1.064838  , 1.0866444 ,\n",
       "        2.1509083 , 2.07087945, 2.24578071, 1.08049834, 1.08606653,\n",
       "        2.13921618, 2.13029631, 1.08636236, 2.11605951, 2.14700442,\n",
       "        1.08287845, 1.07361228, 2.15380115, 1.05453775, 1.0681494 ,\n",
       "        1.06330795, 1.04708872, 2.17129213, 1.0895648 , 1.08577397,\n",
       "        1.09787535, 1.0745957 , 1.11044974, 1.06023056, 1.10467807,\n",
       "        1.09818018, 1.07999014, 1.06313236, 1.05774682, 1.09393765,\n",
       "        1.07518992, 2.11337304, 1.08453693, 1.08532703, 1.04307017,\n",
       "        2.10072166, 2.16505371, 1.04878313, 2.14201867, 1.05639909,\n",
       "        1.05368461, 2.17514641, 2.11282479, 2.12074514, 2.15249924,\n",
       "        1.05873959, 2.15853976, 1.10572942, 2.16042065, 2.15037019,\n",
       "        1.0810845 , 1.07153549, 1.10753028, 1.06345844, 2.17555564,\n",
       "        1.06129531, 1.06257148, 1.10083134, 1.06899118, 1.10739951,\n",
       "        1.07678303, 1.10331112, 2.10300452, 1.07675014, 2.14518399,\n",
       "        1.0939737 , 1.05409456, 1.06317029, 2.14358696, 1.05051671,\n",
       "        1.09919978, 2.08193704, 1.09803279, 1.08899608, 1.0649629 ,\n",
       "        1.09186546, 1.09024406, 1.1228653 , 1.07348174, 1.08093119,\n",
       "        1.07575071, 1.09140113, 2.13859601, 2.12310023, 2.0717006 ,\n",
       "        1.10984644, 1.11205763, 2.09049262, 1.06204147, 1.07200161,\n",
       "        1.10076801, 1.12774468, 2.09141926, 2.11319916, 1.06642113,\n",
       "        1.08257229, 1.079482  , 1.11075938, 1.08785691, 1.08811072,\n",
       "        1.09867689, 2.13963383, 1.07509933, 1.08762683, 1.07883011,\n",
       "        2.0874249 , 1.08439228, 1.05816573, 2.12866131, 1.0524235 ,\n",
       "        1.13614032, 2.04301432, 1.0813418 , 1.05227582, 1.05784615,\n",
       "        2.12049193, 2.1058704 , 1.06354564, 1.09404334])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_mo = {'q'+str(i): y_train[:, i] for i in range(len(quantiles))}\n",
    "y_val_mo = {'q'+str(i): y_val[:, i] for i in range(len(quantiles))}\n",
    "y_train_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 269 samples, validate on 68 samples\n",
      "Epoch 1/300\n",
      "269/269 [==============================] - 3s 9ms/sample - loss: 8.8321 - q0_loss: 0.4345 - q1_loss: 0.6918 - q2_loss: 0.9457 - q3_loss: 1.0499 - q4_loss: 0.9108 - q5_loss: 0.7044 - q6_loss: 0.9783 - q7_loss: 1.4464 - q8_loss: 1.5584 - val_loss: 7.1725 - val_q0_loss: 0.2371 - val_q1_loss: 0.5503 - val_q2_loss: 0.3605 - val_q3_loss: 1.5631 - val_q4_loss: 0.8020 - val_q5_loss: 0.6186 - val_q6_loss: 0.9241 - val_q7_loss: 1.6932 - val_q8_loss: 0.5717\n",
      "Epoch 2/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 6.7288 - q0_loss: 0.2636 - q1_loss: 0.3623 - q2_loss: 0.4209 - q3_loss: 0.8993 - q4_loss: 0.5300 - q5_loss: 0.6492 - q6_loss: 1.1235 - q7_loss: 1.7033 - q8_loss: 0.7830 - val_loss: 5.6954 - val_q0_loss: 0.2700 - val_q1_loss: 0.2695 - val_q2_loss: 0.4728 - val_q3_loss: 0.4006 - val_q4_loss: 0.3968 - val_q5_loss: 0.5506 - val_q6_loss: 0.8797 - val_q7_loss: 1.3433 - val_q8_loss: 0.9200\n",
      "Epoch 3/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 5.8238 - q0_loss: 0.2795 - q1_loss: 0.2442 - q2_loss: 0.4932 - q3_loss: 0.4577 - q4_loss: 0.3579 - q5_loss: 0.4846 - q6_loss: 0.9718 - q7_loss: 1.5310 - q8_loss: 0.9922 - val_loss: 5.3588 - val_q0_loss: 0.1944 - val_q1_loss: 0.2556 - val_q2_loss: 0.2682 - val_q3_loss: 0.6702 - val_q4_loss: 0.3732 - val_q5_loss: 0.4772 - val_q6_loss: 1.0557 - val_q7_loss: 1.4474 - val_q8_loss: 0.5034\n",
      "Epoch 4/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 5.4764 - q0_loss: 0.2062 - q1_loss: 0.2303 - q2_loss: 0.3274 - q3_loss: 0.6028 - q4_loss: 0.3224 - q5_loss: 0.3961 - q6_loss: 1.0877 - q7_loss: 1.5972 - q8_loss: 0.6849 - val_loss: 4.8687 - val_q0_loss: 0.1829 - val_q1_loss: 0.1756 - val_q2_loss: 0.3055 - val_q3_loss: 0.3820 - val_q4_loss: 0.2527 - val_q5_loss: 0.3375 - val_q6_loss: 0.9103 - val_q7_loss: 1.3717 - val_q8_loss: 0.6829\n",
      "Epoch 5/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 5.1084 - q0_loss: 0.2116 - q1_loss: 0.1910 - q2_loss: 0.3281 - q3_loss: 0.3837 - q4_loss: 0.2720 - q5_loss: 0.3956 - q6_loss: 1.0686 - q7_loss: 1.5336 - q8_loss: 0.7278 - val_loss: 4.6494 - val_q0_loss: 0.1359 - val_q1_loss: 0.1737 - val_q2_loss: 0.2309 - val_q3_loss: 0.4907 - val_q4_loss: 0.2305 - val_q5_loss: 0.2775 - val_q6_loss: 0.9921 - val_q7_loss: 1.3908 - val_q8_loss: 0.4891\n",
      "Epoch 6/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 4.8656 - q0_loss: 0.1693 - q1_loss: 0.1628 - q2_loss: 0.2739 - q3_loss: 0.4511 - q4_loss: 0.2280 - q5_loss: 0.3310 - q6_loss: 1.0304 - q7_loss: 1.5217 - q8_loss: 0.6544 - val_loss: 4.4125 - val_q0_loss: 0.1435 - val_q1_loss: 0.1255 - val_q2_loss: 0.2169 - val_q3_loss: 0.2839 - val_q4_loss: 0.1753 - val_q5_loss: 0.3641 - val_q6_loss: 0.9986 - val_q7_loss: 1.3307 - val_q8_loss: 0.5106\n",
      "Epoch 7/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 4.6252 - q0_loss: 0.1489 - q1_loss: 0.1427 - q2_loss: 0.2490 - q3_loss: 0.3948 - q4_loss: 0.2004 - q5_loss: 0.3106 - q6_loss: 1.0257 - q7_loss: 1.5073 - q8_loss: 0.6362 - val_loss: 4.1729 - val_q0_loss: 0.1058 - val_q1_loss: 0.1074 - val_q2_loss: 0.1486 - val_q3_loss: 0.3045 - val_q4_loss: 0.1404 - val_q5_loss: 0.3476 - val_q6_loss: 1.0270 - val_q7_loss: 1.3233 - val_q8_loss: 0.3973\n",
      "Epoch 8/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 4.4060 - q0_loss: 0.1293 - q1_loss: 0.1227 - q2_loss: 0.2171 - q3_loss: 0.4002 - q4_loss: 0.1679 - q5_loss: 0.2804 - q6_loss: 1.0104 - q7_loss: 1.4919 - q8_loss: 0.5835 - val_loss: 3.9772 - val_q0_loss: 0.0753 - val_q1_loss: 0.1050 - val_q2_loss: 0.1244 - val_q3_loss: 0.3419 - val_q4_loss: 0.1261 - val_q5_loss: 0.2975 - val_q6_loss: 1.0197 - val_q7_loss: 1.3145 - val_q8_loss: 0.3192\n",
      "Epoch 9/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 4.1665 - q0_loss: 0.1036 - q1_loss: 0.0971 - q2_loss: 0.1960 - q3_loss: 0.3696 - q4_loss: 0.1277 - q5_loss: 0.2465 - q6_loss: 0.9971 - q7_loss: 1.4638 - q8_loss: 0.5427 - val_loss: 3.7814 - val_q0_loss: 0.0627 - val_q1_loss: 0.1118 - val_q2_loss: 0.1111 - val_q3_loss: 0.4444 - val_q4_loss: 0.1294 - val_q5_loss: 0.1678 - val_q6_loss: 0.9555 - val_q7_loss: 1.3141 - val_q8_loss: 0.2602\n",
      "Epoch 10/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 3.8863 - q0_loss: 0.0768 - q1_loss: 0.0750 - q2_loss: 0.1407 - q3_loss: 0.4185 - q4_loss: 0.0978 - q5_loss: 0.1728 - q6_loss: 0.9841 - q7_loss: 1.4652 - q8_loss: 0.4674 - val_loss: 3.4467 - val_q0_loss: 0.0474 - val_q1_loss: 0.0584 - val_q2_loss: 0.0740 - val_q3_loss: 0.3998 - val_q4_loss: 0.0709 - val_q5_loss: 0.0962 - val_q6_loss: 0.8493 - val_q7_loss: 1.2843 - val_q8_loss: 0.3494\n",
      "Epoch 11/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 3.5896 - q0_loss: 0.0539 - q1_loss: 0.0376 - q2_loss: 0.0917 - q3_loss: 0.3894 - q4_loss: 0.0484 - q5_loss: 0.1341 - q6_loss: 0.9521 - q7_loss: 1.4349 - q8_loss: 0.4421 - val_loss: 3.2040 - val_q0_loss: 0.0320 - val_q1_loss: 0.0342 - val_q2_loss: 0.0413 - val_q3_loss: 0.3468 - val_q4_loss: 0.0412 - val_q5_loss: 0.0981 - val_q6_loss: 0.8510 - val_q7_loss: 1.2570 - val_q8_loss: 0.3088\n",
      "Epoch 12/300\n",
      "269/269 [==============================] - 0s 178us/sample - loss: 3.3850 - q0_loss: 0.0361 - q1_loss: 0.0286 - q2_loss: 0.0452 - q3_loss: 0.3829 - q4_loss: 0.0425 - q5_loss: 0.1084 - q6_loss: 0.9424 - q7_loss: 1.4088 - q8_loss: 0.3694 - val_loss: 3.0346 - val_q0_loss: 0.0205 - val_q1_loss: 0.0171 - val_q2_loss: 0.0166 - val_q3_loss: 0.2602 - val_q4_loss: 0.0375 - val_q5_loss: 0.1224 - val_q6_loss: 0.8527 - val_q7_loss: 1.2242 - val_q8_loss: 0.2953\n",
      "Epoch 13/300\n",
      "269/269 [==============================] - 0s 180us/sample - loss: 3.2119 - q0_loss: 0.0260 - q1_loss: 0.0222 - q2_loss: 0.0305 - q3_loss: 0.3466 - q4_loss: 0.0346 - q5_loss: 0.0975 - q6_loss: 0.9380 - q7_loss: 1.3820 - q8_loss: 0.3174 - val_loss: 2.9116 - val_q0_loss: 0.0256 - val_q1_loss: 0.0239 - val_q2_loss: 0.0336 - val_q3_loss: 0.2938 - val_q4_loss: 0.0307 - val_q5_loss: 0.0337 - val_q6_loss: 0.7590 - val_q7_loss: 1.2097 - val_q8_loss: 0.3240\n",
      "Epoch 14/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 3.0823 - q0_loss: 0.0264 - q1_loss: 0.0224 - q2_loss: 0.0422 - q3_loss: 0.3294 - q4_loss: 0.0327 - q5_loss: 0.0572 - q6_loss: 0.8985 - q7_loss: 1.3477 - q8_loss: 0.2838 - val_loss: 2.7993 - val_q0_loss: 0.0272 - val_q1_loss: 0.0213 - val_q2_loss: 0.0318 - val_q3_loss: 0.2803 - val_q4_loss: 0.0252 - val_q5_loss: 0.0331 - val_q6_loss: 0.7283 - val_q7_loss: 1.1882 - val_q8_loss: 0.2998\n",
      "Epoch 15/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 2.9486 - q0_loss: 0.0282 - q1_loss: 0.0174 - q2_loss: 0.0430 - q3_loss: 0.3051 - q4_loss: 0.0208 - q5_loss: 0.0461 - q6_loss: 0.8854 - q7_loss: 1.3331 - q8_loss: 0.2509 - val_loss: 2.6782 - val_q0_loss: 0.0224 - val_q1_loss: 0.0175 - val_q2_loss: 0.0488 - val_q3_loss: 0.1939 - val_q4_loss: 0.0327 - val_q5_loss: 0.0689 - val_q6_loss: 0.8173 - val_q7_loss: 1.1549 - val_q8_loss: 0.1562\n",
      "Epoch 16/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 2.8218 - q0_loss: 0.0271 - q1_loss: 0.0172 - q2_loss: 0.0421 - q3_loss: 0.2741 - q4_loss: 0.0233 - q5_loss: 0.0419 - q6_loss: 0.8928 - q7_loss: 1.3233 - q8_loss: 0.2014 - val_loss: 2.5739 - val_q0_loss: 0.0196 - val_q1_loss: 0.0229 - val_q2_loss: 0.0288 - val_q3_loss: 0.1402 - val_q4_loss: 0.0498 - val_q5_loss: 0.0735 - val_q6_loss: 0.7987 - val_q7_loss: 1.1277 - val_q8_loss: 0.1496\n",
      "Epoch 17/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 2.6958 - q0_loss: 0.0245 - q1_loss: 0.0168 - q2_loss: 0.0311 - q3_loss: 0.2251 - q4_loss: 0.0269 - q5_loss: 0.0428 - q6_loss: 0.8649 - q7_loss: 1.2724 - q8_loss: 0.1621 - val_loss: 2.4015 - val_q0_loss: 0.0248 - val_q1_loss: 0.0127 - val_q2_loss: 0.0176 - val_q3_loss: 0.1741 - val_q4_loss: 0.0192 - val_q5_loss: 0.0207 - val_q6_loss: 0.7355 - val_q7_loss: 1.1116 - val_q8_loss: 0.1278\n",
      "Epoch 18/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 [==============================] - 0s 171us/sample - loss: 2.5472 - q0_loss: 0.0243 - q1_loss: 0.0138 - q2_loss: 0.0248 - q3_loss: 0.1950 - q4_loss: 0.0193 - q5_loss: 0.0293 - q6_loss: 0.8497 - q7_loss: 1.2455 - q8_loss: 0.1177 - val_loss: 2.2843 - val_q0_loss: 0.0252 - val_q1_loss: 0.0108 - val_q2_loss: 0.0140 - val_q3_loss: 0.1393 - val_q4_loss: 0.0145 - val_q5_loss: 0.0262 - val_q6_loss: 0.7257 - val_q7_loss: 1.0859 - val_q8_loss: 0.0881\n",
      "Epoch 19/300\n",
      "269/269 [==============================] - 0s 178us/sample - loss: 2.4592 - q0_loss: 0.0245 - q1_loss: 0.0154 - q2_loss: 0.0283 - q3_loss: 0.1661 - q4_loss: 0.0176 - q5_loss: 0.0383 - q6_loss: 0.8363 - q7_loss: 1.2292 - q8_loss: 0.0984 - val_loss: 2.2986 - val_q0_loss: 0.0244 - val_q1_loss: 0.0237 - val_q2_loss: 0.0118 - val_q3_loss: 0.0766 - val_q4_loss: 0.0529 - val_q5_loss: 0.0900 - val_q6_loss: 0.7774 - val_q7_loss: 1.0498 - val_q8_loss: 0.0411\n",
      "Epoch 20/300\n",
      "269/269 [==============================] - 0s 180us/sample - loss: 2.5229 - q0_loss: 0.0342 - q1_loss: 0.0296 - q2_loss: 0.0218 - q3_loss: 0.1764 - q4_loss: 0.0601 - q5_loss: 0.1154 - q6_loss: 0.8256 - q7_loss: 1.2079 - q8_loss: 0.0671 - val_loss: 2.5949 - val_q0_loss: 0.0821 - val_q1_loss: 0.0724 - val_q2_loss: 0.0361 - val_q3_loss: 0.2884 - val_q4_loss: 0.1332 - val_q5_loss: 0.2219 - val_q6_loss: 0.5833 - val_q7_loss: 1.0564 - val_q8_loss: 0.0356\n",
      "Epoch 21/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 2.5815 - q0_loss: 0.0390 - q1_loss: 0.0410 - q2_loss: 0.0808 - q3_loss: 0.1666 - q4_loss: 0.0519 - q5_loss: 0.1443 - q6_loss: 0.7355 - q7_loss: 1.1725 - q8_loss: 0.1442 - val_loss: 2.2243 - val_q0_loss: 0.0229 - val_q1_loss: 0.0176 - val_q2_loss: 0.0463 - val_q3_loss: 0.0997 - val_q4_loss: 0.0287 - val_q5_loss: 0.0889 - val_q6_loss: 0.6731 - val_q7_loss: 1.0107 - val_q8_loss: 0.0845\n",
      "Epoch 22/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 2.4370 - q0_loss: 0.0293 - q1_loss: 0.0328 - q2_loss: 0.0745 - q3_loss: 0.1226 - q4_loss: 0.0339 - q5_loss: 0.1017 - q6_loss: 0.7560 - q7_loss: 1.1519 - q8_loss: 0.1260 - val_loss: 2.1225 - val_q0_loss: 0.0224 - val_q1_loss: 0.0330 - val_q2_loss: 0.0456 - val_q3_loss: 0.0755 - val_q4_loss: 0.0515 - val_q5_loss: 0.0474 - val_q6_loss: 0.6527 - val_q7_loss: 0.9866 - val_q8_loss: 0.0815\n",
      "Epoch 23/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 2.2880 - q0_loss: 0.0321 - q1_loss: 0.0255 - q2_loss: 0.0413 - q3_loss: 0.1246 - q4_loss: 0.0422 - q5_loss: 0.0870 - q6_loss: 0.7150 - q7_loss: 1.1225 - q8_loss: 0.0826 - val_loss: 1.9525 - val_q0_loss: 0.0234 - val_q1_loss: 0.0144 - val_q2_loss: 0.0236 - val_q3_loss: 0.0580 - val_q4_loss: 0.0305 - val_q5_loss: 0.0282 - val_q6_loss: 0.6366 - val_q7_loss: 0.9621 - val_q8_loss: 0.0438\n",
      "Epoch 24/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 2.1544 - q0_loss: 0.0262 - q1_loss: 0.0214 - q2_loss: 0.0441 - q3_loss: 0.0894 - q4_loss: 0.0214 - q5_loss: 0.0616 - q6_loss: 0.7122 - q7_loss: 1.1046 - q8_loss: 0.0807 - val_loss: 2.0383 - val_q0_loss: 0.0285 - val_q1_loss: 0.0263 - val_q2_loss: 0.0714 - val_q3_loss: 0.0647 - val_q4_loss: 0.0268 - val_q5_loss: 0.0987 - val_q6_loss: 0.5036 - val_q7_loss: 0.9561 - val_q8_loss: 0.1262\n",
      "Epoch 25/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 2.1477 - q0_loss: 0.0262 - q1_loss: 0.0297 - q2_loss: 0.0626 - q3_loss: 0.0945 - q4_loss: 0.0288 - q5_loss: 0.0662 - q6_loss: 0.6454 - q7_loss: 1.0695 - q8_loss: 0.1122 - val_loss: 2.0711 - val_q0_loss: 0.0477 - val_q1_loss: 0.0451 - val_q2_loss: 0.0625 - val_q3_loss: 0.0940 - val_q4_loss: 0.0539 - val_q5_loss: 0.0981 - val_q6_loss: 0.5521 - val_q7_loss: 0.9162 - val_q8_loss: 0.0999\n",
      "Epoch 26/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 2.0321 - q0_loss: 0.0285 - q1_loss: 0.0261 - q2_loss: 0.0363 - q3_loss: 0.0976 - q4_loss: 0.0339 - q5_loss: 0.0555 - q6_loss: 0.6255 - q7_loss: 1.0424 - q8_loss: 0.0679 - val_loss: 1.8702 - val_q0_loss: 0.0394 - val_q1_loss: 0.0252 - val_q2_loss: 0.0307 - val_q3_loss: 0.0848 - val_q4_loss: 0.0386 - val_q5_loss: 0.1080 - val_q6_loss: 0.4852 - val_q7_loss: 0.9003 - val_q8_loss: 0.0549\n",
      "Epoch 27/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 1.9313 - q0_loss: 0.0258 - q1_loss: 0.0223 - q2_loss: 0.0373 - q3_loss: 0.0760 - q4_loss: 0.0214 - q5_loss: 0.0540 - q6_loss: 0.5920 - q7_loss: 1.0179 - q8_loss: 0.0676 - val_loss: 1.7920 - val_q0_loss: 0.0205 - val_q1_loss: 0.0197 - val_q2_loss: 0.0415 - val_q3_loss: 0.0624 - val_q4_loss: 0.0205 - val_q5_loss: 0.0405 - val_q6_loss: 0.5275 - val_q7_loss: 0.8689 - val_q8_loss: 0.0745\n",
      "Epoch 28/300\n",
      "269/269 [==============================] - 0s 178us/sample - loss: 1.8554 - q0_loss: 0.0250 - q1_loss: 0.0157 - q2_loss: 0.0251 - q3_loss: 0.0847 - q4_loss: 0.0200 - q5_loss: 0.0611 - q6_loss: 0.5638 - q7_loss: 1.0039 - q8_loss: 0.0524 - val_loss: 1.6506 - val_q0_loss: 0.0203 - val_q1_loss: 0.0132 - val_q2_loss: 0.0138 - val_q3_loss: 0.0663 - val_q4_loss: 0.0247 - val_q5_loss: 0.0220 - val_q6_loss: 0.4937 - val_q7_loss: 0.8480 - val_q8_loss: 0.0307\n",
      "Epoch 29/300\n",
      "269/269 [==============================] - 0s 180us/sample - loss: 1.7769 - q0_loss: 0.0247 - q1_loss: 0.0158 - q2_loss: 0.0243 - q3_loss: 0.0761 - q4_loss: 0.0171 - q5_loss: 0.0565 - q6_loss: 0.5331 - q7_loss: 0.9790 - q8_loss: 0.0491 - val_loss: 1.6750 - val_q0_loss: 0.0342 - val_q1_loss: 0.0300 - val_q2_loss: 0.0526 - val_q3_loss: 0.0349 - val_q4_loss: 0.0258 - val_q5_loss: 0.0431 - val_q6_loss: 0.4539 - val_q7_loss: 0.8202 - val_q8_loss: 0.0887\n",
      "Epoch 30/300\n",
      "269/269 [==============================] - 0s 178us/sample - loss: 1.7606 - q0_loss: 0.0282 - q1_loss: 0.0239 - q2_loss: 0.0327 - q3_loss: 0.0814 - q4_loss: 0.0293 - q5_loss: 0.0507 - q6_loss: 0.5030 - q7_loss: 0.9477 - q8_loss: 0.0599 - val_loss: 1.5957 - val_q0_loss: 0.0364 - val_q1_loss: 0.0197 - val_q2_loss: 0.0158 - val_q3_loss: 0.0827 - val_q4_loss: 0.0361 - val_q5_loss: 0.1114 - val_q6_loss: 0.3454 - val_q7_loss: 0.8127 - val_q8_loss: 0.0317\n",
      "Epoch 31/300\n",
      "269/269 [==============================] - 0s 180us/sample - loss: 1.6860 - q0_loss: 0.0245 - q1_loss: 0.0167 - q2_loss: 0.0293 - q3_loss: 0.0842 - q4_loss: 0.0205 - q5_loss: 0.0613 - q6_loss: 0.4636 - q7_loss: 0.9256 - q8_loss: 0.0524 - val_loss: 1.4527 - val_q0_loss: 0.0246 - val_q1_loss: 0.0149 - val_q2_loss: 0.0140 - val_q3_loss: 0.0535 - val_q4_loss: 0.0177 - val_q5_loss: 0.0657 - val_q6_loss: 0.3446 - val_q7_loss: 0.7885 - val_q8_loss: 0.0256\n",
      "Epoch 32/300\n",
      "269/269 [==============================] - 0s 180us/sample - loss: 1.5723 - q0_loss: 0.0219 - q1_loss: 0.0137 - q2_loss: 0.0201 - q3_loss: 0.0725 - q4_loss: 0.0122 - q5_loss: 0.0514 - q6_loss: 0.4288 - q7_loss: 0.9062 - q8_loss: 0.0423 - val_loss: 1.4306 - val_q0_loss: 0.0269 - val_q1_loss: 0.0172 - val_q2_loss: 0.0259 - val_q3_loss: 0.0401 - val_q4_loss: 0.0162 - val_q5_loss: 0.0580 - val_q6_loss: 0.3336 - val_q7_loss: 0.7584 - val_q8_loss: 0.0523\n",
      "Epoch 33/300\n",
      "269/269 [==============================] - 0s 199us/sample - loss: 1.5569 - q0_loss: 0.0233 - q1_loss: 0.0206 - q2_loss: 0.0264 - q3_loss: 0.0791 - q4_loss: 0.0248 - q5_loss: 0.0450 - q6_loss: 0.3972 - q7_loss: 0.8749 - q8_loss: 0.0505 - val_loss: 1.4798 - val_q0_loss: 0.0244 - val_q1_loss: 0.0286 - val_q2_loss: 0.0477 - val_q3_loss: 0.0592 - val_q4_loss: 0.0391 - val_q5_loss: 0.0488 - val_q6_loss: 0.2944 - val_q7_loss: 0.7514 - val_q8_loss: 0.0753\n",
      "Epoch 34/300\n",
      "269/269 [==============================] - 0s 184us/sample - loss: 1.5293 - q0_loss: 0.0264 - q1_loss: 0.0247 - q2_loss: 0.0309 - q3_loss: 0.0884 - q4_loss: 0.0326 - q5_loss: 0.0503 - q6_loss: 0.3620 - q7_loss: 0.8559 - q8_loss: 0.0536 - val_loss: 1.4474 - val_q0_loss: 0.0302 - val_q1_loss: 0.0298 - val_q2_loss: 0.0460 - val_q3_loss: 0.0588 - val_q4_loss: 0.0430 - val_q5_loss: 0.0617 - val_q6_loss: 0.2567 - val_q7_loss: 0.7298 - val_q8_loss: 0.0773\n",
      "Epoch 35/300\n",
      "269/269 [==============================] - 0s 186us/sample - loss: 1.4656 - q0_loss: 0.0249 - q1_loss: 0.0245 - q2_loss: 0.0376 - q3_loss: 0.0737 - q4_loss: 0.0271 - q5_loss: 0.0495 - q6_loss: 0.3376 - q7_loss: 0.8455 - q8_loss: 0.0676 - val_loss: 1.2971 - val_q0_loss: 0.0286 - val_q1_loss: 0.0212 - val_q2_loss: 0.0353 - val_q3_loss: 0.0462 - val_q4_loss: 0.0239 - val_q5_loss: 0.0389 - val_q6_loss: 0.2439 - val_q7_loss: 0.6938 - val_q8_loss: 0.0608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/300\n",
      "269/269 [==============================] - 0s 184us/sample - loss: 1.4031 - q0_loss: 0.0241 - q1_loss: 0.0232 - q2_loss: 0.0395 - q3_loss: 0.0723 - q4_loss: 0.0229 - q5_loss: 0.0436 - q6_loss: 0.2847 - q7_loss: 0.8005 - q8_loss: 0.0718 - val_loss: 1.2394 - val_q0_loss: 0.0255 - val_q1_loss: 0.0172 - val_q2_loss: 0.0384 - val_q3_loss: 0.0527 - val_q4_loss: 0.0102 - val_q5_loss: 0.0367 - val_q6_loss: 0.2446 - val_q7_loss: 0.6587 - val_q8_loss: 0.0762\n",
      "Epoch 37/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 1.3184 - q0_loss: 0.0265 - q1_loss: 0.0214 - q2_loss: 0.0278 - q3_loss: 0.0817 - q4_loss: 0.0262 - q5_loss: 0.0440 - q6_loss: 0.2612 - q7_loss: 0.7802 - q8_loss: 0.0526 - val_loss: 1.1247 - val_q0_loss: 0.0214 - val_q1_loss: 0.0124 - val_q2_loss: 0.0249 - val_q3_loss: 0.0391 - val_q4_loss: 0.0157 - val_q5_loss: 0.0443 - val_q6_loss: 0.1661 - val_q7_loss: 0.6551 - val_q8_loss: 0.0393\n",
      "Epoch 38/300\n",
      "269/269 [==============================] - 0s 199us/sample - loss: 1.2236 - q0_loss: 0.0217 - q1_loss: 0.0159 - q2_loss: 0.0256 - q3_loss: 0.0680 - q4_loss: 0.0158 - q5_loss: 0.0419 - q6_loss: 0.2240 - q7_loss: 0.7490 - q8_loss: 0.0453 - val_loss: 1.1155 - val_q0_loss: 0.0189 - val_q1_loss: 0.0182 - val_q2_loss: 0.0452 - val_q3_loss: 0.0381 - val_q4_loss: 0.0140 - val_q5_loss: 0.0625 - val_q6_loss: 0.1110 - val_q7_loss: 0.6407 - val_q8_loss: 0.0750\n",
      "Epoch 39/300\n",
      "269/269 [==============================] - 0s 132us/sample - loss: 1.1877 - q0_loss: 0.0219 - q1_loss: 0.0178 - q2_loss: 0.0342 - q3_loss: 0.0674 - q4_loss: 0.0140 - q5_loss: 0.0416 - q6_loss: 0.1945 - q7_loss: 0.7314 - q8_loss: 0.0617 - val_loss: 1.0617 - val_q0_loss: 0.0221 - val_q1_loss: 0.0120 - val_q2_loss: 0.0382 - val_q3_loss: 0.0566 - val_q4_loss: 0.0097 - val_q5_loss: 0.0800 - val_q6_loss: 0.0806 - val_q7_loss: 0.6152 - val_q8_loss: 0.0571\n",
      "Epoch 40/300\n",
      "269/269 [==============================] - 0s 244us/sample - loss: 1.0940 - q0_loss: 0.0198 - q1_loss: 0.0141 - q2_loss: 0.0238 - q3_loss: 0.0737 - q4_loss: 0.0106 - q5_loss: 0.0406 - q6_loss: 0.1616 - q7_loss: 0.7093 - q8_loss: 0.0454 - val_loss: 0.9398 - val_q0_loss: 0.0161 - val_q1_loss: 0.0132 - val_q2_loss: 0.0171 - val_q3_loss: 0.0582 - val_q4_loss: 0.0216 - val_q5_loss: 0.0214 - val_q6_loss: 0.0918 - val_q7_loss: 0.5828 - val_q8_loss: 0.0237\n",
      "Epoch 41/300\n",
      "269/269 [==============================] - 0s 184us/sample - loss: 1.0893 - q0_loss: 0.0241 - q1_loss: 0.0210 - q2_loss: 0.0297 - q3_loss: 0.0799 - q4_loss: 0.0229 - q5_loss: 0.0347 - q6_loss: 0.1500 - q7_loss: 0.6876 - q8_loss: 0.0552 - val_loss: 1.0153 - val_q0_loss: 0.0175 - val_q1_loss: 0.0297 - val_q2_loss: 0.0489 - val_q3_loss: 0.0626 - val_q4_loss: 0.0406 - val_q5_loss: 0.0213 - val_q6_loss: 0.0695 - val_q7_loss: 0.5718 - val_q8_loss: 0.0704\n",
      "Epoch 42/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 1.0662 - q0_loss: 0.0246 - q1_loss: 0.0233 - q2_loss: 0.0389 - q3_loss: 0.0829 - q4_loss: 0.0256 - q5_loss: 0.0298 - q6_loss: 0.1337 - q7_loss: 0.6548 - q8_loss: 0.0675 - val_loss: 0.8745 - val_q0_loss: 0.0187 - val_q1_loss: 0.0142 - val_q2_loss: 0.0125 - val_q3_loss: 0.0637 - val_q4_loss: 0.0261 - val_q5_loss: 0.0197 - val_q6_loss: 0.0718 - val_q7_loss: 0.5274 - val_q8_loss: 0.0341\n",
      "Epoch 43/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.9782 - q0_loss: 0.0211 - q1_loss: 0.0178 - q2_loss: 0.0254 - q3_loss: 0.0788 - q4_loss: 0.0169 - q5_loss: 0.0266 - q6_loss: 0.1295 - q7_loss: 0.6369 - q8_loss: 0.0520 - val_loss: 0.8441 - val_q0_loss: 0.0198 - val_q1_loss: 0.0127 - val_q2_loss: 0.0230 - val_q3_loss: 0.0480 - val_q4_loss: 0.0086 - val_q5_loss: 0.0269 - val_q6_loss: 0.0753 - val_q7_loss: 0.4983 - val_q8_loss: 0.0513\n",
      "Epoch 44/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.9237 - q0_loss: 0.0203 - q1_loss: 0.0140 - q2_loss: 0.0189 - q3_loss: 0.0772 - q4_loss: 0.0137 - q5_loss: 0.0248 - q6_loss: 0.1169 - q7_loss: 0.5958 - q8_loss: 0.0429 - val_loss: 0.7649 - val_q0_loss: 0.0186 - val_q1_loss: 0.0104 - val_q2_loss: 0.0086 - val_q3_loss: 0.0466 - val_q4_loss: 0.0092 - val_q5_loss: 0.0207 - val_q6_loss: 0.0772 - val_q7_loss: 0.4758 - val_q8_loss: 0.0277\n",
      "Epoch 45/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.8748 - q0_loss: 0.0203 - q1_loss: 0.0142 - q2_loss: 0.0112 - q3_loss: 0.0757 - q4_loss: 0.0149 - q5_loss: 0.0222 - q6_loss: 0.1200 - q7_loss: 0.5724 - q8_loss: 0.0304 - val_loss: 0.7397 - val_q0_loss: 0.0173 - val_q1_loss: 0.0104 - val_q2_loss: 0.0096 - val_q3_loss: 0.0587 - val_q4_loss: 0.0124 - val_q5_loss: 0.0128 - val_q6_loss: 0.0633 - val_q7_loss: 0.4495 - val_q8_loss: 0.0319\n",
      "Epoch 46/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.8427 - q0_loss: 0.0189 - q1_loss: 0.0121 - q2_loss: 0.0120 - q3_loss: 0.0783 - q4_loss: 0.0142 - q5_loss: 0.0191 - q6_loss: 0.1095 - q7_loss: 0.5338 - q8_loss: 0.0331 - val_loss: 0.7394 - val_q0_loss: 0.0222 - val_q1_loss: 0.0169 - val_q2_loss: 0.0146 - val_q3_loss: 0.0415 - val_q4_loss: 0.0155 - val_q5_loss: 0.0280 - val_q6_loss: 0.0814 - val_q7_loss: 0.4197 - val_q8_loss: 0.0368\n",
      "Epoch 47/300\n",
      "269/269 [==============================] - 0s 178us/sample - loss: 0.8424 - q0_loss: 0.0199 - q1_loss: 0.0149 - q2_loss: 0.0193 - q3_loss: 0.0743 - q4_loss: 0.0161 - q5_loss: 0.0268 - q6_loss: 0.1216 - q7_loss: 0.5139 - q8_loss: 0.0432 - val_loss: 0.8453 - val_q0_loss: 0.0161 - val_q1_loss: 0.0114 - val_q2_loss: 0.0396 - val_q3_loss: 0.0824 - val_q4_loss: 0.0196 - val_q5_loss: 0.0646 - val_q6_loss: 0.0906 - val_q7_loss: 0.3827 - val_q8_loss: 0.0822\n",
      "Epoch 48/300\n",
      "269/269 [==============================] - 0s 187us/sample - loss: 0.8557 - q0_loss: 0.0196 - q1_loss: 0.0145 - q2_loss: 0.0286 - q3_loss: 0.0821 - q4_loss: 0.0149 - q5_loss: 0.0385 - q6_loss: 0.1292 - q7_loss: 0.4912 - q8_loss: 0.0523 - val_loss: 0.8244 - val_q0_loss: 0.0311 - val_q1_loss: 0.0313 - val_q2_loss: 0.0522 - val_q3_loss: 0.0652 - val_q4_loss: 0.0364 - val_q5_loss: 0.0347 - val_q6_loss: 0.0691 - val_q7_loss: 0.3474 - val_q8_loss: 0.1021\n",
      "Epoch 49/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.8448 - q0_loss: 0.0232 - q1_loss: 0.0213 - q2_loss: 0.0333 - q3_loss: 0.0869 - q4_loss: 0.0258 - q5_loss: 0.0305 - q6_loss: 0.1139 - q7_loss: 0.4582 - q8_loss: 0.0573 - val_loss: 0.7417 - val_q0_loss: 0.0215 - val_q1_loss: 0.0204 - val_q2_loss: 0.0572 - val_q3_loss: 0.0580 - val_q4_loss: 0.0129 - val_q5_loss: 0.0230 - val_q6_loss: 0.0626 - val_q7_loss: 0.3187 - val_q8_loss: 0.1124\n",
      "Epoch 50/300\n",
      "269/269 [==============================] - 0s 179us/sample - loss: 0.7712 - q0_loss: 0.0201 - q1_loss: 0.0181 - q2_loss: 0.0306 - q3_loss: 0.0713 - q4_loss: 0.0176 - q5_loss: 0.0180 - q6_loss: 0.1116 - q7_loss: 0.4173 - q8_loss: 0.0611 - val_loss: 0.6060 - val_q0_loss: 0.0190 - val_q1_loss: 0.0111 - val_q2_loss: 0.0182 - val_q3_loss: 0.0482 - val_q4_loss: 0.0068 - val_q5_loss: 0.0111 - val_q6_loss: 0.0700 - val_q7_loss: 0.3128 - val_q8_loss: 0.0457\n",
      "Epoch 51/300\n",
      "269/269 [==============================] - 0s 180us/sample - loss: 0.6979 - q0_loss: 0.0200 - q1_loss: 0.0133 - q2_loss: 0.0201 - q3_loss: 0.0771 - q4_loss: 0.0118 - q5_loss: 0.0130 - q6_loss: 0.1133 - q7_loss: 0.4043 - q8_loss: 0.0454 - val_loss: 0.6182 - val_q0_loss: 0.0196 - val_q1_loss: 0.0130 - val_q2_loss: 0.0284 - val_q3_loss: 0.0564 - val_q4_loss: 0.0088 - val_q5_loss: 0.0234 - val_q6_loss: 0.0657 - val_q7_loss: 0.2781 - val_q8_loss: 0.0681\n",
      "Epoch 52/300\n",
      "269/269 [==============================] - 0s 178us/sample - loss: 0.6539 - q0_loss: 0.0193 - q1_loss: 0.0128 - q2_loss: 0.0157 - q3_loss: 0.0733 - q4_loss: 0.0110 - q5_loss: 0.0135 - q6_loss: 0.1098 - q7_loss: 0.3664 - q8_loss: 0.0372 - val_loss: 0.5557 - val_q0_loss: 0.0179 - val_q1_loss: 0.0108 - val_q2_loss: 0.0129 - val_q3_loss: 0.0547 - val_q4_loss: 0.0154 - val_q5_loss: 0.0126 - val_q6_loss: 0.0606 - val_q7_loss: 0.2679 - val_q8_loss: 0.0285\n",
      "Epoch 53/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.6496 - q0_loss: 0.0199 - q1_loss: 0.0134 - q2_loss: 0.0183 - q3_loss: 0.0738 - q4_loss: 0.0144 - q5_loss: 0.0229 - q6_loss: 0.1149 - q7_loss: 0.3436 - q8_loss: 0.0391 - val_loss: 0.6187 - val_q0_loss: 0.0162 - val_q1_loss: 0.0104 - val_q2_loss: 0.0278 - val_q3_loss: 0.0758 - val_q4_loss: 0.0170 - val_q5_loss: 0.0548 - val_q6_loss: 0.0766 - val_q7_loss: 0.2233 - val_q8_loss: 0.0680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/300\n",
      "269/269 [==============================] - 0s 190us/sample - loss: 0.6833 - q0_loss: 0.0210 - q1_loss: 0.0135 - q2_loss: 0.0200 - q3_loss: 0.0883 - q4_loss: 0.0204 - q5_loss: 0.0503 - q6_loss: 0.1197 - q7_loss: 0.3055 - q8_loss: 0.0433 - val_loss: 0.5922 - val_q0_loss: 0.0156 - val_q1_loss: 0.0139 - val_q2_loss: 0.0250 - val_q3_loss: 0.0701 - val_q4_loss: 0.0214 - val_q5_loss: 0.0490 - val_q6_loss: 0.0705 - val_q7_loss: 0.2020 - val_q8_loss: 0.0570\n",
      "Epoch 55/300\n",
      "269/269 [==============================] - 0s 172us/sample - loss: 0.6687 - q0_loss: 0.0211 - q1_loss: 0.0152 - q2_loss: 0.0223 - q3_loss: 0.0805 - q4_loss: 0.0222 - q5_loss: 0.0535 - q6_loss: 0.1277 - q7_loss: 0.2704 - q8_loss: 0.0467 - val_loss: 0.5825 - val_q0_loss: 0.0202 - val_q1_loss: 0.0146 - val_q2_loss: 0.0144 - val_q3_loss: 0.0678 - val_q4_loss: 0.0297 - val_q5_loss: 0.0545 - val_q6_loss: 0.0811 - val_q7_loss: 0.1857 - val_q8_loss: 0.0338\n",
      "Epoch 56/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.6369 - q0_loss: 0.0237 - q1_loss: 0.0163 - q2_loss: 0.0136 - q3_loss: 0.0850 - q4_loss: 0.0277 - q5_loss: 0.0575 - q6_loss: 0.1255 - q7_loss: 0.2434 - q8_loss: 0.0323 - val_loss: 0.4724 - val_q0_loss: 0.0225 - val_q1_loss: 0.0171 - val_q2_loss: 0.0240 - val_q3_loss: 0.0456 - val_q4_loss: 0.0126 - val_q5_loss: 0.0287 - val_q6_loss: 0.0768 - val_q7_loss: 0.1382 - val_q8_loss: 0.0573\n",
      "Epoch 57/300\n",
      "269/269 [==============================] - 0s 268us/sample - loss: 0.5472 - q0_loss: 0.0221 - q1_loss: 0.0171 - q2_loss: 0.0183 - q3_loss: 0.0719 - q4_loss: 0.0217 - q5_loss: 0.0296 - q6_loss: 0.1192 - q7_loss: 0.2239 - q8_loss: 0.0381 - val_loss: 0.5893 - val_q0_loss: 0.0398 - val_q1_loss: 0.0366 - val_q2_loss: 0.0389 - val_q3_loss: 0.0605 - val_q4_loss: 0.0457 - val_q5_loss: 0.0614 - val_q6_loss: 0.1080 - val_q7_loss: 0.1000 - val_q8_loss: 0.0762\n",
      "Epoch 58/300\n",
      "269/269 [==============================] - 0s 249us/sample - loss: 0.5537 - q0_loss: 0.0245 - q1_loss: 0.0195 - q2_loss: 0.0206 - q3_loss: 0.0777 - q4_loss: 0.0276 - q5_loss: 0.0407 - q6_loss: 0.1135 - q7_loss: 0.1811 - q8_loss: 0.0440 - val_loss: 0.4422 - val_q0_loss: 0.0269 - val_q1_loss: 0.0215 - val_q2_loss: 0.0224 - val_q3_loss: 0.0475 - val_q4_loss: 0.0237 - val_q5_loss: 0.0308 - val_q6_loss: 0.0824 - val_q7_loss: 0.0912 - val_q8_loss: 0.0503\n",
      "Epoch 59/300\n",
      "269/269 [==============================] - 0s 201us/sample - loss: 0.5021 - q0_loss: 0.0228 - q1_loss: 0.0193 - q2_loss: 0.0274 - q3_loss: 0.0738 - q4_loss: 0.0221 - q5_loss: 0.0201 - q6_loss: 0.1072 - q7_loss: 0.1646 - q8_loss: 0.0513 - val_loss: 0.4621 - val_q0_loss: 0.0249 - val_q1_loss: 0.0167 - val_q2_loss: 0.0250 - val_q3_loss: 0.0428 - val_q4_loss: 0.0225 - val_q5_loss: 0.0403 - val_q6_loss: 0.0962 - val_q7_loss: 0.0907 - val_q8_loss: 0.0355\n",
      "Epoch 60/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.5271 - q0_loss: 0.0219 - q1_loss: 0.0193 - q2_loss: 0.0346 - q3_loss: 0.0661 - q4_loss: 0.0191 - q5_loss: 0.0277 - q6_loss: 0.1118 - q7_loss: 0.1564 - q8_loss: 0.0663 - val_loss: 0.4685 - val_q0_loss: 0.0212 - val_q1_loss: 0.0137 - val_q2_loss: 0.0360 - val_q3_loss: 0.0412 - val_q4_loss: 0.0156 - val_q5_loss: 0.0545 - val_q6_loss: 0.1051 - val_q7_loss: 0.0857 - val_q8_loss: 0.0519\n",
      "Epoch 61/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.4499 - q0_loss: 0.0200 - q1_loss: 0.0145 - q2_loss: 0.0184 - q3_loss: 0.0669 - q4_loss: 0.0127 - q5_loss: 0.0225 - q6_loss: 0.1126 - q7_loss: 0.1497 - q8_loss: 0.0395 - val_loss: 0.3421 - val_q0_loss: 0.0164 - val_q1_loss: 0.0122 - val_q2_loss: 0.0150 - val_q3_loss: 0.0499 - val_q4_loss: 0.0110 - val_q5_loss: 0.0153 - val_q6_loss: 0.0732 - val_q7_loss: 0.0783 - val_q8_loss: 0.0235\n",
      "Epoch 62/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.4337 - q0_loss: 0.0197 - q1_loss: 0.0127 - q2_loss: 0.0100 - q3_loss: 0.0603 - q4_loss: 0.0161 - q5_loss: 0.0315 - q6_loss: 0.1110 - q7_loss: 0.1359 - q8_loss: 0.0250 - val_loss: 0.3880 - val_q0_loss: 0.0169 - val_q1_loss: 0.0118 - val_q2_loss: 0.0223 - val_q3_loss: 0.0688 - val_q4_loss: 0.0112 - val_q5_loss: 0.0287 - val_q6_loss: 0.0607 - val_q7_loss: 0.0930 - val_q8_loss: 0.0466\n",
      "Epoch 63/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.4422 - q0_loss: 0.0190 - q1_loss: 0.0125 - q2_loss: 0.0167 - q3_loss: 0.0679 - q4_loss: 0.0147 - q5_loss: 0.0294 - q6_loss: 0.1068 - q7_loss: 0.1408 - q8_loss: 0.0339 - val_loss: 0.3491 - val_q0_loss: 0.0180 - val_q1_loss: 0.0101 - val_q2_loss: 0.0133 - val_q3_loss: 0.0380 - val_q4_loss: 0.0065 - val_q5_loss: 0.0275 - val_q6_loss: 0.0814 - val_q7_loss: 0.0779 - val_q8_loss: 0.0272\n",
      "Epoch 64/300\n",
      "269/269 [==============================] - 0s 180us/sample - loss: 0.4138 - q0_loss: 0.0185 - q1_loss: 0.0119 - q2_loss: 0.0130 - q3_loss: 0.0634 - q4_loss: 0.0104 - q5_loss: 0.0196 - q6_loss: 0.1079 - q7_loss: 0.1433 - q8_loss: 0.0288 - val_loss: 0.3447 - val_q0_loss: 0.0184 - val_q1_loss: 0.0104 - val_q2_loss: 0.0144 - val_q3_loss: 0.0363 - val_q4_loss: 0.0070 - val_q5_loss: 0.0290 - val_q6_loss: 0.0826 - val_q7_loss: 0.0775 - val_q8_loss: 0.0230\n",
      "Epoch 65/300\n",
      "269/269 [==============================] - 0s 180us/sample - loss: 0.4183 - q0_loss: 0.0192 - q1_loss: 0.0118 - q2_loss: 0.0140 - q3_loss: 0.0638 - q4_loss: 0.0102 - q5_loss: 0.0223 - q6_loss: 0.1083 - q7_loss: 0.1456 - q8_loss: 0.0317 - val_loss: 0.3631 - val_q0_loss: 0.0174 - val_q1_loss: 0.0116 - val_q2_loss: 0.0173 - val_q3_loss: 0.0582 - val_q4_loss: 0.0105 - val_q5_loss: 0.0280 - val_q6_loss: 0.0554 - val_q7_loss: 0.0883 - val_q8_loss: 0.0420\n",
      "Epoch 66/300\n",
      "269/269 [==============================] - 0s 214us/sample - loss: 0.4578 - q0_loss: 0.0194 - q1_loss: 0.0139 - q2_loss: 0.0247 - q3_loss: 0.0625 - q4_loss: 0.0123 - q5_loss: 0.0272 - q6_loss: 0.1043 - q7_loss: 0.1437 - q8_loss: 0.0498 - val_loss: 0.3734 - val_q0_loss: 0.0191 - val_q1_loss: 0.0111 - val_q2_loss: 0.0188 - val_q3_loss: 0.0362 - val_q4_loss: 0.0110 - val_q5_loss: 0.0364 - val_q6_loss: 0.0895 - val_q7_loss: 0.0751 - val_q8_loss: 0.0328\n",
      "Epoch 67/300\n",
      "269/269 [==============================] - 0s 230us/sample - loss: 0.4642 - q0_loss: 0.0203 - q1_loss: 0.0175 - q2_loss: 0.0259 - q3_loss: 0.0597 - q4_loss: 0.0178 - q5_loss: 0.0249 - q6_loss: 0.1074 - q7_loss: 0.1401 - q8_loss: 0.0481 - val_loss: 0.3390 - val_q0_loss: 0.0172 - val_q1_loss: 0.0121 - val_q2_loss: 0.0146 - val_q3_loss: 0.0529 - val_q4_loss: 0.0145 - val_q5_loss: 0.0187 - val_q6_loss: 0.0586 - val_q7_loss: 0.0769 - val_q8_loss: 0.0198\n",
      "Epoch 68/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.4033 - q0_loss: 0.0189 - q1_loss: 0.0130 - q2_loss: 0.0123 - q3_loss: 0.0652 - q4_loss: 0.0129 - q5_loss: 0.0172 - q6_loss: 0.0941 - q7_loss: 0.1340 - q8_loss: 0.0268 - val_loss: 0.3317 - val_q0_loss: 0.0189 - val_q1_loss: 0.0119 - val_q2_loss: 0.0116 - val_q3_loss: 0.0296 - val_q4_loss: 0.0099 - val_q5_loss: 0.0238 - val_q6_loss: 0.0785 - val_q7_loss: 0.0795 - val_q8_loss: 0.0258\n",
      "Epoch 69/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.3958 - q0_loss: 0.0187 - q1_loss: 0.0118 - q2_loss: 0.0127 - q3_loss: 0.0511 - q4_loss: 0.0089 - q5_loss: 0.0189 - q6_loss: 0.1041 - q7_loss: 0.1337 - q8_loss: 0.0281 - val_loss: 0.3352 - val_q0_loss: 0.0159 - val_q1_loss: 0.0109 - val_q2_loss: 0.0081 - val_q3_loss: 0.0578 - val_q4_loss: 0.0159 - val_q5_loss: 0.0261 - val_q6_loss: 0.0572 - val_q7_loss: 0.0762 - val_q8_loss: 0.0194\n",
      "Epoch 70/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.4263 - q0_loss: 0.0185 - q1_loss: 0.0130 - q2_loss: 0.0174 - q3_loss: 0.0628 - q4_loss: 0.0127 - q5_loss: 0.0262 - q6_loss: 0.0990 - q7_loss: 0.1346 - q8_loss: 0.0369 - val_loss: 0.4875 - val_q0_loss: 0.0215 - val_q1_loss: 0.0116 - val_q2_loss: 0.0301 - val_q3_loss: 0.0642 - val_q4_loss: 0.0202 - val_q5_loss: 0.0794 - val_q6_loss: 0.1064 - val_q7_loss: 0.0718 - val_q8_loss: 0.0477\n",
      "Epoch 71/300\n",
      "269/269 [==============================] - 0s 178us/sample - loss: 0.4764 - q0_loss: 0.0205 - q1_loss: 0.0117 - q2_loss: 0.0197 - q3_loss: 0.0688 - q4_loss: 0.0166 - q5_loss: 0.0495 - q6_loss: 0.1141 - q7_loss: 0.1355 - q8_loss: 0.0393 - val_loss: 0.4153 - val_q0_loss: 0.0217 - val_q1_loss: 0.0123 - val_q2_loss: 0.0208 - val_q3_loss: 0.0453 - val_q4_loss: 0.0136 - val_q5_loss: 0.0583 - val_q6_loss: 0.0975 - val_q7_loss: 0.0759 - val_q8_loss: 0.0316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.4368 - q0_loss: 0.0188 - q1_loss: 0.0121 - q2_loss: 0.0180 - q3_loss: 0.0595 - q4_loss: 0.0123 - q5_loss: 0.0359 - q6_loss: 0.1062 - q7_loss: 0.1353 - q8_loss: 0.0376 - val_loss: 0.3753 - val_q0_loss: 0.0180 - val_q1_loss: 0.0124 - val_q2_loss: 0.0226 - val_q3_loss: 0.0593 - val_q4_loss: 0.0097 - val_q5_loss: 0.0281 - val_q6_loss: 0.0577 - val_q7_loss: 0.0886 - val_q8_loss: 0.0506\n",
      "Epoch 73/300\n",
      "269/269 [==============================] - 0s 178us/sample - loss: 0.4395 - q0_loss: 0.0194 - q1_loss: 0.0126 - q2_loss: 0.0196 - q3_loss: 0.0631 - q4_loss: 0.0140 - q5_loss: 0.0340 - q6_loss: 0.1024 - q7_loss: 0.1341 - q8_loss: 0.0362 - val_loss: 0.3321 - val_q0_loss: 0.0179 - val_q1_loss: 0.0136 - val_q2_loss: 0.0117 - val_q3_loss: 0.0515 - val_q4_loss: 0.0125 - val_q5_loss: 0.0178 - val_q6_loss: 0.0639 - val_q7_loss: 0.0776 - val_q8_loss: 0.0258\n",
      "Epoch 74/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.4063 - q0_loss: 0.0195 - q1_loss: 0.0139 - q2_loss: 0.0163 - q3_loss: 0.0573 - q4_loss: 0.0142 - q5_loss: 0.0198 - q6_loss: 0.0971 - q7_loss: 0.1315 - q8_loss: 0.0338 - val_loss: 0.3983 - val_q0_loss: 0.0166 - val_q1_loss: 0.0161 - val_q2_loss: 0.0279 - val_q3_loss: 0.0536 - val_q4_loss: 0.0243 - val_q5_loss: 0.0246 - val_q6_loss: 0.0674 - val_q7_loss: 0.0752 - val_q8_loss: 0.0455\n",
      "Epoch 75/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.4612 - q0_loss: 0.0210 - q1_loss: 0.0202 - q2_loss: 0.0319 - q3_loss: 0.0527 - q4_loss: 0.0218 - q5_loss: 0.0197 - q6_loss: 0.0982 - q7_loss: 0.1377 - q8_loss: 0.0585 - val_loss: 0.4247 - val_q0_loss: 0.0163 - val_q1_loss: 0.0237 - val_q2_loss: 0.0445 - val_q3_loss: 0.0500 - val_q4_loss: 0.0258 - val_q5_loss: 0.0094 - val_q6_loss: 0.0673 - val_q7_loss: 0.0855 - val_q8_loss: 0.0763\n",
      "Epoch 76/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.4273 - q0_loss: 0.0208 - q1_loss: 0.0172 - q2_loss: 0.0274 - q3_loss: 0.0536 - q4_loss: 0.0156 - q5_loss: 0.0095 - q6_loss: 0.0902 - q7_loss: 0.1340 - q8_loss: 0.0498 - val_loss: 0.3389 - val_q0_loss: 0.0164 - val_q1_loss: 0.0145 - val_q2_loss: 0.0188 - val_q3_loss: 0.0523 - val_q4_loss: 0.0215 - val_q5_loss: 0.0187 - val_q6_loss: 0.0529 - val_q7_loss: 0.0690 - val_q8_loss: 0.0266\n",
      "Epoch 77/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.3851 - q0_loss: 0.0195 - q1_loss: 0.0124 - q2_loss: 0.0128 - q3_loss: 0.0534 - q4_loss: 0.0126 - q5_loss: 0.0206 - q6_loss: 0.0970 - q7_loss: 0.1330 - q8_loss: 0.0279 - val_loss: 0.3152 - val_q0_loss: 0.0177 - val_q1_loss: 0.0119 - val_q2_loss: 0.0095 - val_q3_loss: 0.0404 - val_q4_loss: 0.0106 - val_q5_loss: 0.0199 - val_q6_loss: 0.0572 - val_q7_loss: 0.0806 - val_q8_loss: 0.0263\n",
      "Epoch 78/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.3736 - q0_loss: 0.0194 - q1_loss: 0.0119 - q2_loss: 0.0118 - q3_loss: 0.0521 - q4_loss: 0.0106 - q5_loss: 0.0177 - q6_loss: 0.0948 - q7_loss: 0.1296 - q8_loss: 0.0267 - val_loss: 0.3028 - val_q0_loss: 0.0178 - val_q1_loss: 0.0110 - val_q2_loss: 0.0126 - val_q3_loss: 0.0343 - val_q4_loss: 0.0084 - val_q5_loss: 0.0154 - val_q6_loss: 0.0588 - val_q7_loss: 0.0744 - val_q8_loss: 0.0278\n",
      "Epoch 79/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.3645 - q0_loss: 0.0185 - q1_loss: 0.0114 - q2_loss: 0.0098 - q3_loss: 0.0526 - q4_loss: 0.0103 - q5_loss: 0.0168 - q6_loss: 0.0918 - q7_loss: 0.1334 - q8_loss: 0.0254 - val_loss: 0.2909 - val_q0_loss: 0.0164 - val_q1_loss: 0.0100 - val_q2_loss: 0.0108 - val_q3_loss: 0.0386 - val_q4_loss: 0.0103 - val_q5_loss: 0.0086 - val_q6_loss: 0.0518 - val_q7_loss: 0.0738 - val_q8_loss: 0.0215\n",
      "Epoch 80/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.4253 - q0_loss: 0.0222 - q1_loss: 0.0170 - q2_loss: 0.0160 - q3_loss: 0.0560 - q4_loss: 0.0231 - q5_loss: 0.0313 - q6_loss: 0.0969 - q7_loss: 0.1253 - q8_loss: 0.0313 - val_loss: 0.4501 - val_q0_loss: 0.0225 - val_q1_loss: 0.0206 - val_q2_loss: 0.0169 - val_q3_loss: 0.0679 - val_q4_loss: 0.0407 - val_q5_loss: 0.0567 - val_q6_loss: 0.0691 - val_q7_loss: 0.0711 - val_q8_loss: 0.0268\n",
      "Epoch 81/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.5097 - q0_loss: 0.0220 - q1_loss: 0.0169 - q2_loss: 0.0268 - q3_loss: 0.0666 - q4_loss: 0.0260 - q5_loss: 0.0596 - q6_loss: 0.1117 - q7_loss: 0.1315 - q8_loss: 0.0518 - val_loss: 0.6648 - val_q0_loss: 0.0216 - val_q1_loss: 0.0113 - val_q2_loss: 0.0683 - val_q3_loss: 0.0715 - val_q4_loss: 0.0202 - val_q5_loss: 0.1140 - val_q6_loss: 0.1285 - val_q7_loss: 0.0861 - val_q8_loss: 0.1160\n",
      "Epoch 82/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.5802 - q0_loss: 0.0216 - q1_loss: 0.0183 - q2_loss: 0.0482 - q3_loss: 0.0642 - q4_loss: 0.0198 - q5_loss: 0.0672 - q6_loss: 0.1159 - q7_loss: 0.1327 - q8_loss: 0.0896 - val_loss: 0.4148 - val_q0_loss: 0.0229 - val_q1_loss: 0.0200 - val_q2_loss: 0.0380 - val_q3_loss: 0.0548 - val_q4_loss: 0.0161 - val_q5_loss: 0.0250 - val_q6_loss: 0.0518 - val_q7_loss: 0.1014 - val_q8_loss: 0.0715\n",
      "Epoch 83/300\n",
      "269/269 [==============================] - 0s 172us/sample - loss: 0.4258 - q0_loss: 0.0231 - q1_loss: 0.0163 - q2_loss: 0.0209 - q3_loss: 0.0502 - q4_loss: 0.0181 - q5_loss: 0.0300 - q6_loss: 0.0936 - q7_loss: 0.1347 - q8_loss: 0.0416 - val_loss: 0.3607 - val_q0_loss: 0.0224 - val_q1_loss: 0.0186 - val_q2_loss: 0.0277 - val_q3_loss: 0.0422 - val_q4_loss: 0.0154 - val_q5_loss: 0.0104 - val_q6_loss: 0.0634 - val_q7_loss: 0.0943 - val_q8_loss: 0.0520\n",
      "Epoch 84/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.4022 - q0_loss: 0.0205 - q1_loss: 0.0155 - q2_loss: 0.0234 - q3_loss: 0.0502 - q4_loss: 0.0164 - q5_loss: 0.0126 - q6_loss: 0.0918 - q7_loss: 0.1325 - q8_loss: 0.0457 - val_loss: 0.3574 - val_q0_loss: 0.0227 - val_q1_loss: 0.0183 - val_q2_loss: 0.0259 - val_q3_loss: 0.0361 - val_q4_loss: 0.0168 - val_q5_loss: 0.0130 - val_q6_loss: 0.0684 - val_q7_loss: 0.0877 - val_q8_loss: 0.0490\n",
      "Epoch 85/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.3986 - q0_loss: 0.0197 - q1_loss: 0.0155 - q2_loss: 0.0243 - q3_loss: 0.0454 - q4_loss: 0.0143 - q5_loss: 0.0135 - q6_loss: 0.0918 - q7_loss: 0.1280 - q8_loss: 0.0449 - val_loss: 0.3599 - val_q0_loss: 0.0192 - val_q1_loss: 0.0142 - val_q2_loss: 0.0267 - val_q3_loss: 0.0531 - val_q4_loss: 0.0074 - val_q5_loss: 0.0265 - val_q6_loss: 0.0478 - val_q7_loss: 0.0906 - val_q8_loss: 0.0544\n",
      "Epoch 86/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.3858 - q0_loss: 0.0195 - q1_loss: 0.0133 - q2_loss: 0.0220 - q3_loss: 0.0457 - q4_loss: 0.0105 - q5_loss: 0.0161 - q6_loss: 0.0888 - q7_loss: 0.1265 - q8_loss: 0.0456 - val_loss: 0.3307 - val_q0_loss: 0.0196 - val_q1_loss: 0.0161 - val_q2_loss: 0.0195 - val_q3_loss: 0.0307 - val_q4_loss: 0.0139 - val_q5_loss: 0.0087 - val_q6_loss: 0.0625 - val_q7_loss: 0.0845 - val_q8_loss: 0.0430\n",
      "Epoch 87/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.3527 - q0_loss: 0.0185 - q1_loss: 0.0125 - q2_loss: 0.0147 - q3_loss: 0.0446 - q4_loss: 0.0097 - q5_loss: 0.0133 - q6_loss: 0.0852 - q7_loss: 0.1236 - q8_loss: 0.0310 - val_loss: 0.3864 - val_q0_loss: 0.0165 - val_q1_loss: 0.0180 - val_q2_loss: 0.0457 - val_q3_loss: 0.0237 - val_q4_loss: 0.0139 - val_q5_loss: 0.0214 - val_q6_loss: 0.0704 - val_q7_loss: 0.0698 - val_q8_loss: 0.0716\n",
      "Epoch 88/300\n",
      "269/269 [==============================] - 0s 178us/sample - loss: 0.4143 - q0_loss: 0.0197 - q1_loss: 0.0150 - q2_loss: 0.0306 - q3_loss: 0.0414 - q4_loss: 0.0114 - q5_loss: 0.0234 - q6_loss: 0.0888 - q7_loss: 0.1277 - q8_loss: 0.0577 - val_loss: 0.3669 - val_q0_loss: 0.0175 - val_q1_loss: 0.0205 - val_q2_loss: 0.0314 - val_q3_loss: 0.0263 - val_q4_loss: 0.0186 - val_q5_loss: 0.0188 - val_q6_loss: 0.0641 - val_q7_loss: 0.0758 - val_q8_loss: 0.0585\n",
      "Epoch 89/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.3881 - q0_loss: 0.0215 - q1_loss: 0.0166 - q2_loss: 0.0231 - q3_loss: 0.0437 - q4_loss: 0.0173 - q5_loss: 0.0183 - q6_loss: 0.0836 - q7_loss: 0.1235 - q8_loss: 0.0423 - val_loss: 0.3880 - val_q0_loss: 0.0164 - val_q1_loss: 0.0168 - val_q2_loss: 0.0352 - val_q3_loss: 0.0327 - val_q4_loss: 0.0089 - val_q5_loss: 0.0277 - val_q6_loss: 0.0600 - val_q7_loss: 0.0781 - val_q8_loss: 0.0690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/300\n",
      "269/269 [==============================] - 0s 174us/sample - loss: 0.3878 - q0_loss: 0.0191 - q1_loss: 0.0129 - q2_loss: 0.0202 - q3_loss: 0.0438 - q4_loss: 0.0125 - q5_loss: 0.0266 - q6_loss: 0.0886 - q7_loss: 0.1192 - q8_loss: 0.0399 - val_loss: 0.2990 - val_q0_loss: 0.0180 - val_q1_loss: 0.0108 - val_q2_loss: 0.0145 - val_q3_loss: 0.0258 - val_q4_loss: 0.0070 - val_q5_loss: 0.0196 - val_q6_loss: 0.0612 - val_q7_loss: 0.0717 - val_q8_loss: 0.0320\n",
      "Epoch 91/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.3545 - q0_loss: 0.0189 - q1_loss: 0.0116 - q2_loss: 0.0129 - q3_loss: 0.0415 - q4_loss: 0.0110 - q5_loss: 0.0245 - q6_loss: 0.0841 - q7_loss: 0.1144 - q8_loss: 0.0303 - val_loss: 0.3153 - val_q0_loss: 0.0166 - val_q1_loss: 0.0111 - val_q2_loss: 0.0182 - val_q3_loss: 0.0483 - val_q4_loss: 0.0090 - val_q5_loss: 0.0279 - val_q6_loss: 0.0502 - val_q7_loss: 0.0797 - val_q8_loss: 0.0350\n",
      "Epoch 92/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.3738 - q0_loss: 0.0194 - q1_loss: 0.0123 - q2_loss: 0.0110 - q3_loss: 0.0457 - q4_loss: 0.0154 - q5_loss: 0.0334 - q6_loss: 0.0817 - q7_loss: 0.1155 - q8_loss: 0.0277 - val_loss: 0.4593 - val_q0_loss: 0.0247 - val_q1_loss: 0.0272 - val_q2_loss: 0.0343 - val_q3_loss: 0.0681 - val_q4_loss: 0.0419 - val_q5_loss: 0.0481 - val_q6_loss: 0.0537 - val_q7_loss: 0.0709 - val_q8_loss: 0.0517\n",
      "Epoch 93/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.4426 - q0_loss: 0.0248 - q1_loss: 0.0224 - q2_loss: 0.0322 - q3_loss: 0.0440 - q4_loss: 0.0284 - q5_loss: 0.0311 - q6_loss: 0.0836 - q7_loss: 0.1254 - q8_loss: 0.0584 - val_loss: 0.3781 - val_q0_loss: 0.0188 - val_q1_loss: 0.0123 - val_q2_loss: 0.0245 - val_q3_loss: 0.0596 - val_q4_loss: 0.0132 - val_q5_loss: 0.0459 - val_q6_loss: 0.0500 - val_q7_loss: 0.0802 - val_q8_loss: 0.0506\n",
      "Epoch 94/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.4460 - q0_loss: 0.0207 - q1_loss: 0.0171 - q2_loss: 0.0333 - q3_loss: 0.0433 - q4_loss: 0.0175 - q5_loss: 0.0426 - q6_loss: 0.0917 - q7_loss: 0.1211 - q8_loss: 0.0610 - val_loss: 0.2926 - val_q0_loss: 0.0212 - val_q1_loss: 0.0151 - val_q2_loss: 0.0133 - val_q3_loss: 0.0221 - val_q4_loss: 0.0135 - val_q5_loss: 0.0212 - val_q6_loss: 0.0679 - val_q7_loss: 0.0742 - val_q8_loss: 0.0267\n",
      "Epoch 95/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.4494 - q0_loss: 0.0186 - q1_loss: 0.0125 - q2_loss: 0.0292 - q3_loss: 0.0497 - q4_loss: 0.0141 - q5_loss: 0.0502 - q6_loss: 0.0910 - q7_loss: 0.1184 - q8_loss: 0.0564 - val_loss: 0.4541 - val_q0_loss: 0.0182 - val_q1_loss: 0.0118 - val_q2_loss: 0.0508 - val_q3_loss: 0.0290 - val_q4_loss: 0.0105 - val_q5_loss: 0.0566 - val_q6_loss: 0.0924 - val_q7_loss: 0.0689 - val_q8_loss: 0.0843\n",
      "Epoch 96/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.4797 - q0_loss: 0.0230 - q1_loss: 0.0239 - q2_loss: 0.0498 - q3_loss: 0.0388 - q4_loss: 0.0216 - q5_loss: 0.0257 - q6_loss: 0.0825 - q7_loss: 0.1244 - q8_loss: 0.0894 - val_loss: 0.4274 - val_q0_loss: 0.0276 - val_q1_loss: 0.0302 - val_q2_loss: 0.0425 - val_q3_loss: 0.0290 - val_q4_loss: 0.0298 - val_q5_loss: 0.0169 - val_q6_loss: 0.0656 - val_q7_loss: 0.1038 - val_q8_loss: 0.0796\n",
      "Epoch 97/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.4833 - q0_loss: 0.0228 - q1_loss: 0.0250 - q2_loss: 0.0528 - q3_loss: 0.0372 - q4_loss: 0.0209 - q5_loss: 0.0234 - q6_loss: 0.0761 - q7_loss: 0.1260 - q8_loss: 0.0965 - val_loss: 0.3472 - val_q0_loss: 0.0196 - val_q1_loss: 0.0171 - val_q2_loss: 0.0278 - val_q3_loss: 0.0285 - val_q4_loss: 0.0127 - val_q5_loss: 0.0139 - val_q6_loss: 0.0598 - val_q7_loss: 0.0921 - val_q8_loss: 0.0568\n",
      "Epoch 98/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.4074 - q0_loss: 0.0207 - q1_loss: 0.0186 - q2_loss: 0.0374 - q3_loss: 0.0311 - q4_loss: 0.0163 - q5_loss: 0.0193 - q6_loss: 0.0800 - q7_loss: 0.1126 - q8_loss: 0.0662 - val_loss: 0.3001 - val_q0_loss: 0.0167 - val_q1_loss: 0.0133 - val_q2_loss: 0.0151 - val_q3_loss: 0.0424 - val_q4_loss: 0.0185 - val_q5_loss: 0.0217 - val_q6_loss: 0.0436 - val_q7_loss: 0.0703 - val_q8_loss: 0.0273\n",
      "Epoch 99/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.4169 - q0_loss: 0.0222 - q1_loss: 0.0197 - q2_loss: 0.0343 - q3_loss: 0.0378 - q4_loss: 0.0219 - q5_loss: 0.0276 - q6_loss: 0.0770 - q7_loss: 0.1175 - q8_loss: 0.0643 - val_loss: 0.3558 - val_q0_loss: 0.0166 - val_q1_loss: 0.0120 - val_q2_loss: 0.0169 - val_q3_loss: 0.0573 - val_q4_loss: 0.0227 - val_q5_loss: 0.0473 - val_q6_loss: 0.0497 - val_q7_loss: 0.0809 - val_q8_loss: 0.0334\n",
      "Epoch 100/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.3996 - q0_loss: 0.0196 - q1_loss: 0.0130 - q2_loss: 0.0280 - q3_loss: 0.0408 - q4_loss: 0.0121 - q5_loss: 0.0376 - q6_loss: 0.0858 - q7_loss: 0.1190 - q8_loss: 0.0531 - val_loss: 0.3186 - val_q0_loss: 0.0160 - val_q1_loss: 0.0103 - val_q2_loss: 0.0151 - val_q3_loss: 0.0494 - val_q4_loss: 0.0134 - val_q5_loss: 0.0421 - val_q6_loss: 0.0414 - val_q7_loss: 0.0801 - val_q8_loss: 0.0303\n",
      "Epoch 101/300\n",
      "269/269 [==============================] - 0s 165us/sample - loss: 0.3563 - q0_loss: 0.0204 - q1_loss: 0.0152 - q2_loss: 0.0211 - q3_loss: 0.0342 - q4_loss: 0.0145 - q5_loss: 0.0221 - q6_loss: 0.0752 - q7_loss: 0.1128 - q8_loss: 0.0432 - val_loss: 0.3090 - val_q0_loss: 0.0160 - val_q1_loss: 0.0135 - val_q2_loss: 0.0244 - val_q3_loss: 0.0419 - val_q4_loss: 0.0200 - val_q5_loss: 0.0192 - val_q6_loss: 0.0444 - val_q7_loss: 0.0590 - val_q8_loss: 0.0448\n",
      "Epoch 102/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.3360 - q0_loss: 0.0196 - q1_loss: 0.0141 - q2_loss: 0.0175 - q3_loss: 0.0305 - q4_loss: 0.0134 - q5_loss: 0.0184 - q6_loss: 0.0722 - q7_loss: 0.1090 - q8_loss: 0.0357 - val_loss: 0.2710 - val_q0_loss: 0.0163 - val_q1_loss: 0.0122 - val_q2_loss: 0.0191 - val_q3_loss: 0.0233 - val_q4_loss: 0.0093 - val_q5_loss: 0.0107 - val_q6_loss: 0.0581 - val_q7_loss: 0.0638 - val_q8_loss: 0.0327\n",
      "Epoch 103/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.3056 - q0_loss: 0.0180 - q1_loss: 0.0124 - q2_loss: 0.0133 - q3_loss: 0.0273 - q4_loss: 0.0101 - q5_loss: 0.0120 - q6_loss: 0.0712 - q7_loss: 0.1084 - q8_loss: 0.0311 - val_loss: 0.2475 - val_q0_loss: 0.0186 - val_q1_loss: 0.0115 - val_q2_loss: 0.0104 - val_q3_loss: 0.0217 - val_q4_loss: 0.0090 - val_q5_loss: 0.0083 - val_q6_loss: 0.0517 - val_q7_loss: 0.0711 - val_q8_loss: 0.0210\n",
      "Epoch 104/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.3046 - q0_loss: 0.0197 - q1_loss: 0.0132 - q2_loss: 0.0141 - q3_loss: 0.0258 - q4_loss: 0.0118 - q5_loss: 0.0121 - q6_loss: 0.0690 - q7_loss: 0.1068 - q8_loss: 0.0303 - val_loss: 0.3029 - val_q0_loss: 0.0171 - val_q1_loss: 0.0125 - val_q2_loss: 0.0230 - val_q3_loss: 0.0344 - val_q4_loss: 0.0161 - val_q5_loss: 0.0207 - val_q6_loss: 0.0365 - val_q7_loss: 0.0655 - val_q8_loss: 0.0384\n",
      "Epoch 105/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.3398 - q0_loss: 0.0193 - q1_loss: 0.0149 - q2_loss: 0.0250 - q3_loss: 0.0266 - q4_loss: 0.0118 - q5_loss: 0.0164 - q6_loss: 0.0694 - q7_loss: 0.1109 - q8_loss: 0.0501 - val_loss: 0.2967 - val_q0_loss: 0.0231 - val_q1_loss: 0.0203 - val_q2_loss: 0.0222 - val_q3_loss: 0.0155 - val_q4_loss: 0.0183 - val_q5_loss: 0.0155 - val_q6_loss: 0.0517 - val_q7_loss: 0.0822 - val_q8_loss: 0.0401\n",
      "Epoch 106/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.3223 - q0_loss: 0.0196 - q1_loss: 0.0143 - q2_loss: 0.0203 - q3_loss: 0.0237 - q4_loss: 0.0132 - q5_loss: 0.0141 - q6_loss: 0.0685 - q7_loss: 0.1102 - q8_loss: 0.0383 - val_loss: 0.3950 - val_q0_loss: 0.0307 - val_q1_loss: 0.0295 - val_q2_loss: 0.0361 - val_q3_loss: 0.0220 - val_q4_loss: 0.0353 - val_q5_loss: 0.0310 - val_q6_loss: 0.0566 - val_q7_loss: 0.0903 - val_q8_loss: 0.0650\n",
      "Epoch 107/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.4000 - q0_loss: 0.0237 - q1_loss: 0.0203 - q2_loss: 0.0311 - q3_loss: 0.0328 - q4_loss: 0.0224 - q5_loss: 0.0307 - q6_loss: 0.0723 - q7_loss: 0.1116 - q8_loss: 0.0583 - val_loss: 0.4069 - val_q0_loss: 0.0185 - val_q1_loss: 0.0110 - val_q2_loss: 0.0386 - val_q3_loss: 0.0378 - val_q4_loss: 0.0106 - val_q5_loss: 0.0590 - val_q6_loss: 0.0703 - val_q7_loss: 0.0632 - val_q8_loss: 0.0721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.3728 - q0_loss: 0.0196 - q1_loss: 0.0139 - q2_loss: 0.0278 - q3_loss: 0.0339 - q4_loss: 0.0155 - q5_loss: 0.0344 - q6_loss: 0.0681 - q7_loss: 0.0977 - q8_loss: 0.0525 - val_loss: 0.3132 - val_q0_loss: 0.0247 - val_q1_loss: 0.0189 - val_q2_loss: 0.0147 - val_q3_loss: 0.0282 - val_q4_loss: 0.0229 - val_q5_loss: 0.0375 - val_q6_loss: 0.0609 - val_q7_loss: 0.0804 - val_q8_loss: 0.0277\n",
      "Epoch 109/300\n",
      "269/269 [==============================] - 0s 174us/sample - loss: 0.3711 - q0_loss: 0.0225 - q1_loss: 0.0194 - q2_loss: 0.0245 - q3_loss: 0.0302 - q4_loss: 0.0240 - q5_loss: 0.0288 - q6_loss: 0.0656 - q7_loss: 0.1057 - q8_loss: 0.0462 - val_loss: 0.3462 - val_q0_loss: 0.0257 - val_q1_loss: 0.0215 - val_q2_loss: 0.0097 - val_q3_loss: 0.0312 - val_q4_loss: 0.0296 - val_q5_loss: 0.0538 - val_q6_loss: 0.0710 - val_q7_loss: 0.0744 - val_q8_loss: 0.0224\n",
      "Epoch 110/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.3478 - q0_loss: 0.0218 - q1_loss: 0.0150 - q2_loss: 0.0193 - q3_loss: 0.0326 - q4_loss: 0.0180 - q5_loss: 0.0317 - q6_loss: 0.0644 - q7_loss: 0.1017 - q8_loss: 0.0413 - val_loss: 0.3199 - val_q0_loss: 0.0186 - val_q1_loss: 0.0126 - val_q2_loss: 0.0255 - val_q3_loss: 0.0268 - val_q4_loss: 0.0131 - val_q5_loss: 0.0385 - val_q6_loss: 0.0648 - val_q7_loss: 0.0534 - val_q8_loss: 0.0457\n",
      "Epoch 111/300\n",
      "269/269 [==============================] - 0s 165us/sample - loss: 0.3668 - q0_loss: 0.0202 - q1_loss: 0.0142 - q2_loss: 0.0302 - q3_loss: 0.0334 - q4_loss: 0.0137 - q5_loss: 0.0338 - q6_loss: 0.0651 - q7_loss: 0.1032 - q8_loss: 0.0563 - val_loss: 0.3004 - val_q0_loss: 0.0224 - val_q1_loss: 0.0185 - val_q2_loss: 0.0196 - val_q3_loss: 0.0169 - val_q4_loss: 0.0199 - val_q5_loss: 0.0222 - val_q6_loss: 0.0587 - val_q7_loss: 0.0779 - val_q8_loss: 0.0403\n",
      "Epoch 112/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.3365 - q0_loss: 0.0222 - q1_loss: 0.0166 - q2_loss: 0.0208 - q3_loss: 0.0298 - q4_loss: 0.0186 - q5_loss: 0.0265 - q6_loss: 0.0590 - q7_loss: 0.0983 - q8_loss: 0.0421 - val_loss: 0.3011 - val_q0_loss: 0.0224 - val_q1_loss: 0.0168 - val_q2_loss: 0.0086 - val_q3_loss: 0.0202 - val_q4_loss: 0.0213 - val_q5_loss: 0.0385 - val_q6_loss: 0.0684 - val_q7_loss: 0.0688 - val_q8_loss: 0.0196\n",
      "Epoch 113/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.3056 - q0_loss: 0.0203 - q1_loss: 0.0147 - q2_loss: 0.0188 - q3_loss: 0.0218 - q4_loss: 0.0132 - q5_loss: 0.0196 - q6_loss: 0.0634 - q7_loss: 0.0992 - q8_loss: 0.0379 - val_loss: 0.2712 - val_q0_loss: 0.0220 - val_q1_loss: 0.0175 - val_q2_loss: 0.0190 - val_q3_loss: 0.0129 - val_q4_loss: 0.0142 - val_q5_loss: 0.0120 - val_q6_loss: 0.0543 - val_q7_loss: 0.0786 - val_q8_loss: 0.0343\n",
      "Epoch 114/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.3412 - q0_loss: 0.0213 - q1_loss: 0.0157 - q2_loss: 0.0232 - q3_loss: 0.0297 - q4_loss: 0.0173 - q5_loss: 0.0306 - q6_loss: 0.0600 - q7_loss: 0.1000 - q8_loss: 0.0457 - val_loss: 0.3225 - val_q0_loss: 0.0184 - val_q1_loss: 0.0130 - val_q2_loss: 0.0193 - val_q3_loss: 0.0367 - val_q4_loss: 0.0133 - val_q5_loss: 0.0411 - val_q6_loss: 0.0451 - val_q7_loss: 0.0699 - val_q8_loss: 0.0358\n",
      "Epoch 115/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.3576 - q0_loss: 0.0193 - q1_loss: 0.0121 - q2_loss: 0.0253 - q3_loss: 0.0389 - q4_loss: 0.0147 - q5_loss: 0.0427 - q6_loss: 0.0594 - q7_loss: 0.0943 - q8_loss: 0.0493 - val_loss: 0.3204 - val_q0_loss: 0.0167 - val_q1_loss: 0.0103 - val_q2_loss: 0.0256 - val_q3_loss: 0.0379 - val_q4_loss: 0.0121 - val_q5_loss: 0.0409 - val_q6_loss: 0.0452 - val_q7_loss: 0.0606 - val_q8_loss: 0.0427\n",
      "Epoch 116/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.2891 - q0_loss: 0.0189 - q1_loss: 0.0114 - q2_loss: 0.0152 - q3_loss: 0.0275 - q4_loss: 0.0104 - q5_loss: 0.0246 - q6_loss: 0.0516 - q7_loss: 0.0950 - q8_loss: 0.0320 - val_loss: 0.2344 - val_q0_loss: 0.0184 - val_q1_loss: 0.0113 - val_q2_loss: 0.0105 - val_q3_loss: 0.0116 - val_q4_loss: 0.0089 - val_q5_loss: 0.0142 - val_q6_loss: 0.0574 - val_q7_loss: 0.0601 - val_q8_loss: 0.0207\n",
      "Epoch 117/300\n",
      "269/269 [==============================] - 0s 184us/sample - loss: 0.2611 - q0_loss: 0.0187 - q1_loss: 0.0114 - q2_loss: 0.0095 - q3_loss: 0.0169 - q4_loss: 0.0089 - q5_loss: 0.0153 - q6_loss: 0.0601 - q7_loss: 0.0961 - q8_loss: 0.0247 - val_loss: 0.2384 - val_q0_loss: 0.0162 - val_q1_loss: 0.0102 - val_q2_loss: 0.0084 - val_q3_loss: 0.0346 - val_q4_loss: 0.0112 - val_q5_loss: 0.0331 - val_q6_loss: 0.0280 - val_q7_loss: 0.0618 - val_q8_loss: 0.0212\n",
      "Epoch 118/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.2653 - q0_loss: 0.0193 - q1_loss: 0.0121 - q2_loss: 0.0104 - q3_loss: 0.0250 - q4_loss: 0.0117 - q5_loss: 0.0213 - q6_loss: 0.0487 - q7_loss: 0.0905 - q8_loss: 0.0268 - val_loss: 0.2288 - val_q0_loss: 0.0188 - val_q1_loss: 0.0140 - val_q2_loss: 0.0118 - val_q3_loss: 0.0167 - val_q4_loss: 0.0097 - val_q5_loss: 0.0131 - val_q6_loss: 0.0393 - val_q7_loss: 0.0651 - val_q8_loss: 0.0278\n",
      "Epoch 119/300\n",
      "269/269 [==============================] - 0s 170us/sample - loss: 0.2657 - q0_loss: 0.0199 - q1_loss: 0.0138 - q2_loss: 0.0121 - q3_loss: 0.0179 - q4_loss: 0.0139 - q5_loss: 0.0165 - q6_loss: 0.0548 - q7_loss: 0.0921 - q8_loss: 0.0290 - val_loss: 0.2130 - val_q0_loss: 0.0169 - val_q1_loss: 0.0125 - val_q2_loss: 0.0116 - val_q3_loss: 0.0157 - val_q4_loss: 0.0063 - val_q5_loss: 0.0110 - val_q6_loss: 0.0366 - val_q7_loss: 0.0587 - val_q8_loss: 0.0289\n",
      "Epoch 120/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.2571 - q0_loss: 0.0191 - q1_loss: 0.0124 - q2_loss: 0.0117 - q3_loss: 0.0207 - q4_loss: 0.0122 - q5_loss: 0.0173 - q6_loss: 0.0454 - q7_loss: 0.0855 - q8_loss: 0.0291 - val_loss: 0.2196 - val_q0_loss: 0.0159 - val_q1_loss: 0.0114 - val_q2_loss: 0.0166 - val_q3_loss: 0.0193 - val_q4_loss: 0.0129 - val_q5_loss: 0.0126 - val_q6_loss: 0.0294 - val_q7_loss: 0.0453 - val_q8_loss: 0.0306\n",
      "Epoch 121/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.2521 - q0_loss: 0.0192 - q1_loss: 0.0121 - q2_loss: 0.0153 - q3_loss: 0.0162 - q4_loss: 0.0082 - q5_loss: 0.0142 - q6_loss: 0.0481 - q7_loss: 0.0875 - q8_loss: 0.0348 - val_loss: 0.2731 - val_q0_loss: 0.0164 - val_q1_loss: 0.0148 - val_q2_loss: 0.0288 - val_q3_loss: 0.0150 - val_q4_loss: 0.0150 - val_q5_loss: 0.0146 - val_q6_loss: 0.0306 - val_q7_loss: 0.0576 - val_q8_loss: 0.0490\n",
      "Epoch 122/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.2709 - q0_loss: 0.0193 - q1_loss: 0.0140 - q2_loss: 0.0198 - q3_loss: 0.0140 - q4_loss: 0.0112 - q5_loss: 0.0115 - q6_loss: 0.0464 - q7_loss: 0.0933 - q8_loss: 0.0400 - val_loss: 0.2427 - val_q0_loss: 0.0182 - val_q1_loss: 0.0119 - val_q2_loss: 0.0231 - val_q3_loss: 0.0081 - val_q4_loss: 0.0116 - val_q5_loss: 0.0096 - val_q6_loss: 0.0345 - val_q7_loss: 0.0526 - val_q8_loss: 0.0382\n",
      "Epoch 123/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.2753 - q0_loss: 0.0198 - q1_loss: 0.0144 - q2_loss: 0.0233 - q3_loss: 0.0141 - q4_loss: 0.0147 - q5_loss: 0.0117 - q6_loss: 0.0453 - q7_loss: 0.0920 - q8_loss: 0.0449 - val_loss: 0.2603 - val_q0_loss: 0.0178 - val_q1_loss: 0.0114 - val_q2_loss: 0.0114 - val_q3_loss: 0.0306 - val_q4_loss: 0.0167 - val_q5_loss: 0.0320 - val_q6_loss: 0.0386 - val_q7_loss: 0.0544 - val_q8_loss: 0.0220\n",
      "Epoch 124/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.3095 - q0_loss: 0.0201 - q1_loss: 0.0134 - q2_loss: 0.0195 - q3_loss: 0.0284 - q4_loss: 0.0150 - q5_loss: 0.0344 - q6_loss: 0.0550 - q7_loss: 0.0846 - q8_loss: 0.0412 - val_loss: 0.3589 - val_q0_loss: 0.0166 - val_q1_loss: 0.0113 - val_q2_loss: 0.0157 - val_q3_loss: 0.0602 - val_q4_loss: 0.0257 - val_q5_loss: 0.0663 - val_q6_loss: 0.0571 - val_q7_loss: 0.0567 - val_q8_loss: 0.0332\n",
      "Epoch 125/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.3519 - q0_loss: 0.0244 - q1_loss: 0.0173 - q2_loss: 0.0124 - q3_loss: 0.0485 - q4_loss: 0.0270 - q5_loss: 0.0570 - q6_loss: 0.0575 - q7_loss: 0.0812 - q8_loss: 0.0316 - val_loss: 0.4463 - val_q0_loss: 0.0342 - val_q1_loss: 0.0307 - val_q2_loss: 0.0095 - val_q3_loss: 0.0707 - val_q4_loss: 0.0500 - val_q5_loss: 0.0943 - val_q6_loss: 0.0930 - val_q7_loss: 0.0721 - val_q8_loss: 0.0198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/300\n",
      "269/269 [==============================] - 0s 165us/sample - loss: 0.3841 - q0_loss: 0.0250 - q1_loss: 0.0192 - q2_loss: 0.0104 - q3_loss: 0.0558 - q4_loss: 0.0339 - q5_loss: 0.0673 - q6_loss: 0.0601 - q7_loss: 0.0823 - q8_loss: 0.0265 - val_loss: 0.3250 - val_q0_loss: 0.0202 - val_q1_loss: 0.0140 - val_q2_loss: 0.0104 - val_q3_loss: 0.0436 - val_q4_loss: 0.0247 - val_q5_loss: 0.0496 - val_q6_loss: 0.0469 - val_q7_loss: 0.0515 - val_q8_loss: 0.0266\n",
      "Epoch 127/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.2989 - q0_loss: 0.0215 - q1_loss: 0.0146 - q2_loss: 0.0117 - q3_loss: 0.0361 - q4_loss: 0.0211 - q5_loss: 0.0412 - q6_loss: 0.0441 - q7_loss: 0.0774 - q8_loss: 0.0287 - val_loss: 0.2614 - val_q0_loss: 0.0183 - val_q1_loss: 0.0114 - val_q2_loss: 0.0117 - val_q3_loss: 0.0291 - val_q4_loss: 0.0207 - val_q5_loss: 0.0344 - val_q6_loss: 0.0416 - val_q7_loss: 0.0459 - val_q8_loss: 0.0213\n",
      "Epoch 128/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.2524 - q0_loss: 0.0205 - q1_loss: 0.0135 - q2_loss: 0.0107 - q3_loss: 0.0211 - q4_loss: 0.0149 - q5_loss: 0.0231 - q6_loss: 0.0422 - q7_loss: 0.0824 - q8_loss: 0.0287 - val_loss: 0.2166 - val_q0_loss: 0.0177 - val_q1_loss: 0.0118 - val_q2_loss: 0.0073 - val_q3_loss: 0.0284 - val_q4_loss: 0.0169 - val_q5_loss: 0.0305 - val_q6_loss: 0.0262 - val_q7_loss: 0.0471 - val_q8_loss: 0.0178\n",
      "Epoch 129/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.2507 - q0_loss: 0.0202 - q1_loss: 0.0137 - q2_loss: 0.0107 - q3_loss: 0.0254 - q4_loss: 0.0162 - q5_loss: 0.0275 - q6_loss: 0.0372 - q7_loss: 0.0793 - q8_loss: 0.0272 - val_loss: 0.2324 - val_q0_loss: 0.0166 - val_q1_loss: 0.0146 - val_q2_loss: 0.0143 - val_q3_loss: 0.0185 - val_q4_loss: 0.0154 - val_q5_loss: 0.0218 - val_q6_loss: 0.0364 - val_q7_loss: 0.0448 - val_q8_loss: 0.0290\n",
      "Epoch 130/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.2905 - q0_loss: 0.0215 - q1_loss: 0.0147 - q2_loss: 0.0150 - q3_loss: 0.0308 - q4_loss: 0.0199 - q5_loss: 0.0358 - q6_loss: 0.0402 - q7_loss: 0.0761 - q8_loss: 0.0344 - val_loss: 0.3116 - val_q0_loss: 0.0215 - val_q1_loss: 0.0155 - val_q2_loss: 0.0113 - val_q3_loss: 0.0394 - val_q4_loss: 0.0226 - val_q5_loss: 0.0487 - val_q6_loss: 0.0671 - val_q7_loss: 0.0462 - val_q8_loss: 0.0288\n",
      "Epoch 131/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.3084 - q0_loss: 0.0221 - q1_loss: 0.0161 - q2_loss: 0.0116 - q3_loss: 0.0383 - q4_loss: 0.0245 - q5_loss: 0.0459 - q6_loss: 0.0443 - q7_loss: 0.0750 - q8_loss: 0.0289 - val_loss: 0.2073 - val_q0_loss: 0.0186 - val_q1_loss: 0.0135 - val_q2_loss: 0.0158 - val_q3_loss: 0.0151 - val_q4_loss: 0.0071 - val_q5_loss: 0.0152 - val_q6_loss: 0.0297 - val_q7_loss: 0.0602 - val_q8_loss: 0.0297\n",
      "Epoch 132/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.2580 - q0_loss: 0.0192 - q1_loss: 0.0113 - q2_loss: 0.0123 - q3_loss: 0.0267 - q4_loss: 0.0134 - q5_loss: 0.0310 - q6_loss: 0.0367 - q7_loss: 0.0771 - q8_loss: 0.0301 - val_loss: 0.1856 - val_q0_loss: 0.0178 - val_q1_loss: 0.0114 - val_q2_loss: 0.0098 - val_q3_loss: 0.0095 - val_q4_loss: 0.0068 - val_q5_loss: 0.0127 - val_q6_loss: 0.0409 - val_q7_loss: 0.0436 - val_q8_loss: 0.0249\n",
      "Epoch 133/300\n",
      "269/269 [==============================] - 0s 165us/sample - loss: 0.2261 - q0_loss: 0.0191 - q1_loss: 0.0122 - q2_loss: 0.0122 - q3_loss: 0.0166 - q4_loss: 0.0120 - q5_loss: 0.0179 - q6_loss: 0.0337 - q7_loss: 0.0710 - q8_loss: 0.0290 - val_loss: 0.2299 - val_q0_loss: 0.0162 - val_q1_loss: 0.0136 - val_q2_loss: 0.0205 - val_q3_loss: 0.0234 - val_q4_loss: 0.0210 - val_q5_loss: 0.0244 - val_q6_loss: 0.0186 - val_q7_loss: 0.0389 - val_q8_loss: 0.0343\n",
      "Epoch 134/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.2306 - q0_loss: 0.0192 - q1_loss: 0.0124 - q2_loss: 0.0167 - q3_loss: 0.0160 - q4_loss: 0.0122 - q5_loss: 0.0171 - q6_loss: 0.0298 - q7_loss: 0.0711 - q8_loss: 0.0354 - val_loss: 0.1958 - val_q0_loss: 0.0165 - val_q1_loss: 0.0102 - val_q2_loss: 0.0198 - val_q3_loss: 0.0080 - val_q4_loss: 0.0083 - val_q5_loss: 0.0091 - val_q6_loss: 0.0309 - val_q7_loss: 0.0381 - val_q8_loss: 0.0391\n",
      "Epoch 135/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.2152 - q0_loss: 0.0190 - q1_loss: 0.0119 - q2_loss: 0.0145 - q3_loss: 0.0148 - q4_loss: 0.0088 - q5_loss: 0.0161 - q6_loss: 0.0290 - q7_loss: 0.0665 - q8_loss: 0.0353 - val_loss: 0.2033 - val_q0_loss: 0.0166 - val_q1_loss: 0.0102 - val_q2_loss: 0.0113 - val_q3_loss: 0.0203 - val_q4_loss: 0.0098 - val_q5_loss: 0.0244 - val_q6_loss: 0.0213 - val_q7_loss: 0.0441 - val_q8_loss: 0.0279\n",
      "Epoch 136/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.2094 - q0_loss: 0.0187 - q1_loss: 0.0107 - q2_loss: 0.0098 - q3_loss: 0.0154 - q4_loss: 0.0082 - q5_loss: 0.0172 - q6_loss: 0.0303 - q7_loss: 0.0684 - q8_loss: 0.0270 - val_loss: 0.1637 - val_q0_loss: 0.0177 - val_q1_loss: 0.0120 - val_q2_loss: 0.0061 - val_q3_loss: 0.0101 - val_q4_loss: 0.0066 - val_q5_loss: 0.0113 - val_q6_loss: 0.0259 - val_q7_loss: 0.0459 - val_q8_loss: 0.0183\n",
      "Epoch 137/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1984 - q0_loss: 0.0183 - q1_loss: 0.0115 - q2_loss: 0.0112 - q3_loss: 0.0119 - q4_loss: 0.0091 - q5_loss: 0.0132 - q6_loss: 0.0261 - q7_loss: 0.0644 - q8_loss: 0.0299 - val_loss: 0.1900 - val_q0_loss: 0.0163 - val_q1_loss: 0.0118 - val_q2_loss: 0.0194 - val_q3_loss: 0.0107 - val_q4_loss: 0.0120 - val_q5_loss: 0.0121 - val_q6_loss: 0.0161 - val_q7_loss: 0.0375 - val_q8_loss: 0.0347\n",
      "Epoch 138/300\n",
      "269/269 [==============================] - 0s 203us/sample - loss: 0.2147 - q0_loss: 0.0196 - q1_loss: 0.0132 - q2_loss: 0.0160 - q3_loss: 0.0134 - q4_loss: 0.0110 - q5_loss: 0.0146 - q6_loss: 0.0243 - q7_loss: 0.0684 - q8_loss: 0.0340 - val_loss: 0.1658 - val_q0_loss: 0.0161 - val_q1_loss: 0.0109 - val_q2_loss: 0.0123 - val_q3_loss: 0.0089 - val_q4_loss: 0.0088 - val_q5_loss: 0.0095 - val_q6_loss: 0.0173 - val_q7_loss: 0.0347 - val_q8_loss: 0.0270\n",
      "Epoch 139/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.2355 - q0_loss: 0.0202 - q1_loss: 0.0135 - q2_loss: 0.0100 - q3_loss: 0.0252 - q4_loss: 0.0170 - q5_loss: 0.0301 - q6_loss: 0.0344 - q7_loss: 0.0616 - q8_loss: 0.0274 - val_loss: 0.3199 - val_q0_loss: 0.0288 - val_q1_loss: 0.0263 - val_q2_loss: 0.0264 - val_q3_loss: 0.0272 - val_q4_loss: 0.0331 - val_q5_loss: 0.0347 - val_q6_loss: 0.0445 - val_q7_loss: 0.0655 - val_q8_loss: 0.0488\n",
      "Epoch 140/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.3173 - q0_loss: 0.0246 - q1_loss: 0.0194 - q2_loss: 0.0203 - q3_loss: 0.0344 - q4_loss: 0.0261 - q5_loss: 0.0408 - q6_loss: 0.0399 - q7_loss: 0.0689 - q8_loss: 0.0418 - val_loss: 0.3035 - val_q0_loss: 0.0184 - val_q1_loss: 0.0115 - val_q2_loss: 0.0187 - val_q3_loss: 0.0507 - val_q4_loss: 0.0223 - val_q5_loss: 0.0608 - val_q6_loss: 0.0333 - val_q7_loss: 0.0515 - val_q8_loss: 0.0382\n",
      "Epoch 141/300\n",
      "269/269 [==============================] - 0s 178us/sample - loss: 0.2429 - q0_loss: 0.0196 - q1_loss: 0.0122 - q2_loss: 0.0121 - q3_loss: 0.0271 - q4_loss: 0.0155 - q5_loss: 0.0326 - q6_loss: 0.0277 - q7_loss: 0.0613 - q8_loss: 0.0314 - val_loss: 0.1816 - val_q0_loss: 0.0192 - val_q1_loss: 0.0136 - val_q2_loss: 0.0079 - val_q3_loss: 0.0120 - val_q4_loss: 0.0107 - val_q5_loss: 0.0146 - val_q6_loss: 0.0302 - val_q7_loss: 0.0473 - val_q8_loss: 0.0226\n",
      "Epoch 142/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.1912 - q0_loss: 0.0191 - q1_loss: 0.0118 - q2_loss: 0.0113 - q3_loss: 0.0127 - q4_loss: 0.0097 - q5_loss: 0.0147 - q6_loss: 0.0222 - q7_loss: 0.0616 - q8_loss: 0.0295 - val_loss: 0.1490 - val_q0_loss: 0.0167 - val_q1_loss: 0.0105 - val_q2_loss: 0.0094 - val_q3_loss: 0.0093 - val_q4_loss: 0.0077 - val_q5_loss: 0.0119 - val_q6_loss: 0.0180 - val_q7_loss: 0.0358 - val_q8_loss: 0.0219\n",
      "Epoch 143/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.2032 - q0_loss: 0.0203 - q1_loss: 0.0134 - q2_loss: 0.0140 - q3_loss: 0.0178 - q4_loss: 0.0135 - q5_loss: 0.0205 - q6_loss: 0.0229 - q7_loss: 0.0613 - q8_loss: 0.0304 - val_loss: 0.2599 - val_q0_loss: 0.0178 - val_q1_loss: 0.0149 - val_q2_loss: 0.0115 - val_q3_loss: 0.0372 - val_q4_loss: 0.0251 - val_q5_loss: 0.0463 - val_q6_loss: 0.0278 - val_q7_loss: 0.0306 - val_q8_loss: 0.0254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.2566 - q0_loss: 0.0225 - q1_loss: 0.0175 - q2_loss: 0.0160 - q3_loss: 0.0247 - q4_loss: 0.0217 - q5_loss: 0.0291 - q6_loss: 0.0291 - q7_loss: 0.0607 - q8_loss: 0.0351 - val_loss: 0.2394 - val_q0_loss: 0.0180 - val_q1_loss: 0.0104 - val_q2_loss: 0.0235 - val_q3_loss: 0.0257 - val_q4_loss: 0.0084 - val_q5_loss: 0.0298 - val_q6_loss: 0.0351 - val_q7_loss: 0.0320 - val_q8_loss: 0.0441\n",
      "Epoch 145/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.2310 - q0_loss: 0.0189 - q1_loss: 0.0115 - q2_loss: 0.0142 - q3_loss: 0.0259 - q4_loss: 0.0129 - q5_loss: 0.0317 - q6_loss: 0.0271 - q7_loss: 0.0555 - q8_loss: 0.0338 - val_loss: 0.3032 - val_q0_loss: 0.0187 - val_q1_loss: 0.0105 - val_q2_loss: 0.0284 - val_q3_loss: 0.0409 - val_q4_loss: 0.0143 - val_q5_loss: 0.0498 - val_q6_loss: 0.0455 - val_q7_loss: 0.0310 - val_q8_loss: 0.0541\n",
      "Epoch 146/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.2651 - q0_loss: 0.0192 - q1_loss: 0.0129 - q2_loss: 0.0197 - q3_loss: 0.0332 - q4_loss: 0.0147 - q5_loss: 0.0408 - q6_loss: 0.0338 - q7_loss: 0.0538 - q8_loss: 0.0407 - val_loss: 0.2193 - val_q0_loss: 0.0164 - val_q1_loss: 0.0102 - val_q2_loss: 0.0142 - val_q3_loss: 0.0250 - val_q4_loss: 0.0107 - val_q5_loss: 0.0317 - val_q6_loss: 0.0267 - val_q7_loss: 0.0344 - val_q8_loss: 0.0296\n",
      "Epoch 147/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.2403 - q0_loss: 0.0200 - q1_loss: 0.0147 - q2_loss: 0.0159 - q3_loss: 0.0252 - q4_loss: 0.0172 - q5_loss: 0.0309 - q6_loss: 0.0255 - q7_loss: 0.0526 - q8_loss: 0.0363 - val_loss: 0.2174 - val_q0_loss: 0.0195 - val_q1_loss: 0.0115 - val_q2_loss: 0.0147 - val_q3_loss: 0.0217 - val_q4_loss: 0.0130 - val_q5_loss: 0.0264 - val_q6_loss: 0.0312 - val_q7_loss: 0.0330 - val_q8_loss: 0.0318\n",
      "Epoch 148/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.2525 - q0_loss: 0.0202 - q1_loss: 0.0134 - q2_loss: 0.0182 - q3_loss: 0.0274 - q4_loss: 0.0159 - q5_loss: 0.0333 - q6_loss: 0.0312 - q7_loss: 0.0543 - q8_loss: 0.0390 - val_loss: 0.3229 - val_q0_loss: 0.0220 - val_q1_loss: 0.0155 - val_q2_loss: 0.0150 - val_q3_loss: 0.0453 - val_q4_loss: 0.0234 - val_q5_loss: 0.0559 - val_q6_loss: 0.0470 - val_q7_loss: 0.0489 - val_q8_loss: 0.0343\n",
      "Epoch 149/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.2404 - q0_loss: 0.0207 - q1_loss: 0.0135 - q2_loss: 0.0118 - q3_loss: 0.0308 - q4_loss: 0.0172 - q5_loss: 0.0387 - q6_loss: 0.0324 - q7_loss: 0.0496 - q8_loss: 0.0299 - val_loss: 0.2138 - val_q0_loss: 0.0228 - val_q1_loss: 0.0189 - val_q2_loss: 0.0157 - val_q3_loss: 0.0144 - val_q4_loss: 0.0169 - val_q5_loss: 0.0180 - val_q6_loss: 0.0289 - val_q7_loss: 0.0522 - val_q8_loss: 0.0287\n",
      "Epoch 150/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1887 - q0_loss: 0.0180 - q1_loss: 0.0121 - q2_loss: 0.0136 - q3_loss: 0.0153 - q4_loss: 0.0105 - q5_loss: 0.0181 - q6_loss: 0.0196 - q7_loss: 0.0479 - q8_loss: 0.0316 - val_loss: 0.1468 - val_q0_loss: 0.0188 - val_q1_loss: 0.0126 - val_q2_loss: 0.0087 - val_q3_loss: 0.0087 - val_q4_loss: 0.0079 - val_q5_loss: 0.0096 - val_q6_loss: 0.0198 - val_q7_loss: 0.0394 - val_q8_loss: 0.0181\n",
      "Epoch 151/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.1741 - q0_loss: 0.0191 - q1_loss: 0.0120 - q2_loss: 0.0121 - q3_loss: 0.0128 - q4_loss: 0.0101 - q5_loss: 0.0143 - q6_loss: 0.0154 - q7_loss: 0.0465 - q8_loss: 0.0332 - val_loss: 0.1993 - val_q0_loss: 0.0190 - val_q1_loss: 0.0160 - val_q2_loss: 0.0164 - val_q3_loss: 0.0162 - val_q4_loss: 0.0157 - val_q5_loss: 0.0203 - val_q6_loss: 0.0185 - val_q7_loss: 0.0413 - val_q8_loss: 0.0344\n",
      "Epoch 152/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.2057 - q0_loss: 0.0204 - q1_loss: 0.0144 - q2_loss: 0.0175 - q3_loss: 0.0176 - q4_loss: 0.0136 - q5_loss: 0.0216 - q6_loss: 0.0196 - q7_loss: 0.0465 - q8_loss: 0.0366 - val_loss: 0.1650 - val_q0_loss: 0.0175 - val_q1_loss: 0.0121 - val_q2_loss: 0.0118 - val_q3_loss: 0.0137 - val_q4_loss: 0.0076 - val_q5_loss: 0.0168 - val_q6_loss: 0.0180 - val_q7_loss: 0.0343 - val_q8_loss: 0.0289\n",
      "Epoch 153/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.1951 - q0_loss: 0.0184 - q1_loss: 0.0112 - q2_loss: 0.0156 - q3_loss: 0.0188 - q4_loss: 0.0096 - q5_loss: 0.0237 - q6_loss: 0.0211 - q7_loss: 0.0430 - q8_loss: 0.0357 - val_loss: 0.1585 - val_q0_loss: 0.0177 - val_q1_loss: 0.0112 - val_q2_loss: 0.0106 - val_q3_loss: 0.0126 - val_q4_loss: 0.0102 - val_q5_loss: 0.0156 - val_q6_loss: 0.0164 - val_q7_loss: 0.0292 - val_q8_loss: 0.0223\n",
      "Epoch 154/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.2126 - q0_loss: 0.0187 - q1_loss: 0.0130 - q2_loss: 0.0145 - q3_loss: 0.0226 - q4_loss: 0.0139 - q5_loss: 0.0275 - q6_loss: 0.0238 - q7_loss: 0.0453 - q8_loss: 0.0346 - val_loss: 0.1571 - val_q0_loss: 0.0187 - val_q1_loss: 0.0121 - val_q2_loss: 0.0100 - val_q3_loss: 0.0162 - val_q4_loss: 0.0078 - val_q5_loss: 0.0189 - val_q6_loss: 0.0141 - val_q7_loss: 0.0399 - val_q8_loss: 0.0215\n",
      "Epoch 155/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.2021 - q0_loss: 0.0200 - q1_loss: 0.0134 - q2_loss: 0.0107 - q3_loss: 0.0228 - q4_loss: 0.0158 - q5_loss: 0.0274 - q6_loss: 0.0234 - q7_loss: 0.0431 - q8_loss: 0.0285 - val_loss: 0.2279 - val_q0_loss: 0.0189 - val_q1_loss: 0.0171 - val_q2_loss: 0.0132 - val_q3_loss: 0.0235 - val_q4_loss: 0.0201 - val_q5_loss: 0.0304 - val_q6_loss: 0.0296 - val_q7_loss: 0.0323 - val_q8_loss: 0.0294\n",
      "Epoch 156/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.2241 - q0_loss: 0.0222 - q1_loss: 0.0159 - q2_loss: 0.0140 - q3_loss: 0.0253 - q4_loss: 0.0192 - q5_loss: 0.0305 - q6_loss: 0.0276 - q7_loss: 0.0409 - q8_loss: 0.0320 - val_loss: 0.2936 - val_q0_loss: 0.0240 - val_q1_loss: 0.0219 - val_q2_loss: 0.0195 - val_q3_loss: 0.0333 - val_q4_loss: 0.0281 - val_q5_loss: 0.0412 - val_q6_loss: 0.0447 - val_q7_loss: 0.0381 - val_q8_loss: 0.0433\n",
      "Epoch 157/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.2905 - q0_loss: 0.0252 - q1_loss: 0.0208 - q2_loss: 0.0189 - q3_loss: 0.0348 - q4_loss: 0.0281 - q5_loss: 0.0422 - q6_loss: 0.0387 - q7_loss: 0.0451 - q8_loss: 0.0398 - val_loss: 0.2142 - val_q0_loss: 0.0179 - val_q1_loss: 0.0122 - val_q2_loss: 0.0204 - val_q3_loss: 0.0234 - val_q4_loss: 0.0076 - val_q5_loss: 0.0315 - val_q6_loss: 0.0321 - val_q7_loss: 0.0250 - val_q8_loss: 0.0435\n",
      "Epoch 158/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.2664 - q0_loss: 0.0221 - q1_loss: 0.0152 - q2_loss: 0.0238 - q3_loss: 0.0317 - q4_loss: 0.0173 - q5_loss: 0.0384 - q6_loss: 0.0325 - q7_loss: 0.0413 - q8_loss: 0.0461 - val_loss: 0.1828 - val_q0_loss: 0.0208 - val_q1_loss: 0.0144 - val_q2_loss: 0.0137 - val_q3_loss: 0.0190 - val_q4_loss: 0.0107 - val_q5_loss: 0.0238 - val_q6_loss: 0.0227 - val_q7_loss: 0.0363 - val_q8_loss: 0.0261\n",
      "Epoch 159/300\n",
      "269/269 [==============================] - 0s 188us/sample - loss: 0.1968 - q0_loss: 0.0195 - q1_loss: 0.0135 - q2_loss: 0.0184 - q3_loss: 0.0174 - q4_loss: 0.0131 - q5_loss: 0.0213 - q6_loss: 0.0183 - q7_loss: 0.0338 - q8_loss: 0.0412 - val_loss: 0.2225 - val_q0_loss: 0.0161 - val_q1_loss: 0.0106 - val_q2_loss: 0.0188 - val_q3_loss: 0.0337 - val_q4_loss: 0.0123 - val_q5_loss: 0.0414 - val_q6_loss: 0.0255 - val_q7_loss: 0.0386 - val_q8_loss: 0.0321\n",
      "Epoch 160/300\n",
      "269/269 [==============================] - 0s 178us/sample - loss: 0.2270 - q0_loss: 0.0209 - q1_loss: 0.0117 - q2_loss: 0.0146 - q3_loss: 0.0295 - q4_loss: 0.0146 - q5_loss: 0.0365 - q6_loss: 0.0290 - q7_loss: 0.0340 - q8_loss: 0.0342 - val_loss: 0.2572 - val_q0_loss: 0.0165 - val_q1_loss: 0.0101 - val_q2_loss: 0.0090 - val_q3_loss: 0.0445 - val_q4_loss: 0.0219 - val_q5_loss: 0.0557 - val_q6_loss: 0.0386 - val_q7_loss: 0.0297 - val_q8_loss: 0.0230\n",
      "Epoch 161/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.2753 - q0_loss: 0.0209 - q1_loss: 0.0150 - q2_loss: 0.0104 - q3_loss: 0.0441 - q4_loss: 0.0248 - q5_loss: 0.0547 - q6_loss: 0.0456 - q7_loss: 0.0293 - q8_loss: 0.0285 - val_loss: 0.3129 - val_q0_loss: 0.0272 - val_q1_loss: 0.0204 - val_q2_loss: 0.0074 - val_q3_loss: 0.0512 - val_q4_loss: 0.0331 - val_q5_loss: 0.0617 - val_q6_loss: 0.0597 - val_q7_loss: 0.0361 - val_q8_loss: 0.0190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/300\n",
      "269/269 [==============================] - 0s 194us/sample - loss: 0.3331 - q0_loss: 0.0253 - q1_loss: 0.0184 - q2_loss: 0.0113 - q3_loss: 0.0553 - q4_loss: 0.0339 - q5_loss: 0.0680 - q6_loss: 0.0585 - q7_loss: 0.0324 - q8_loss: 0.0290 - val_loss: 0.2951 - val_q0_loss: 0.0189 - val_q1_loss: 0.0186 - val_q2_loss: 0.0083 - val_q3_loss: 0.0476 - val_q4_loss: 0.0287 - val_q5_loss: 0.0601 - val_q6_loss: 0.0470 - val_q7_loss: 0.0261 - val_q8_loss: 0.0240\n",
      "Epoch 163/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.2897 - q0_loss: 0.0234 - q1_loss: 0.0176 - q2_loss: 0.0143 - q3_loss: 0.0413 - q4_loss: 0.0269 - q5_loss: 0.0513 - q6_loss: 0.0440 - q7_loss: 0.0316 - q8_loss: 0.0341 - val_loss: 0.3612 - val_q0_loss: 0.0289 - val_q1_loss: 0.0241 - val_q2_loss: 0.0089 - val_q3_loss: 0.0622 - val_q4_loss: 0.0383 - val_q5_loss: 0.0767 - val_q6_loss: 0.0673 - val_q7_loss: 0.0355 - val_q8_loss: 0.0214\n",
      "Epoch 164/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.3142 - q0_loss: 0.0240 - q1_loss: 0.0182 - q2_loss: 0.0110 - q3_loss: 0.0493 - q4_loss: 0.0305 - q5_loss: 0.0594 - q6_loss: 0.0514 - q7_loss: 0.0310 - q8_loss: 0.0301 - val_loss: 0.3633 - val_q0_loss: 0.0198 - val_q1_loss: 0.0165 - val_q2_loss: 0.0139 - val_q3_loss: 0.0741 - val_q4_loss: 0.0401 - val_q5_loss: 0.0929 - val_q6_loss: 0.0686 - val_q7_loss: 0.0232 - val_q8_loss: 0.0303\n",
      "Epoch 165/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.3151 - q0_loss: 0.0257 - q1_loss: 0.0184 - q2_loss: 0.0083 - q3_loss: 0.0527 - q4_loss: 0.0324 - q5_loss: 0.0657 - q6_loss: 0.0535 - q7_loss: 0.0266 - q8_loss: 0.0260 - val_loss: 0.1917 - val_q0_loss: 0.0201 - val_q1_loss: 0.0127 - val_q2_loss: 0.0116 - val_q3_loss: 0.0210 - val_q4_loss: 0.0157 - val_q5_loss: 0.0257 - val_q6_loss: 0.0248 - val_q7_loss: 0.0198 - val_q8_loss: 0.0284\n",
      "Epoch 166/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.1933 - q0_loss: 0.0192 - q1_loss: 0.0132 - q2_loss: 0.0092 - q3_loss: 0.0241 - q4_loss: 0.0168 - q5_loss: 0.0301 - q6_loss: 0.0259 - q7_loss: 0.0267 - q8_loss: 0.0259 - val_loss: 0.1551 - val_q0_loss: 0.0181 - val_q1_loss: 0.0118 - val_q2_loss: 0.0139 - val_q3_loss: 0.0100 - val_q4_loss: 0.0119 - val_q5_loss: 0.0125 - val_q6_loss: 0.0142 - val_q7_loss: 0.0197 - val_q8_loss: 0.0269\n",
      "Epoch 167/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1900 - q0_loss: 0.0212 - q1_loss: 0.0138 - q2_loss: 0.0114 - q3_loss: 0.0203 - q4_loss: 0.0160 - q5_loss: 0.0241 - q6_loss: 0.0231 - q7_loss: 0.0280 - q8_loss: 0.0285 - val_loss: 0.2307 - val_q0_loss: 0.0154 - val_q1_loss: 0.0121 - val_q2_loss: 0.0125 - val_q3_loss: 0.0437 - val_q4_loss: 0.0203 - val_q5_loss: 0.0531 - val_q6_loss: 0.0365 - val_q7_loss: 0.0223 - val_q8_loss: 0.0289\n",
      "Epoch 168/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.2214 - q0_loss: 0.0207 - q1_loss: 0.0142 - q2_loss: 0.0144 - q3_loss: 0.0296 - q4_loss: 0.0175 - q5_loss: 0.0366 - q6_loss: 0.0297 - q7_loss: 0.0253 - q8_loss: 0.0336 - val_loss: 0.2479 - val_q0_loss: 0.0183 - val_q1_loss: 0.0182 - val_q2_loss: 0.0144 - val_q3_loss: 0.0332 - val_q4_loss: 0.0224 - val_q5_loss: 0.0418 - val_q6_loss: 0.0328 - val_q7_loss: 0.0265 - val_q8_loss: 0.0327\n",
      "Epoch 169/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.1787 - q0_loss: 0.0208 - q1_loss: 0.0131 - q2_loss: 0.0106 - q3_loss: 0.0196 - q4_loss: 0.0149 - q5_loss: 0.0239 - q6_loss: 0.0226 - q7_loss: 0.0244 - q8_loss: 0.0269 - val_loss: 0.1522 - val_q0_loss: 0.0174 - val_q1_loss: 0.0105 - val_q2_loss: 0.0116 - val_q3_loss: 0.0151 - val_q4_loss: 0.0114 - val_q5_loss: 0.0194 - val_q6_loss: 0.0145 - val_q7_loss: 0.0137 - val_q8_loss: 0.0255\n",
      "Epoch 170/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1558 - q0_loss: 0.0187 - q1_loss: 0.0118 - q2_loss: 0.0086 - q3_loss: 0.0165 - q4_loss: 0.0118 - q5_loss: 0.0212 - q6_loss: 0.0184 - q7_loss: 0.0209 - q8_loss: 0.0242 - val_loss: 0.1694 - val_q0_loss: 0.0162 - val_q1_loss: 0.0116 - val_q2_loss: 0.0131 - val_q3_loss: 0.0221 - val_q4_loss: 0.0139 - val_q5_loss: 0.0276 - val_q6_loss: 0.0208 - val_q7_loss: 0.0143 - val_q8_loss: 0.0270\n",
      "Epoch 171/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.1703 - q0_loss: 0.0197 - q1_loss: 0.0123 - q2_loss: 0.0126 - q3_loss: 0.0186 - q4_loss: 0.0110 - q5_loss: 0.0230 - q6_loss: 0.0190 - q7_loss: 0.0236 - q8_loss: 0.0315 - val_loss: 0.1357 - val_q0_loss: 0.0173 - val_q1_loss: 0.0121 - val_q2_loss: 0.0131 - val_q3_loss: 0.0120 - val_q4_loss: 0.0078 - val_q5_loss: 0.0153 - val_q6_loss: 0.0136 - val_q7_loss: 0.0186 - val_q8_loss: 0.0310\n",
      "Epoch 172/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1700 - q0_loss: 0.0180 - q1_loss: 0.0113 - q2_loss: 0.0109 - q3_loss: 0.0214 - q4_loss: 0.0118 - q5_loss: 0.0259 - q6_loss: 0.0211 - q7_loss: 0.0182 - q8_loss: 0.0296 - val_loss: 0.1452 - val_q0_loss: 0.0181 - val_q1_loss: 0.0107 - val_q2_loss: 0.0054 - val_q3_loss: 0.0191 - val_q4_loss: 0.0107 - val_q5_loss: 0.0228 - val_q6_loss: 0.0161 - val_q7_loss: 0.0138 - val_q8_loss: 0.0170\n",
      "Epoch 173/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.1850 - q0_loss: 0.0200 - q1_loss: 0.0135 - q2_loss: 0.0092 - q3_loss: 0.0261 - q4_loss: 0.0152 - q5_loss: 0.0319 - q6_loss: 0.0265 - q7_loss: 0.0197 - q8_loss: 0.0292 - val_loss: 0.1801 - val_q0_loss: 0.0179 - val_q1_loss: 0.0147 - val_q2_loss: 0.0109 - val_q3_loss: 0.0180 - val_q4_loss: 0.0158 - val_q5_loss: 0.0233 - val_q6_loss: 0.0230 - val_q7_loss: 0.0225 - val_q8_loss: 0.0253\n",
      "Epoch 174/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.2686 - q0_loss: 0.0219 - q1_loss: 0.0149 - q2_loss: 0.0157 - q3_loss: 0.0399 - q4_loss: 0.0236 - q5_loss: 0.0482 - q6_loss: 0.0400 - q7_loss: 0.0257 - q8_loss: 0.0351 - val_loss: 0.3142 - val_q0_loss: 0.0168 - val_q1_loss: 0.0145 - val_q2_loss: 0.0171 - val_q3_loss: 0.0584 - val_q4_loss: 0.0306 - val_q5_loss: 0.0726 - val_q6_loss: 0.0539 - val_q7_loss: 0.0205 - val_q8_loss: 0.0335\n",
      "Epoch 175/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.3018 - q0_loss: 0.0211 - q1_loss: 0.0134 - q2_loss: 0.0281 - q3_loss: 0.0426 - q4_loss: 0.0164 - q5_loss: 0.0529 - q6_loss: 0.0394 - q7_loss: 0.0324 - q8_loss: 0.0534 - val_loss: 0.2004 - val_q0_loss: 0.0204 - val_q1_loss: 0.0138 - val_q2_loss: 0.0088 - val_q3_loss: 0.0314 - val_q4_loss: 0.0162 - val_q5_loss: 0.0374 - val_q6_loss: 0.0354 - val_q7_loss: 0.0138 - val_q8_loss: 0.0237\n",
      "Epoch 176/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.2245 - q0_loss: 0.0191 - q1_loss: 0.0125 - q2_loss: 0.0206 - q3_loss: 0.0267 - q4_loss: 0.0134 - q5_loss: 0.0329 - q6_loss: 0.0249 - q7_loss: 0.0291 - q8_loss: 0.0425 - val_loss: 0.2444 - val_q0_loss: 0.0241 - val_q1_loss: 0.0218 - val_q2_loss: 0.0143 - val_q3_loss: 0.0299 - val_q4_loss: 0.0265 - val_q5_loss: 0.0361 - val_q6_loss: 0.0390 - val_q7_loss: 0.0320 - val_q8_loss: 0.0266\n",
      "Epoch 177/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.2112 - q0_loss: 0.0199 - q1_loss: 0.0134 - q2_loss: 0.0170 - q3_loss: 0.0259 - q4_loss: 0.0160 - q5_loss: 0.0317 - q6_loss: 0.0264 - q7_loss: 0.0233 - q8_loss: 0.0339 - val_loss: 0.2255 - val_q0_loss: 0.0209 - val_q1_loss: 0.0196 - val_q2_loss: 0.0284 - val_q3_loss: 0.0181 - val_q4_loss: 0.0179 - val_q5_loss: 0.0235 - val_q6_loss: 0.0212 - val_q7_loss: 0.0435 - val_q8_loss: 0.0481\n",
      "Epoch 178/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.2421 - q0_loss: 0.0228 - q1_loss: 0.0177 - q2_loss: 0.0221 - q3_loss: 0.0248 - q4_loss: 0.0224 - q5_loss: 0.0313 - q6_loss: 0.0290 - q7_loss: 0.0321 - q8_loss: 0.0445 - val_loss: 0.2248 - val_q0_loss: 0.0196 - val_q1_loss: 0.0143 - val_q2_loss: 0.0272 - val_q3_loss: 0.0192 - val_q4_loss: 0.0153 - val_q5_loss: 0.0248 - val_q6_loss: 0.0167 - val_q7_loss: 0.0297 - val_q8_loss: 0.0443\n",
      "Epoch 179/300\n",
      "269/269 [==============================] - 0s 178us/sample - loss: 0.2300 - q0_loss: 0.0202 - q1_loss: 0.0168 - q2_loss: 0.0249 - q3_loss: 0.0213 - q4_loss: 0.0160 - q5_loss: 0.0256 - q6_loss: 0.0210 - q7_loss: 0.0338 - q8_loss: 0.0467 - val_loss: 0.1767 - val_q0_loss: 0.0202 - val_q1_loss: 0.0149 - val_q2_loss: 0.0150 - val_q3_loss: 0.0198 - val_q4_loss: 0.0122 - val_q5_loss: 0.0216 - val_q6_loss: 0.0222 - val_q7_loss: 0.0239 - val_q8_loss: 0.0270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.1595 - q0_loss: 0.0187 - q1_loss: 0.0130 - q2_loss: 0.0160 - q3_loss: 0.0120 - q4_loss: 0.0109 - q5_loss: 0.0146 - q6_loss: 0.0133 - q7_loss: 0.0239 - q8_loss: 0.0361 - val_loss: 0.1624 - val_q0_loss: 0.0200 - val_q1_loss: 0.0151 - val_q2_loss: 0.0172 - val_q3_loss: 0.0080 - val_q4_loss: 0.0131 - val_q5_loss: 0.0103 - val_q6_loss: 0.0103 - val_q7_loss: 0.0271 - val_q8_loss: 0.0309\n",
      "Epoch 181/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1677 - q0_loss: 0.0193 - q1_loss: 0.0132 - q2_loss: 0.0171 - q3_loss: 0.0150 - q4_loss: 0.0134 - q5_loss: 0.0171 - q6_loss: 0.0152 - q7_loss: 0.0251 - q8_loss: 0.0343 - val_loss: 0.2126 - val_q0_loss: 0.0150 - val_q1_loss: 0.0138 - val_q2_loss: 0.0273 - val_q3_loss: 0.0164 - val_q4_loss: 0.0124 - val_q5_loss: 0.0205 - val_q6_loss: 0.0167 - val_q7_loss: 0.0258 - val_q8_loss: 0.0512\n",
      "Epoch 182/300\n",
      "269/269 [==============================] - 0s 174us/sample - loss: 0.1931 - q0_loss: 0.0189 - q1_loss: 0.0157 - q2_loss: 0.0191 - q3_loss: 0.0177 - q4_loss: 0.0156 - q5_loss: 0.0220 - q6_loss: 0.0202 - q7_loss: 0.0277 - q8_loss: 0.0353 - val_loss: 0.1717 - val_q0_loss: 0.0164 - val_q1_loss: 0.0119 - val_q2_loss: 0.0094 - val_q3_loss: 0.0197 - val_q4_loss: 0.0170 - val_q5_loss: 0.0249 - val_q6_loss: 0.0210 - val_q7_loss: 0.0128 - val_q8_loss: 0.0232\n",
      "Epoch 183/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1915 - q0_loss: 0.0208 - q1_loss: 0.0150 - q2_loss: 0.0133 - q3_loss: 0.0229 - q4_loss: 0.0165 - q5_loss: 0.0280 - q6_loss: 0.0242 - q7_loss: 0.0204 - q8_loss: 0.0296 - val_loss: 0.1806 - val_q0_loss: 0.0182 - val_q1_loss: 0.0108 - val_q2_loss: 0.0148 - val_q3_loss: 0.0200 - val_q4_loss: 0.0115 - val_q5_loss: 0.0238 - val_q6_loss: 0.0193 - val_q7_loss: 0.0136 - val_q8_loss: 0.0310\n",
      "Epoch 184/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1637 - q0_loss: 0.0188 - q1_loss: 0.0122 - q2_loss: 0.0141 - q3_loss: 0.0162 - q4_loss: 0.0130 - q5_loss: 0.0195 - q6_loss: 0.0176 - q7_loss: 0.0212 - q8_loss: 0.0329 - val_loss: 0.1270 - val_q0_loss: 0.0175 - val_q1_loss: 0.0108 - val_q2_loss: 0.0119 - val_q3_loss: 0.0088 - val_q4_loss: 0.0099 - val_q5_loss: 0.0111 - val_q6_loss: 0.0095 - val_q7_loss: 0.0157 - val_q8_loss: 0.0226\n",
      "Epoch 185/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.2107 - q0_loss: 0.0204 - q1_loss: 0.0156 - q2_loss: 0.0149 - q3_loss: 0.0244 - q4_loss: 0.0208 - q5_loss: 0.0301 - q6_loss: 0.0283 - q7_loss: 0.0271 - q8_loss: 0.0321 - val_loss: 0.2090 - val_q0_loss: 0.0150 - val_q1_loss: 0.0142 - val_q2_loss: 0.0113 - val_q3_loss: 0.0306 - val_q4_loss: 0.0218 - val_q5_loss: 0.0391 - val_q6_loss: 0.0328 - val_q7_loss: 0.0163 - val_q8_loss: 0.0237\n",
      "Epoch 186/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.2048 - q0_loss: 0.0206 - q1_loss: 0.0154 - q2_loss: 0.0163 - q3_loss: 0.0224 - q4_loss: 0.0185 - q5_loss: 0.0279 - q6_loss: 0.0248 - q7_loss: 0.0248 - q8_loss: 0.0344 - val_loss: 0.1653 - val_q0_loss: 0.0193 - val_q1_loss: 0.0130 - val_q2_loss: 0.0142 - val_q3_loss: 0.0194 - val_q4_loss: 0.0080 - val_q5_loss: 0.0232 - val_q6_loss: 0.0188 - val_q7_loss: 0.0184 - val_q8_loss: 0.0257\n",
      "Epoch 187/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1714 - q0_loss: 0.0193 - q1_loss: 0.0127 - q2_loss: 0.0182 - q3_loss: 0.0162 - q4_loss: 0.0126 - q5_loss: 0.0197 - q6_loss: 0.0169 - q7_loss: 0.0255 - q8_loss: 0.0375 - val_loss: 0.2448 - val_q0_loss: 0.0203 - val_q1_loss: 0.0129 - val_q2_loss: 0.0233 - val_q3_loss: 0.0310 - val_q4_loss: 0.0140 - val_q5_loss: 0.0381 - val_q6_loss: 0.0298 - val_q7_loss: 0.0224 - val_q8_loss: 0.0458\n",
      "Epoch 188/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.1706 - q0_loss: 0.0187 - q1_loss: 0.0114 - q2_loss: 0.0126 - q3_loss: 0.0205 - q4_loss: 0.0125 - q5_loss: 0.0251 - q6_loss: 0.0213 - q7_loss: 0.0191 - q8_loss: 0.0305 - val_loss: 0.1731 - val_q0_loss: 0.0160 - val_q1_loss: 0.0117 - val_q2_loss: 0.0094 - val_q3_loss: 0.0223 - val_q4_loss: 0.0164 - val_q5_loss: 0.0283 - val_q6_loss: 0.0236 - val_q7_loss: 0.0136 - val_q8_loss: 0.0219\n",
      "Epoch 189/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1625 - q0_loss: 0.0186 - q1_loss: 0.0112 - q2_loss: 0.0090 - q3_loss: 0.0218 - q4_loss: 0.0119 - q5_loss: 0.0264 - q6_loss: 0.0212 - q7_loss: 0.0143 - q8_loss: 0.0243 - val_loss: 0.1572 - val_q0_loss: 0.0165 - val_q1_loss: 0.0105 - val_q2_loss: 0.0121 - val_q3_loss: 0.0228 - val_q4_loss: 0.0092 - val_q5_loss: 0.0285 - val_q6_loss: 0.0196 - val_q7_loss: 0.0155 - val_q8_loss: 0.0255\n",
      "Epoch 190/300\n",
      "269/269 [==============================] - 0s 170us/sample - loss: 0.1658 - q0_loss: 0.0193 - q1_loss: 0.0108 - q2_loss: 0.0108 - q3_loss: 0.0227 - q4_loss: 0.0103 - q5_loss: 0.0271 - q6_loss: 0.0216 - q7_loss: 0.0154 - q8_loss: 0.0273 - val_loss: 0.1941 - val_q0_loss: 0.0194 - val_q1_loss: 0.0128 - val_q2_loss: 0.0106 - val_q3_loss: 0.0336 - val_q4_loss: 0.0149 - val_q5_loss: 0.0405 - val_q6_loss: 0.0321 - val_q7_loss: 0.0108 - val_q8_loss: 0.0255\n",
      "Epoch 191/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1509 - q0_loss: 0.0185 - q1_loss: 0.0109 - q2_loss: 0.0100 - q3_loss: 0.0194 - q4_loss: 0.0098 - q5_loss: 0.0231 - q6_loss: 0.0181 - q7_loss: 0.0151 - q8_loss: 0.0256 - val_loss: 0.1307 - val_q0_loss: 0.0182 - val_q1_loss: 0.0113 - val_q2_loss: 0.0067 - val_q3_loss: 0.0127 - val_q4_loss: 0.0103 - val_q5_loss: 0.0154 - val_q6_loss: 0.0148 - val_q7_loss: 0.0110 - val_q8_loss: 0.0169\n",
      "Epoch 192/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.1296 - q0_loss: 0.0195 - q1_loss: 0.0120 - q2_loss: 0.0082 - q3_loss: 0.0123 - q4_loss: 0.0105 - q5_loss: 0.0148 - q6_loss: 0.0149 - q7_loss: 0.0152 - q8_loss: 0.0238 - val_loss: 0.1486 - val_q0_loss: 0.0189 - val_q1_loss: 0.0134 - val_q2_loss: 0.0084 - val_q3_loss: 0.0140 - val_q4_loss: 0.0121 - val_q5_loss: 0.0190 - val_q6_loss: 0.0176 - val_q7_loss: 0.0172 - val_q8_loss: 0.0228\n",
      "Epoch 193/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1780 - q0_loss: 0.0203 - q1_loss: 0.0139 - q2_loss: 0.0106 - q3_loss: 0.0215 - q4_loss: 0.0170 - q5_loss: 0.0261 - q6_loss: 0.0238 - q7_loss: 0.0190 - q8_loss: 0.0266 - val_loss: 0.1909 - val_q0_loss: 0.0173 - val_q1_loss: 0.0144 - val_q2_loss: 0.0093 - val_q3_loss: 0.0296 - val_q4_loss: 0.0202 - val_q5_loss: 0.0365 - val_q6_loss: 0.0313 - val_q7_loss: 0.0171 - val_q8_loss: 0.0215\n",
      "Epoch 194/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1517 - q0_loss: 0.0191 - q1_loss: 0.0120 - q2_loss: 0.0074 - q3_loss: 0.0192 - q4_loss: 0.0122 - q5_loss: 0.0233 - q6_loss: 0.0193 - q7_loss: 0.0141 - q8_loss: 0.0225 - val_loss: 0.2233 - val_q0_loss: 0.0174 - val_q1_loss: 0.0123 - val_q2_loss: 0.0098 - val_q3_loss: 0.0377 - val_q4_loss: 0.0247 - val_q5_loss: 0.0465 - val_q6_loss: 0.0389 - val_q7_loss: 0.0162 - val_q8_loss: 0.0191\n",
      "Epoch 195/300\n",
      "269/269 [==============================] - 0s 170us/sample - loss: 0.1721 - q0_loss: 0.0202 - q1_loss: 0.0124 - q2_loss: 0.0094 - q3_loss: 0.0238 - q4_loss: 0.0137 - q5_loss: 0.0285 - q6_loss: 0.0236 - q7_loss: 0.0154 - q8_loss: 0.0256 - val_loss: 0.1731 - val_q0_loss: 0.0182 - val_q1_loss: 0.0110 - val_q2_loss: 0.0136 - val_q3_loss: 0.0222 - val_q4_loss: 0.0110 - val_q5_loss: 0.0275 - val_q6_loss: 0.0201 - val_q7_loss: 0.0157 - val_q8_loss: 0.0303\n",
      "Epoch 196/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.1550 - q0_loss: 0.0193 - q1_loss: 0.0113 - q2_loss: 0.0094 - q3_loss: 0.0205 - q4_loss: 0.0112 - q5_loss: 0.0241 - q6_loss: 0.0194 - q7_loss: 0.0140 - q8_loss: 0.0255 - val_loss: 0.1589 - val_q0_loss: 0.0160 - val_q1_loss: 0.0113 - val_q2_loss: 0.0092 - val_q3_loss: 0.0230 - val_q4_loss: 0.0147 - val_q5_loss: 0.0286 - val_q6_loss: 0.0235 - val_q7_loss: 0.0127 - val_q8_loss: 0.0186\n",
      "Epoch 197/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.1634 - q0_loss: 0.0197 - q1_loss: 0.0126 - q2_loss: 0.0081 - q3_loss: 0.0214 - q4_loss: 0.0139 - q5_loss: 0.0265 - q6_loss: 0.0224 - q7_loss: 0.0152 - q8_loss: 0.0235 - val_loss: 0.1597 - val_q0_loss: 0.0185 - val_q1_loss: 0.0136 - val_q2_loss: 0.0128 - val_q3_loss: 0.0135 - val_q4_loss: 0.0128 - val_q5_loss: 0.0165 - val_q6_loss: 0.0177 - val_q7_loss: 0.0215 - val_q8_loss: 0.0306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1727 - q0_loss: 0.0196 - q1_loss: 0.0132 - q2_loss: 0.0139 - q3_loss: 0.0172 - q4_loss: 0.0144 - q5_loss: 0.0204 - q6_loss: 0.0186 - q7_loss: 0.0226 - q8_loss: 0.0324 - val_loss: 0.1618 - val_q0_loss: 0.0200 - val_q1_loss: 0.0164 - val_q2_loss: 0.0144 - val_q3_loss: 0.0152 - val_q4_loss: 0.0147 - val_q5_loss: 0.0179 - val_q6_loss: 0.0174 - val_q7_loss: 0.0251 - val_q8_loss: 0.0301\n",
      "Epoch 199/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.1861 - q0_loss: 0.0190 - q1_loss: 0.0133 - q2_loss: 0.0219 - q3_loss: 0.0174 - q4_loss: 0.0101 - q5_loss: 0.0215 - q6_loss: 0.0152 - q7_loss: 0.0271 - q8_loss: 0.0436 - val_loss: 0.1905 - val_q0_loss: 0.0179 - val_q1_loss: 0.0116 - val_q2_loss: 0.0231 - val_q3_loss: 0.0157 - val_q4_loss: 0.0120 - val_q5_loss: 0.0188 - val_q6_loss: 0.0154 - val_q7_loss: 0.0263 - val_q8_loss: 0.0415\n",
      "Epoch 200/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.1510 - q0_loss: 0.0195 - q1_loss: 0.0124 - q2_loss: 0.0151 - q3_loss: 0.0124 - q4_loss: 0.0105 - q5_loss: 0.0146 - q6_loss: 0.0123 - q7_loss: 0.0211 - q8_loss: 0.0312 - val_loss: 0.1117 - val_q0_loss: 0.0172 - val_q1_loss: 0.0114 - val_q2_loss: 0.0094 - val_q3_loss: 0.0094 - val_q4_loss: 0.0066 - val_q5_loss: 0.0128 - val_q6_loss: 0.0110 - val_q7_loss: 0.0122 - val_q8_loss: 0.0198\n",
      "Epoch 201/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.1485 - q0_loss: 0.0188 - q1_loss: 0.0121 - q2_loss: 0.0090 - q3_loss: 0.0192 - q4_loss: 0.0114 - q5_loss: 0.0234 - q6_loss: 0.0196 - q7_loss: 0.0150 - q8_loss: 0.0251 - val_loss: 0.1672 - val_q0_loss: 0.0177 - val_q1_loss: 0.0110 - val_q2_loss: 0.0118 - val_q3_loss: 0.0202 - val_q4_loss: 0.0101 - val_q5_loss: 0.0249 - val_q6_loss: 0.0176 - val_q7_loss: 0.0152 - val_q8_loss: 0.0303\n",
      "Epoch 202/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1727 - q0_loss: 0.0194 - q1_loss: 0.0139 - q2_loss: 0.0122 - q3_loss: 0.0217 - q4_loss: 0.0140 - q5_loss: 0.0262 - q6_loss: 0.0217 - q7_loss: 0.0190 - q8_loss: 0.0292 - val_loss: 0.1901 - val_q0_loss: 0.0187 - val_q1_loss: 0.0162 - val_q2_loss: 0.0259 - val_q3_loss: 0.0193 - val_q4_loss: 0.0067 - val_q5_loss: 0.0244 - val_q6_loss: 0.0129 - val_q7_loss: 0.0319 - val_q8_loss: 0.0514\n",
      "Epoch 203/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1840 - q0_loss: 0.0202 - q1_loss: 0.0134 - q2_loss: 0.0161 - q3_loss: 0.0186 - q4_loss: 0.0141 - q5_loss: 0.0221 - q6_loss: 0.0196 - q7_loss: 0.0235 - q8_loss: 0.0353 - val_loss: 0.1535 - val_q0_loss: 0.0196 - val_q1_loss: 0.0145 - val_q2_loss: 0.0174 - val_q3_loss: 0.0089 - val_q4_loss: 0.0106 - val_q5_loss: 0.0103 - val_q6_loss: 0.0125 - val_q7_loss: 0.0229 - val_q8_loss: 0.0334\n",
      "Epoch 204/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1651 - q0_loss: 0.0194 - q1_loss: 0.0126 - q2_loss: 0.0105 - q3_loss: 0.0215 - q4_loss: 0.0138 - q5_loss: 0.0270 - q6_loss: 0.0228 - q7_loss: 0.0159 - q8_loss: 0.0249 - val_loss: 0.1375 - val_q0_loss: 0.0173 - val_q1_loss: 0.0122 - val_q2_loss: 0.0123 - val_q3_loss: 0.0124 - val_q4_loss: 0.0104 - val_q5_loss: 0.0157 - val_q6_loss: 0.0133 - val_q7_loss: 0.0181 - val_q8_loss: 0.0269\n",
      "Epoch 205/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1670 - q0_loss: 0.0193 - q1_loss: 0.0116 - q2_loss: 0.0112 - q3_loss: 0.0204 - q4_loss: 0.0131 - q5_loss: 0.0250 - q6_loss: 0.0209 - q7_loss: 0.0171 - q8_loss: 0.0274 - val_loss: 0.1613 - val_q0_loss: 0.0155 - val_q1_loss: 0.0116 - val_q2_loss: 0.0064 - val_q3_loss: 0.0263 - val_q4_loss: 0.0168 - val_q5_loss: 0.0320 - val_q6_loss: 0.0277 - val_q7_loss: 0.0117 - val_q8_loss: 0.0165\n",
      "Epoch 206/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1688 - q0_loss: 0.0200 - q1_loss: 0.0141 - q2_loss: 0.0107 - q3_loss: 0.0194 - q4_loss: 0.0153 - q5_loss: 0.0231 - q6_loss: 0.0220 - q7_loss: 0.0192 - q8_loss: 0.0269 - val_loss: 0.1894 - val_q0_loss: 0.0153 - val_q1_loss: 0.0127 - val_q2_loss: 0.0129 - val_q3_loss: 0.0253 - val_q4_loss: 0.0214 - val_q5_loss: 0.0305 - val_q6_loss: 0.0283 - val_q7_loss: 0.0205 - val_q8_loss: 0.0222\n",
      "Epoch 207/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1351 - q0_loss: 0.0190 - q1_loss: 0.0116 - q2_loss: 0.0103 - q3_loss: 0.0131 - q4_loss: 0.0108 - q5_loss: 0.0160 - q6_loss: 0.0137 - q7_loss: 0.0166 - q8_loss: 0.0236 - val_loss: 0.1154 - val_q0_loss: 0.0174 - val_q1_loss: 0.0108 - val_q2_loss: 0.0071 - val_q3_loss: 0.0116 - val_q4_loss: 0.0093 - val_q5_loss: 0.0127 - val_q6_loss: 0.0116 - val_q7_loss: 0.0106 - val_q8_loss: 0.0159\n",
      "Epoch 208/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1578 - q0_loss: 0.0190 - q1_loss: 0.0123 - q2_loss: 0.0170 - q3_loss: 0.0135 - q4_loss: 0.0099 - q5_loss: 0.0155 - q6_loss: 0.0137 - q7_loss: 0.0228 - q8_loss: 0.0355 - val_loss: 0.1310 - val_q0_loss: 0.0181 - val_q1_loss: 0.0116 - val_q2_loss: 0.0122 - val_q3_loss: 0.0117 - val_q4_loss: 0.0076 - val_q5_loss: 0.0147 - val_q6_loss: 0.0117 - val_q7_loss: 0.0139 - val_q8_loss: 0.0220\n",
      "Epoch 209/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1948 - q0_loss: 0.0203 - q1_loss: 0.0130 - q2_loss: 0.0172 - q3_loss: 0.0209 - q4_loss: 0.0131 - q5_loss: 0.0264 - q6_loss: 0.0213 - q7_loss: 0.0240 - q8_loss: 0.0373 - val_loss: 0.2109 - val_q0_loss: 0.0202 - val_q1_loss: 0.0136 - val_q2_loss: 0.0251 - val_q3_loss: 0.0176 - val_q4_loss: 0.0152 - val_q5_loss: 0.0198 - val_q6_loss: 0.0163 - val_q7_loss: 0.0302 - val_q8_loss: 0.0426\n",
      "Epoch 210/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.2218 - q0_loss: 0.0197 - q1_loss: 0.0149 - q2_loss: 0.0211 - q3_loss: 0.0241 - q4_loss: 0.0171 - q5_loss: 0.0301 - q6_loss: 0.0259 - q7_loss: 0.0288 - q8_loss: 0.0407 - val_loss: 0.2027 - val_q0_loss: 0.0228 - val_q1_loss: 0.0184 - val_q2_loss: 0.0201 - val_q3_loss: 0.0204 - val_q4_loss: 0.0186 - val_q5_loss: 0.0246 - val_q6_loss: 0.0231 - val_q7_loss: 0.0268 - val_q8_loss: 0.0361\n",
      "Epoch 211/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.1881 - q0_loss: 0.0189 - q1_loss: 0.0123 - q2_loss: 0.0140 - q3_loss: 0.0236 - q4_loss: 0.0139 - q5_loss: 0.0295 - q6_loss: 0.0245 - q7_loss: 0.0189 - q8_loss: 0.0325 - val_loss: 0.1655 - val_q0_loss: 0.0208 - val_q1_loss: 0.0142 - val_q2_loss: 0.0089 - val_q3_loss: 0.0245 - val_q4_loss: 0.0148 - val_q5_loss: 0.0310 - val_q6_loss: 0.0267 - val_q7_loss: 0.0140 - val_q8_loss: 0.0191\n",
      "Epoch 212/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.2129 - q0_loss: 0.0202 - q1_loss: 0.0127 - q2_loss: 0.0143 - q3_loss: 0.0308 - q4_loss: 0.0168 - q5_loss: 0.0369 - q6_loss: 0.0303 - q7_loss: 0.0185 - q8_loss: 0.0321 - val_loss: 0.1508 - val_q0_loss: 0.0183 - val_q1_loss: 0.0130 - val_q2_loss: 0.0144 - val_q3_loss: 0.0142 - val_q4_loss: 0.0090 - val_q5_loss: 0.0165 - val_q6_loss: 0.0147 - val_q7_loss: 0.0201 - val_q8_loss: 0.0315\n",
      "Epoch 213/300\n",
      "269/269 [==============================] - 0s 170us/sample - loss: 0.1769 - q0_loss: 0.0193 - q1_loss: 0.0121 - q2_loss: 0.0134 - q3_loss: 0.0221 - q4_loss: 0.0113 - q5_loss: 0.0261 - q6_loss: 0.0209 - q7_loss: 0.0181 - q8_loss: 0.0322 - val_loss: 0.1439 - val_q0_loss: 0.0190 - val_q1_loss: 0.0128 - val_q2_loss: 0.0098 - val_q3_loss: 0.0151 - val_q4_loss: 0.0105 - val_q5_loss: 0.0188 - val_q6_loss: 0.0160 - val_q7_loss: 0.0134 - val_q8_loss: 0.0256\n",
      "Epoch 214/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1487 - q0_loss: 0.0192 - q1_loss: 0.0113 - q2_loss: 0.0129 - q3_loss: 0.0144 - q4_loss: 0.0093 - q5_loss: 0.0169 - q6_loss: 0.0145 - q7_loss: 0.0179 - q8_loss: 0.0308 - val_loss: 0.1417 - val_q0_loss: 0.0172 - val_q1_loss: 0.0110 - val_q2_loss: 0.0139 - val_q3_loss: 0.0109 - val_q4_loss: 0.0075 - val_q5_loss: 0.0129 - val_q6_loss: 0.0096 - val_q7_loss: 0.0164 - val_q8_loss: 0.0267\n",
      "Epoch 215/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1289 - q0_loss: 0.0189 - q1_loss: 0.0114 - q2_loss: 0.0095 - q3_loss: 0.0114 - q4_loss: 0.0087 - q5_loss: 0.0145 - q6_loss: 0.0126 - q7_loss: 0.0160 - q8_loss: 0.0254 - val_loss: 0.1434 - val_q0_loss: 0.0186 - val_q1_loss: 0.0116 - val_q2_loss: 0.0091 - val_q3_loss: 0.0165 - val_q4_loss: 0.0090 - val_q5_loss: 0.0204 - val_q6_loss: 0.0169 - val_q7_loss: 0.0120 - val_q8_loss: 0.0226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 216/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1302 - q0_loss: 0.0183 - q1_loss: 0.0106 - q2_loss: 0.0084 - q3_loss: 0.0137 - q4_loss: 0.0079 - q5_loss: 0.0170 - q6_loss: 0.0137 - q7_loss: 0.0138 - q8_loss: 0.0247 - val_loss: 0.1271 - val_q0_loss: 0.0168 - val_q1_loss: 0.0110 - val_q2_loss: 0.0108 - val_q3_loss: 0.0130 - val_q4_loss: 0.0075 - val_q5_loss: 0.0156 - val_q6_loss: 0.0130 - val_q7_loss: 0.0138 - val_q8_loss: 0.0200\n",
      "Epoch 217/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1280 - q0_loss: 0.0191 - q1_loss: 0.0118 - q2_loss: 0.0103 - q3_loss: 0.0116 - q4_loss: 0.0088 - q5_loss: 0.0133 - q6_loss: 0.0115 - q7_loss: 0.0159 - q8_loss: 0.0262 - val_loss: 0.1294 - val_q0_loss: 0.0195 - val_q1_loss: 0.0154 - val_q2_loss: 0.0140 - val_q3_loss: 0.0092 - val_q4_loss: 0.0102 - val_q5_loss: 0.0106 - val_q6_loss: 0.0123 - val_q7_loss: 0.0209 - val_q8_loss: 0.0287\n",
      "Epoch 218/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1357 - q0_loss: 0.0192 - q1_loss: 0.0115 - q2_loss: 0.0133 - q3_loss: 0.0103 - q4_loss: 0.0079 - q5_loss: 0.0121 - q6_loss: 0.0100 - q7_loss: 0.0185 - q8_loss: 0.0306 - val_loss: 0.1410 - val_q0_loss: 0.0202 - val_q1_loss: 0.0149 - val_q2_loss: 0.0105 - val_q3_loss: 0.0143 - val_q4_loss: 0.0123 - val_q5_loss: 0.0171 - val_q6_loss: 0.0160 - val_q7_loss: 0.0166 - val_q8_loss: 0.0228\n",
      "Epoch 219/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1340 - q0_loss: 0.0197 - q1_loss: 0.0117 - q2_loss: 0.0079 - q3_loss: 0.0137 - q4_loss: 0.0107 - q5_loss: 0.0158 - q6_loss: 0.0142 - q7_loss: 0.0149 - q8_loss: 0.0235 - val_loss: 0.1656 - val_q0_loss: 0.0153 - val_q1_loss: 0.0105 - val_q2_loss: 0.0067 - val_q3_loss: 0.0279 - val_q4_loss: 0.0133 - val_q5_loss: 0.0345 - val_q6_loss: 0.0275 - val_q7_loss: 0.0109 - val_q8_loss: 0.0201\n",
      "Epoch 220/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1716 - q0_loss: 0.0195 - q1_loss: 0.0120 - q2_loss: 0.0085 - q3_loss: 0.0245 - q4_loss: 0.0139 - q5_loss: 0.0310 - q6_loss: 0.0256 - q7_loss: 0.0135 - q8_loss: 0.0236 - val_loss: 0.1347 - val_q0_loss: 0.0162 - val_q1_loss: 0.0110 - val_q2_loss: 0.0080 - val_q3_loss: 0.0188 - val_q4_loss: 0.0097 - val_q5_loss: 0.0240 - val_q6_loss: 0.0203 - val_q7_loss: 0.0104 - val_q8_loss: 0.0177\n",
      "Epoch 221/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1810 - q0_loss: 0.0184 - q1_loss: 0.0123 - q2_loss: 0.0179 - q3_loss: 0.0197 - q4_loss: 0.0100 - q5_loss: 0.0234 - q6_loss: 0.0184 - q7_loss: 0.0222 - q8_loss: 0.0368 - val_loss: 0.2126 - val_q0_loss: 0.0171 - val_q1_loss: 0.0146 - val_q2_loss: 0.0302 - val_q3_loss: 0.0106 - val_q4_loss: 0.0157 - val_q5_loss: 0.0117 - val_q6_loss: 0.0090 - val_q7_loss: 0.0397 - val_q8_loss: 0.0565\n",
      "Epoch 222/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.1614 - q0_loss: 0.0200 - q1_loss: 0.0133 - q2_loss: 0.0176 - q3_loss: 0.0121 - q4_loss: 0.0110 - q5_loss: 0.0152 - q6_loss: 0.0126 - q7_loss: 0.0235 - q8_loss: 0.0352 - val_loss: 0.1760 - val_q0_loss: 0.0170 - val_q1_loss: 0.0109 - val_q2_loss: 0.0173 - val_q3_loss: 0.0220 - val_q4_loss: 0.0065 - val_q5_loss: 0.0276 - val_q6_loss: 0.0197 - val_q7_loss: 0.0193 - val_q8_loss: 0.0341\n",
      "Epoch 223/300\n",
      "269/269 [==============================] - 0s 219us/sample - loss: 0.1622 - q0_loss: 0.0188 - q1_loss: 0.0116 - q2_loss: 0.0139 - q3_loss: 0.0190 - q4_loss: 0.0100 - q5_loss: 0.0230 - q6_loss: 0.0176 - q7_loss: 0.0179 - q8_loss: 0.0306 - val_loss: 0.1556 - val_q0_loss: 0.0169 - val_q1_loss: 0.0115 - val_q2_loss: 0.0154 - val_q3_loss: 0.0142 - val_q4_loss: 0.0077 - val_q5_loss: 0.0177 - val_q6_loss: 0.0140 - val_q7_loss: 0.0211 - val_q8_loss: 0.0308\n",
      "Epoch 224/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1378 - q0_loss: 0.0192 - q1_loss: 0.0115 - q2_loss: 0.0112 - q3_loss: 0.0135 - q4_loss: 0.0086 - q5_loss: 0.0167 - q6_loss: 0.0139 - q7_loss: 0.0162 - q8_loss: 0.0282 - val_loss: 0.1547 - val_q0_loss: 0.0158 - val_q1_loss: 0.0114 - val_q2_loss: 0.0226 - val_q3_loss: 0.0058 - val_q4_loss: 0.0108 - val_q5_loss: 0.0066 - val_q6_loss: 0.0078 - val_q7_loss: 0.0274 - val_q8_loss: 0.0372\n",
      "Epoch 225/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1448 - q0_loss: 0.0187 - q1_loss: 0.0129 - q2_loss: 0.0151 - q3_loss: 0.0112 - q4_loss: 0.0083 - q5_loss: 0.0133 - q6_loss: 0.0107 - q7_loss: 0.0201 - q8_loss: 0.0321 - val_loss: 0.1297 - val_q0_loss: 0.0175 - val_q1_loss: 0.0115 - val_q2_loss: 0.0138 - val_q3_loss: 0.0079 - val_q4_loss: 0.0083 - val_q5_loss: 0.0094 - val_q6_loss: 0.0092 - val_q7_loss: 0.0151 - val_q8_loss: 0.0219\n",
      "Epoch 226/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.1533 - q0_loss: 0.0196 - q1_loss: 0.0119 - q2_loss: 0.0124 - q3_loss: 0.0169 - q4_loss: 0.0110 - q5_loss: 0.0209 - q6_loss: 0.0174 - q7_loss: 0.0171 - q8_loss: 0.0265 - val_loss: 0.1827 - val_q0_loss: 0.0190 - val_q1_loss: 0.0119 - val_q2_loss: 0.0135 - val_q3_loss: 0.0271 - val_q4_loss: 0.0112 - val_q5_loss: 0.0325 - val_q6_loss: 0.0257 - val_q7_loss: 0.0173 - val_q8_loss: 0.0281\n",
      "Epoch 227/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1373 - q0_loss: 0.0186 - q1_loss: 0.0110 - q2_loss: 0.0107 - q3_loss: 0.0147 - q4_loss: 0.0081 - q5_loss: 0.0173 - q6_loss: 0.0144 - q7_loss: 0.0150 - q8_loss: 0.0275 - val_loss: 0.1093 - val_q0_loss: 0.0171 - val_q1_loss: 0.0106 - val_q2_loss: 0.0068 - val_q3_loss: 0.0103 - val_q4_loss: 0.0065 - val_q5_loss: 0.0128 - val_q6_loss: 0.0104 - val_q7_loss: 0.0110 - val_q8_loss: 0.0187\n",
      "Epoch 228/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1215 - q0_loss: 0.0192 - q1_loss: 0.0113 - q2_loss: 0.0094 - q3_loss: 0.0103 - q4_loss: 0.0082 - q5_loss: 0.0119 - q6_loss: 0.0106 - q7_loss: 0.0157 - q8_loss: 0.0252 - val_loss: 0.1059 - val_q0_loss: 0.0183 - val_q1_loss: 0.0128 - val_q2_loss: 0.0106 - val_q3_loss: 0.0062 - val_q4_loss: 0.0069 - val_q5_loss: 0.0071 - val_q6_loss: 0.0064 - val_q7_loss: 0.0145 - val_q8_loss: 0.0219\n",
      "Epoch 229/300\n",
      "269/269 [==============================] - 0s 165us/sample - loss: 0.1442 - q0_loss: 0.0195 - q1_loss: 0.0120 - q2_loss: 0.0091 - q3_loss: 0.0163 - q4_loss: 0.0112 - q5_loss: 0.0201 - q6_loss: 0.0173 - q7_loss: 0.0158 - q8_loss: 0.0241 - val_loss: 0.1843 - val_q0_loss: 0.0180 - val_q1_loss: 0.0103 - val_q2_loss: 0.0193 - val_q3_loss: 0.0212 - val_q4_loss: 0.0097 - val_q5_loss: 0.0263 - val_q6_loss: 0.0189 - val_q7_loss: 0.0215 - val_q8_loss: 0.0364\n",
      "Epoch 230/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1791 - q0_loss: 0.0200 - q1_loss: 0.0138 - q2_loss: 0.0204 - q3_loss: 0.0146 - q4_loss: 0.0115 - q5_loss: 0.0170 - q6_loss: 0.0152 - q7_loss: 0.0270 - q8_loss: 0.0418 - val_loss: 0.2105 - val_q0_loss: 0.0157 - val_q1_loss: 0.0151 - val_q2_loss: 0.0212 - val_q3_loss: 0.0187 - val_q4_loss: 0.0140 - val_q5_loss: 0.0234 - val_q6_loss: 0.0194 - val_q7_loss: 0.0303 - val_q8_loss: 0.0436\n",
      "Epoch 231/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.2203 - q0_loss: 0.0196 - q1_loss: 0.0151 - q2_loss: 0.0236 - q3_loss: 0.0233 - q4_loss: 0.0136 - q5_loss: 0.0285 - q6_loss: 0.0224 - q7_loss: 0.0285 - q8_loss: 0.0453 - val_loss: 0.1867 - val_q0_loss: 0.0165 - val_q1_loss: 0.0149 - val_q2_loss: 0.0163 - val_q3_loss: 0.0168 - val_q4_loss: 0.0138 - val_q5_loss: 0.0219 - val_q6_loss: 0.0202 - val_q7_loss: 0.0255 - val_q8_loss: 0.0356\n",
      "Epoch 232/300\n",
      "269/269 [==============================] - 0s 178us/sample - loss: 0.1585 - q0_loss: 0.0198 - q1_loss: 0.0132 - q2_loss: 0.0154 - q3_loss: 0.0136 - q4_loss: 0.0113 - q5_loss: 0.0162 - q6_loss: 0.0138 - q7_loss: 0.0217 - q8_loss: 0.0334 - val_loss: 0.1446 - val_q0_loss: 0.0174 - val_q1_loss: 0.0102 - val_q2_loss: 0.0114 - val_q3_loss: 0.0147 - val_q4_loss: 0.0066 - val_q5_loss: 0.0180 - val_q6_loss: 0.0138 - val_q7_loss: 0.0149 - val_q8_loss: 0.0256\n",
      "Epoch 233/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.1568 - q0_loss: 0.0200 - q1_loss: 0.0121 - q2_loss: 0.0119 - q3_loss: 0.0153 - q4_loss: 0.0114 - q5_loss: 0.0181 - q6_loss: 0.0165 - q7_loss: 0.0196 - q8_loss: 0.0289 - val_loss: 0.1938 - val_q0_loss: 0.0223 - val_q1_loss: 0.0194 - val_q2_loss: 0.0167 - val_q3_loss: 0.0186 - val_q4_loss: 0.0205 - val_q5_loss: 0.0240 - val_q6_loss: 0.0230 - val_q7_loss: 0.0258 - val_q8_loss: 0.0301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 234/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.1869 - q0_loss: 0.0198 - q1_loss: 0.0136 - q2_loss: 0.0136 - q3_loss: 0.0226 - q4_loss: 0.0167 - q5_loss: 0.0275 - q6_loss: 0.0239 - q7_loss: 0.0214 - q8_loss: 0.0291 - val_loss: 0.1887 - val_q0_loss: 0.0204 - val_q1_loss: 0.0172 - val_q2_loss: 0.0209 - val_q3_loss: 0.0160 - val_q4_loss: 0.0165 - val_q5_loss: 0.0210 - val_q6_loss: 0.0198 - val_q7_loss: 0.0304 - val_q8_loss: 0.0417\n",
      "Epoch 235/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.1919 - q0_loss: 0.0204 - q1_loss: 0.0165 - q2_loss: 0.0171 - q3_loss: 0.0174 - q4_loss: 0.0166 - q5_loss: 0.0213 - q6_loss: 0.0206 - q7_loss: 0.0260 - q8_loss: 0.0347 - val_loss: 0.1795 - val_q0_loss: 0.0211 - val_q1_loss: 0.0149 - val_q2_loss: 0.0116 - val_q3_loss: 0.0248 - val_q4_loss: 0.0161 - val_q5_loss: 0.0315 - val_q6_loss: 0.0264 - val_q7_loss: 0.0155 - val_q8_loss: 0.0194\n",
      "Epoch 236/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1673 - q0_loss: 0.0192 - q1_loss: 0.0129 - q2_loss: 0.0133 - q3_loss: 0.0178 - q4_loss: 0.0131 - q5_loss: 0.0218 - q6_loss: 0.0189 - q7_loss: 0.0198 - q8_loss: 0.0288 - val_loss: 0.1491 - val_q0_loss: 0.0175 - val_q1_loss: 0.0117 - val_q2_loss: 0.0090 - val_q3_loss: 0.0180 - val_q4_loss: 0.0121 - val_q5_loss: 0.0229 - val_q6_loss: 0.0202 - val_q7_loss: 0.0135 - val_q8_loss: 0.0207\n",
      "Epoch 237/300\n",
      "269/269 [==============================] - 0s 191us/sample - loss: 0.1920 - q0_loss: 0.0198 - q1_loss: 0.0134 - q2_loss: 0.0096 - q3_loss: 0.0261 - q4_loss: 0.0174 - q5_loss: 0.0322 - q6_loss: 0.0279 - q7_loss: 0.0171 - q8_loss: 0.0256 - val_loss: 0.1170 - val_q0_loss: 0.0165 - val_q1_loss: 0.0115 - val_q2_loss: 0.0066 - val_q3_loss: 0.0130 - val_q4_loss: 0.0087 - val_q5_loss: 0.0154 - val_q6_loss: 0.0125 - val_q7_loss: 0.0116 - val_q8_loss: 0.0196\n",
      "Epoch 238/300\n",
      "269/269 [==============================] - 0s 182us/sample - loss: 0.1449 - q0_loss: 0.0196 - q1_loss: 0.0123 - q2_loss: 0.0092 - q3_loss: 0.0187 - q4_loss: 0.0110 - q5_loss: 0.0217 - q6_loss: 0.0182 - q7_loss: 0.0148 - q8_loss: 0.0235 - val_loss: 0.1793 - val_q0_loss: 0.0170 - val_q1_loss: 0.0117 - val_q2_loss: 0.0179 - val_q3_loss: 0.0243 - val_q4_loss: 0.0083 - val_q5_loss: 0.0312 - val_q6_loss: 0.0231 - val_q7_loss: 0.0181 - val_q8_loss: 0.0346\n",
      "Epoch 239/300\n",
      "269/269 [==============================] - 0s 186us/sample - loss: 0.1630 - q0_loss: 0.0185 - q1_loss: 0.0114 - q2_loss: 0.0140 - q3_loss: 0.0173 - q4_loss: 0.0108 - q5_loss: 0.0214 - q6_loss: 0.0168 - q7_loss: 0.0202 - q8_loss: 0.0320 - val_loss: 0.1651 - val_q0_loss: 0.0222 - val_q1_loss: 0.0196 - val_q2_loss: 0.0237 - val_q3_loss: 0.0067 - val_q4_loss: 0.0150 - val_q5_loss: 0.0079 - val_q6_loss: 0.0101 - val_q7_loss: 0.0311 - val_q8_loss: 0.0422\n",
      "Epoch 240/300\n",
      "269/269 [==============================] - 0s 182us/sample - loss: 0.1936 - q0_loss: 0.0196 - q1_loss: 0.0125 - q2_loss: 0.0207 - q3_loss: 0.0212 - q4_loss: 0.0092 - q5_loss: 0.0255 - q6_loss: 0.0183 - q7_loss: 0.0256 - q8_loss: 0.0418 - val_loss: 0.2255 - val_q0_loss: 0.0186 - val_q1_loss: 0.0104 - val_q2_loss: 0.0270 - val_q3_loss: 0.0294 - val_q4_loss: 0.0085 - val_q5_loss: 0.0358 - val_q6_loss: 0.0225 - val_q7_loss: 0.0294 - val_q8_loss: 0.0484\n",
      "Epoch 241/300\n",
      "269/269 [==============================] - 0s 182us/sample - loss: 0.1882 - q0_loss: 0.0197 - q1_loss: 0.0150 - q2_loss: 0.0219 - q3_loss: 0.0152 - q4_loss: 0.0129 - q5_loss: 0.0183 - q6_loss: 0.0150 - q7_loss: 0.0290 - q8_loss: 0.0415 - val_loss: 0.1417 - val_q0_loss: 0.0189 - val_q1_loss: 0.0154 - val_q2_loss: 0.0202 - val_q3_loss: 0.0086 - val_q4_loss: 0.0080 - val_q5_loss: 0.0117 - val_q6_loss: 0.0093 - val_q7_loss: 0.0246 - val_q8_loss: 0.0388\n",
      "Epoch 242/300\n",
      "269/269 [==============================] - 0s 188us/sample - loss: 0.1573 - q0_loss: 0.0180 - q1_loss: 0.0120 - q2_loss: 0.0174 - q3_loss: 0.0141 - q4_loss: 0.0086 - q5_loss: 0.0162 - q6_loss: 0.0129 - q7_loss: 0.0218 - q8_loss: 0.0357 - val_loss: 0.1370 - val_q0_loss: 0.0186 - val_q1_loss: 0.0145 - val_q2_loss: 0.0187 - val_q3_loss: 0.0089 - val_q4_loss: 0.0084 - val_q5_loss: 0.0113 - val_q6_loss: 0.0084 - val_q7_loss: 0.0220 - val_q8_loss: 0.0355\n",
      "Epoch 243/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.1614 - q0_loss: 0.0201 - q1_loss: 0.0130 - q2_loss: 0.0123 - q3_loss: 0.0166 - q4_loss: 0.0132 - q5_loss: 0.0201 - q6_loss: 0.0180 - q7_loss: 0.0184 - q8_loss: 0.0279 - val_loss: 0.1258 - val_q0_loss: 0.0200 - val_q1_loss: 0.0143 - val_q2_loss: 0.0079 - val_q3_loss: 0.0156 - val_q4_loss: 0.0108 - val_q5_loss: 0.0174 - val_q6_loss: 0.0158 - val_q7_loss: 0.0126 - val_q8_loss: 0.0207\n",
      "Epoch 244/300\n",
      "269/269 [==============================] - 0s 180us/sample - loss: 0.1375 - q0_loss: 0.0183 - q1_loss: 0.0108 - q2_loss: 0.0083 - q3_loss: 0.0165 - q4_loss: 0.0104 - q5_loss: 0.0188 - q6_loss: 0.0157 - q7_loss: 0.0137 - q8_loss: 0.0233 - val_loss: 0.1680 - val_q0_loss: 0.0155 - val_q1_loss: 0.0121 - val_q2_loss: 0.0084 - val_q3_loss: 0.0221 - val_q4_loss: 0.0141 - val_q5_loss: 0.0278 - val_q6_loss: 0.0242 - val_q7_loss: 0.0146 - val_q8_loss: 0.0193\n",
      "Epoch 245/300\n",
      "269/269 [==============================] - 0s 191us/sample - loss: 0.1622 - q0_loss: 0.0184 - q1_loss: 0.0126 - q2_loss: 0.0095 - q3_loss: 0.0204 - q4_loss: 0.0133 - q5_loss: 0.0250 - q6_loss: 0.0214 - q7_loss: 0.0167 - q8_loss: 0.0262 - val_loss: 0.1630 - val_q0_loss: 0.0176 - val_q1_loss: 0.0121 - val_q2_loss: 0.0166 - val_q3_loss: 0.0230 - val_q4_loss: 0.0065 - val_q5_loss: 0.0266 - val_q6_loss: 0.0182 - val_q7_loss: 0.0184 - val_q8_loss: 0.0326\n",
      "Epoch 246/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1613 - q0_loss: 0.0187 - q1_loss: 0.0120 - q2_loss: 0.0131 - q3_loss: 0.0182 - q4_loss: 0.0100 - q5_loss: 0.0222 - q6_loss: 0.0184 - q7_loss: 0.0176 - q8_loss: 0.0301 - val_loss: 0.1790 - val_q0_loss: 0.0205 - val_q1_loss: 0.0149 - val_q2_loss: 0.0153 - val_q3_loss: 0.0197 - val_q4_loss: 0.0136 - val_q5_loss: 0.0238 - val_q6_loss: 0.0188 - val_q7_loss: 0.0210 - val_q8_loss: 0.0297\n",
      "Epoch 247/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1613 - q0_loss: 0.0190 - q1_loss: 0.0127 - q2_loss: 0.0158 - q3_loss: 0.0151 - q4_loss: 0.0107 - q5_loss: 0.0178 - q6_loss: 0.0147 - q7_loss: 0.0232 - q8_loss: 0.0324 - val_loss: 0.1584 - val_q0_loss: 0.0188 - val_q1_loss: 0.0157 - val_q2_loss: 0.0191 - val_q3_loss: 0.0134 - val_q4_loss: 0.0090 - val_q5_loss: 0.0176 - val_q6_loss: 0.0131 - val_q7_loss: 0.0257 - val_q8_loss: 0.0365\n",
      "Epoch 248/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1860 - q0_loss: 0.0197 - q1_loss: 0.0137 - q2_loss: 0.0165 - q3_loss: 0.0195 - q4_loss: 0.0159 - q5_loss: 0.0242 - q6_loss: 0.0214 - q7_loss: 0.0244 - q8_loss: 0.0335 - val_loss: 0.1269 - val_q0_loss: 0.0168 - val_q1_loss: 0.0103 - val_q2_loss: 0.0129 - val_q3_loss: 0.0063 - val_q4_loss: 0.0082 - val_q5_loss: 0.0075 - val_q6_loss: 0.0072 - val_q7_loss: 0.0177 - val_q8_loss: 0.0243\n",
      "Epoch 249/300\n",
      "269/269 [==============================] - 0s 165us/sample - loss: 0.1799 - q0_loss: 0.0197 - q1_loss: 0.0132 - q2_loss: 0.0118 - q3_loss: 0.0221 - q4_loss: 0.0149 - q5_loss: 0.0265 - q6_loss: 0.0232 - q7_loss: 0.0195 - q8_loss: 0.0287 - val_loss: 0.1214 - val_q0_loss: 0.0185 - val_q1_loss: 0.0136 - val_q2_loss: 0.0140 - val_q3_loss: 0.0095 - val_q4_loss: 0.0075 - val_q5_loss: 0.0122 - val_q6_loss: 0.0111 - val_q7_loss: 0.0177 - val_q8_loss: 0.0289\n",
      "Epoch 250/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1620 - q0_loss: 0.0198 - q1_loss: 0.0128 - q2_loss: 0.0115 - q3_loss: 0.0181 - q4_loss: 0.0134 - q5_loss: 0.0224 - q6_loss: 0.0197 - q7_loss: 0.0177 - q8_loss: 0.0263 - val_loss: 0.1439 - val_q0_loss: 0.0184 - val_q1_loss: 0.0114 - val_q2_loss: 0.0108 - val_q3_loss: 0.0175 - val_q4_loss: 0.0077 - val_q5_loss: 0.0210 - val_q6_loss: 0.0160 - val_q7_loss: 0.0145 - val_q8_loss: 0.0253\n",
      "Epoch 251/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1653 - q0_loss: 0.0189 - q1_loss: 0.0124 - q2_loss: 0.0135 - q3_loss: 0.0196 - q4_loss: 0.0117 - q5_loss: 0.0236 - q6_loss: 0.0186 - q7_loss: 0.0180 - q8_loss: 0.0307 - val_loss: 0.2023 - val_q0_loss: 0.0170 - val_q1_loss: 0.0108 - val_q2_loss: 0.0233 - val_q3_loss: 0.0158 - val_q4_loss: 0.0117 - val_q5_loss: 0.0197 - val_q6_loss: 0.0162 - val_q7_loss: 0.0319 - val_q8_loss: 0.0431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 252/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1636 - q0_loss: 0.0190 - q1_loss: 0.0136 - q2_loss: 0.0180 - q3_loss: 0.0121 - q4_loss: 0.0109 - q5_loss: 0.0146 - q6_loss: 0.0117 - q7_loss: 0.0258 - q8_loss: 0.0385 - val_loss: 0.1128 - val_q0_loss: 0.0193 - val_q1_loss: 0.0131 - val_q2_loss: 0.0094 - val_q3_loss: 0.0109 - val_q4_loss: 0.0084 - val_q5_loss: 0.0116 - val_q6_loss: 0.0121 - val_q7_loss: 0.0144 - val_q8_loss: 0.0195\n",
      "Epoch 253/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.1485 - q0_loss: 0.0194 - q1_loss: 0.0126 - q2_loss: 0.0166 - q3_loss: 0.0092 - q4_loss: 0.0096 - q5_loss: 0.0100 - q6_loss: 0.0092 - q7_loss: 0.0238 - q8_loss: 0.0355 - val_loss: 0.1480 - val_q0_loss: 0.0211 - val_q1_loss: 0.0166 - val_q2_loss: 0.0171 - val_q3_loss: 0.0099 - val_q4_loss: 0.0139 - val_q5_loss: 0.0114 - val_q6_loss: 0.0132 - val_q7_loss: 0.0246 - val_q8_loss: 0.0325\n",
      "Epoch 254/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.1319 - q0_loss: 0.0191 - q1_loss: 0.0123 - q2_loss: 0.0114 - q3_loss: 0.0110 - q4_loss: 0.0101 - q5_loss: 0.0122 - q6_loss: 0.0117 - q7_loss: 0.0179 - q8_loss: 0.0258 - val_loss: 0.1171 - val_q0_loss: 0.0186 - val_q1_loss: 0.0142 - val_q2_loss: 0.0128 - val_q3_loss: 0.0064 - val_q4_loss: 0.0082 - val_q5_loss: 0.0074 - val_q6_loss: 0.0073 - val_q7_loss: 0.0193 - val_q8_loss: 0.0287\n",
      "Epoch 255/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1285 - q0_loss: 0.0186 - q1_loss: 0.0114 - q2_loss: 0.0119 - q3_loss: 0.0094 - q4_loss: 0.0087 - q5_loss: 0.0104 - q6_loss: 0.0098 - q7_loss: 0.0180 - q8_loss: 0.0293 - val_loss: 0.1773 - val_q0_loss: 0.0154 - val_q1_loss: 0.0117 - val_q2_loss: 0.0075 - val_q3_loss: 0.0277 - val_q4_loss: 0.0167 - val_q5_loss: 0.0340 - val_q6_loss: 0.0284 - val_q7_loss: 0.0136 - val_q8_loss: 0.0211\n",
      "Epoch 256/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1873 - q0_loss: 0.0208 - q1_loss: 0.0121 - q2_loss: 0.0113 - q3_loss: 0.0280 - q4_loss: 0.0162 - q5_loss: 0.0351 - q6_loss: 0.0290 - q7_loss: 0.0146 - q8_loss: 0.0246 - val_loss: 0.2140 - val_q0_loss: 0.0185 - val_q1_loss: 0.0117 - val_q2_loss: 0.0162 - val_q3_loss: 0.0304 - val_q4_loss: 0.0116 - val_q5_loss: 0.0384 - val_q6_loss: 0.0301 - val_q7_loss: 0.0156 - val_q8_loss: 0.0307\n",
      "Epoch 257/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1881 - q0_loss: 0.0186 - q1_loss: 0.0109 - q2_loss: 0.0152 - q3_loss: 0.0246 - q4_loss: 0.0106 - q5_loss: 0.0302 - q6_loss: 0.0224 - q7_loss: 0.0184 - q8_loss: 0.0328 - val_loss: 0.1558 - val_q0_loss: 0.0200 - val_q1_loss: 0.0148 - val_q2_loss: 0.0087 - val_q3_loss: 0.0199 - val_q4_loss: 0.0142 - val_q5_loss: 0.0249 - val_q6_loss: 0.0224 - val_q7_loss: 0.0143 - val_q8_loss: 0.0203\n",
      "Epoch 258/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.1490 - q0_loss: 0.0188 - q1_loss: 0.0118 - q2_loss: 0.0125 - q3_loss: 0.0154 - q4_loss: 0.0103 - q5_loss: 0.0185 - q6_loss: 0.0153 - q7_loss: 0.0170 - q8_loss: 0.0288 - val_loss: 0.1099 - val_q0_loss: 0.0176 - val_q1_loss: 0.0126 - val_q2_loss: 0.0122 - val_q3_loss: 0.0080 - val_q4_loss: 0.0050 - val_q5_loss: 0.0098 - val_q6_loss: 0.0079 - val_q7_loss: 0.0161 - val_q8_loss: 0.0261\n",
      "Epoch 259/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1229 - q0_loss: 0.0190 - q1_loss: 0.0109 - q2_loss: 0.0087 - q3_loss: 0.0121 - q4_loss: 0.0092 - q5_loss: 0.0145 - q6_loss: 0.0124 - q7_loss: 0.0146 - q8_loss: 0.0237 - val_loss: 0.0988 - val_q0_loss: 0.0183 - val_q1_loss: 0.0117 - val_q2_loss: 0.0073 - val_q3_loss: 0.0082 - val_q4_loss: 0.0059 - val_q5_loss: 0.0086 - val_q6_loss: 0.0074 - val_q7_loss: 0.0119 - val_q8_loss: 0.0209\n",
      "Epoch 260/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1384 - q0_loss: 0.0186 - q1_loss: 0.0117 - q2_loss: 0.0102 - q3_loss: 0.0134 - q4_loss: 0.0096 - q5_loss: 0.0155 - q6_loss: 0.0141 - q7_loss: 0.0170 - q8_loss: 0.0253 - val_loss: 0.1375 - val_q0_loss: 0.0212 - val_q1_loss: 0.0153 - val_q2_loss: 0.0143 - val_q3_loss: 0.0119 - val_q4_loss: 0.0123 - val_q5_loss: 0.0145 - val_q6_loss: 0.0136 - val_q7_loss: 0.0182 - val_q8_loss: 0.0249\n",
      "Epoch 261/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1386 - q0_loss: 0.0186 - q1_loss: 0.0123 - q2_loss: 0.0094 - q3_loss: 0.0152 - q4_loss: 0.0112 - q5_loss: 0.0180 - q6_loss: 0.0159 - q7_loss: 0.0149 - q8_loss: 0.0238 - val_loss: 0.1917 - val_q0_loss: 0.0167 - val_q1_loss: 0.0121 - val_q2_loss: 0.0069 - val_q3_loss: 0.0306 - val_q4_loss: 0.0200 - val_q5_loss: 0.0369 - val_q6_loss: 0.0331 - val_q7_loss: 0.0159 - val_q8_loss: 0.0175\n",
      "Epoch 262/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1569 - q0_loss: 0.0195 - q1_loss: 0.0115 - q2_loss: 0.0080 - q3_loss: 0.0199 - q4_loss: 0.0124 - q5_loss: 0.0250 - q6_loss: 0.0210 - q7_loss: 0.0133 - q8_loss: 0.0237 - val_loss: 0.1246 - val_q0_loss: 0.0166 - val_q1_loss: 0.0110 - val_q2_loss: 0.0103 - val_q3_loss: 0.0112 - val_q4_loss: 0.0098 - val_q5_loss: 0.0132 - val_q6_loss: 0.0123 - val_q7_loss: 0.0166 - val_q8_loss: 0.0200\n",
      "Epoch 263/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1382 - q0_loss: 0.0189 - q1_loss: 0.0119 - q2_loss: 0.0116 - q3_loss: 0.0103 - q4_loss: 0.0092 - q5_loss: 0.0129 - q6_loss: 0.0119 - q7_loss: 0.0187 - q8_loss: 0.0295 - val_loss: 0.1337 - val_q0_loss: 0.0168 - val_q1_loss: 0.0104 - val_q2_loss: 0.0147 - val_q3_loss: 0.0076 - val_q4_loss: 0.0086 - val_q5_loss: 0.0096 - val_q6_loss: 0.0084 - val_q7_loss: 0.0200 - val_q8_loss: 0.0242\n",
      "Epoch 264/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.1319 - q0_loss: 0.0187 - q1_loss: 0.0118 - q2_loss: 0.0130 - q3_loss: 0.0093 - q4_loss: 0.0092 - q5_loss: 0.0106 - q6_loss: 0.0100 - q7_loss: 0.0190 - q8_loss: 0.0285 - val_loss: 0.1231 - val_q0_loss: 0.0168 - val_q1_loss: 0.0108 - val_q2_loss: 0.0094 - val_q3_loss: 0.0122 - val_q4_loss: 0.0068 - val_q5_loss: 0.0158 - val_q6_loss: 0.0128 - val_q7_loss: 0.0123 - val_q8_loss: 0.0197\n",
      "Epoch 265/300\n",
      "269/269 [==============================] - 0s 197us/sample - loss: 0.1225 - q0_loss: 0.0186 - q1_loss: 0.0111 - q2_loss: 0.0096 - q3_loss: 0.0101 - q4_loss: 0.0082 - q5_loss: 0.0115 - q6_loss: 0.0107 - q7_loss: 0.0157 - q8_loss: 0.0255 - val_loss: 0.1461 - val_q0_loss: 0.0161 - val_q1_loss: 0.0114 - val_q2_loss: 0.0121 - val_q3_loss: 0.0143 - val_q4_loss: 0.0102 - val_q5_loss: 0.0180 - val_q6_loss: 0.0145 - val_q7_loss: 0.0182 - val_q8_loss: 0.0271\n",
      "Epoch 266/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.1309 - q0_loss: 0.0185 - q1_loss: 0.0111 - q2_loss: 0.0098 - q3_loss: 0.0129 - q4_loss: 0.0084 - q5_loss: 0.0152 - q6_loss: 0.0129 - q7_loss: 0.0159 - q8_loss: 0.0255 - val_loss: 0.1886 - val_q0_loss: 0.0222 - val_q1_loss: 0.0188 - val_q2_loss: 0.0143 - val_q3_loss: 0.0244 - val_q4_loss: 0.0195 - val_q5_loss: 0.0287 - val_q6_loss: 0.0266 - val_q7_loss: 0.0211 - val_q8_loss: 0.0283\n",
      "Epoch 267/300\n",
      "269/269 [==============================] - 0s 186us/sample - loss: 0.1696 - q0_loss: 0.0192 - q1_loss: 0.0124 - q2_loss: 0.0105 - q3_loss: 0.0205 - q4_loss: 0.0130 - q5_loss: 0.0249 - q6_loss: 0.0209 - q7_loss: 0.0176 - q8_loss: 0.0273 - val_loss: 0.1804 - val_q0_loss: 0.0214 - val_q1_loss: 0.0156 - val_q2_loss: 0.0074 - val_q3_loss: 0.0269 - val_q4_loss: 0.0192 - val_q5_loss: 0.0331 - val_q6_loss: 0.0291 - val_q7_loss: 0.0143 - val_q8_loss: 0.0184\n",
      "Epoch 268/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1772 - q0_loss: 0.0194 - q1_loss: 0.0131 - q2_loss: 0.0163 - q3_loss: 0.0170 - q4_loss: 0.0131 - q5_loss: 0.0205 - q6_loss: 0.0182 - q7_loss: 0.0232 - q8_loss: 0.0336 - val_loss: 0.1884 - val_q0_loss: 0.0202 - val_q1_loss: 0.0176 - val_q2_loss: 0.0206 - val_q3_loss: 0.0115 - val_q4_loss: 0.0176 - val_q5_loss: 0.0133 - val_q6_loss: 0.0161 - val_q7_loss: 0.0302 - val_q8_loss: 0.0414\n",
      "Epoch 269/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1554 - q0_loss: 0.0189 - q1_loss: 0.0125 - q2_loss: 0.0148 - q3_loss: 0.0147 - q4_loss: 0.0091 - q5_loss: 0.0173 - q6_loss: 0.0140 - q7_loss: 0.0204 - q8_loss: 0.0326 - val_loss: 0.2161 - val_q0_loss: 0.0169 - val_q1_loss: 0.0113 - val_q2_loss: 0.0171 - val_q3_loss: 0.0279 - val_q4_loss: 0.0132 - val_q5_loss: 0.0346 - val_q6_loss: 0.0256 - val_q7_loss: 0.0216 - val_q8_loss: 0.0384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 270/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1913 - q0_loss: 0.0198 - q1_loss: 0.0125 - q2_loss: 0.0170 - q3_loss: 0.0225 - q4_loss: 0.0123 - q5_loss: 0.0284 - q6_loss: 0.0217 - q7_loss: 0.0206 - q8_loss: 0.0344 - val_loss: 0.2110 - val_q0_loss: 0.0175 - val_q1_loss: 0.0111 - val_q2_loss: 0.0247 - val_q3_loss: 0.0249 - val_q4_loss: 0.0064 - val_q5_loss: 0.0313 - val_q6_loss: 0.0215 - val_q7_loss: 0.0277 - val_q8_loss: 0.0455\n",
      "Epoch 271/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.1927 - q0_loss: 0.0210 - q1_loss: 0.0150 - q2_loss: 0.0169 - q3_loss: 0.0212 - q4_loss: 0.0152 - q5_loss: 0.0263 - q6_loss: 0.0230 - q7_loss: 0.0254 - q8_loss: 0.0337 - val_loss: 0.2251 - val_q0_loss: 0.0246 - val_q1_loss: 0.0188 - val_q2_loss: 0.0097 - val_q3_loss: 0.0400 - val_q4_loss: 0.0241 - val_q5_loss: 0.0509 - val_q6_loss: 0.0431 - val_q7_loss: 0.0155 - val_q8_loss: 0.0238\n",
      "Epoch 272/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.2008 - q0_loss: 0.0208 - q1_loss: 0.0142 - q2_loss: 0.0147 - q3_loss: 0.0257 - q4_loss: 0.0166 - q5_loss: 0.0313 - q6_loss: 0.0271 - q7_loss: 0.0201 - q8_loss: 0.0307 - val_loss: 0.1890 - val_q0_loss: 0.0215 - val_q1_loss: 0.0171 - val_q2_loss: 0.0170 - val_q3_loss: 0.0195 - val_q4_loss: 0.0169 - val_q5_loss: 0.0244 - val_q6_loss: 0.0222 - val_q7_loss: 0.0233 - val_q8_loss: 0.0375\n",
      "Epoch 273/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1458 - q0_loss: 0.0185 - q1_loss: 0.0119 - q2_loss: 0.0123 - q3_loss: 0.0137 - q4_loss: 0.0102 - q5_loss: 0.0161 - q6_loss: 0.0138 - q7_loss: 0.0197 - q8_loss: 0.0295 - val_loss: 0.1592 - val_q0_loss: 0.0153 - val_q1_loss: 0.0118 - val_q2_loss: 0.0084 - val_q3_loss: 0.0226 - val_q4_loss: 0.0147 - val_q5_loss: 0.0274 - val_q6_loss: 0.0239 - val_q7_loss: 0.0167 - val_q8_loss: 0.0218\n",
      "Epoch 274/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.1542 - q0_loss: 0.0196 - q1_loss: 0.0134 - q2_loss: 0.0133 - q3_loss: 0.0144 - q4_loss: 0.0123 - q5_loss: 0.0166 - q6_loss: 0.0156 - q7_loss: 0.0200 - q8_loss: 0.0290 - val_loss: 0.1497 - val_q0_loss: 0.0180 - val_q1_loss: 0.0118 - val_q2_loss: 0.0151 - val_q3_loss: 0.0134 - val_q4_loss: 0.0087 - val_q5_loss: 0.0149 - val_q6_loss: 0.0133 - val_q7_loss: 0.0193 - val_q8_loss: 0.0257\n",
      "Epoch 275/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1452 - q0_loss: 0.0184 - q1_loss: 0.0113 - q2_loss: 0.0111 - q3_loss: 0.0140 - q4_loss: 0.0102 - q5_loss: 0.0167 - q6_loss: 0.0146 - q7_loss: 0.0175 - q8_loss: 0.0276 - val_loss: 0.1470 - val_q0_loss: 0.0193 - val_q1_loss: 0.0136 - val_q2_loss: 0.0078 - val_q3_loss: 0.0186 - val_q4_loss: 0.0131 - val_q5_loss: 0.0237 - val_q6_loss: 0.0213 - val_q7_loss: 0.0128 - val_q8_loss: 0.0191\n",
      "Epoch 276/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1399 - q0_loss: 0.0190 - q1_loss: 0.0123 - q2_loss: 0.0101 - q3_loss: 0.0141 - q4_loss: 0.0117 - q5_loss: 0.0165 - q6_loss: 0.0154 - q7_loss: 0.0179 - q8_loss: 0.0248 - val_loss: 0.1539 - val_q0_loss: 0.0171 - val_q1_loss: 0.0109 - val_q2_loss: 0.0111 - val_q3_loss: 0.0148 - val_q4_loss: 0.0144 - val_q5_loss: 0.0172 - val_q6_loss: 0.0166 - val_q7_loss: 0.0184 - val_q8_loss: 0.0233\n",
      "Epoch 277/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1697 - q0_loss: 0.0205 - q1_loss: 0.0142 - q2_loss: 0.0130 - q3_loss: 0.0177 - q4_loss: 0.0154 - q5_loss: 0.0211 - q6_loss: 0.0198 - q7_loss: 0.0225 - q8_loss: 0.0293 - val_loss: 0.1448 - val_q0_loss: 0.0176 - val_q1_loss: 0.0113 - val_q2_loss: 0.0120 - val_q3_loss: 0.0125 - val_q4_loss: 0.0096 - val_q5_loss: 0.0157 - val_q6_loss: 0.0124 - val_q7_loss: 0.0156 - val_q8_loss: 0.0210\n",
      "Epoch 278/300\n",
      "269/269 [==============================] - 0s 178us/sample - loss: 0.1598 - q0_loss: 0.0199 - q1_loss: 0.0134 - q2_loss: 0.0105 - q3_loss: 0.0172 - q4_loss: 0.0136 - q5_loss: 0.0211 - q6_loss: 0.0203 - q7_loss: 0.0196 - q8_loss: 0.0265 - val_loss: 0.1546 - val_q0_loss: 0.0208 - val_q1_loss: 0.0153 - val_q2_loss: 0.0084 - val_q3_loss: 0.0193 - val_q4_loss: 0.0175 - val_q5_loss: 0.0235 - val_q6_loss: 0.0221 - val_q7_loss: 0.0164 - val_q8_loss: 0.0221\n",
      "Epoch 279/300\n",
      "269/269 [==============================] - 0s 178us/sample - loss: 0.1534 - q0_loss: 0.0197 - q1_loss: 0.0126 - q2_loss: 0.0101 - q3_loss: 0.0162 - q4_loss: 0.0139 - q5_loss: 0.0203 - q6_loss: 0.0182 - q7_loss: 0.0173 - q8_loss: 0.0249 - val_loss: 0.1498 - val_q0_loss: 0.0171 - val_q1_loss: 0.0133 - val_q2_loss: 0.0113 - val_q3_loss: 0.0148 - val_q4_loss: 0.0122 - val_q5_loss: 0.0186 - val_q6_loss: 0.0172 - val_q7_loss: 0.0182 - val_q8_loss: 0.0282\n",
      "Epoch 280/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1616 - q0_loss: 0.0203 - q1_loss: 0.0137 - q2_loss: 0.0117 - q3_loss: 0.0162 - q4_loss: 0.0141 - q5_loss: 0.0201 - q6_loss: 0.0188 - q7_loss: 0.0180 - q8_loss: 0.0260 - val_loss: 0.1418 - val_q0_loss: 0.0166 - val_q1_loss: 0.0105 - val_q2_loss: 0.0081 - val_q3_loss: 0.0186 - val_q4_loss: 0.0114 - val_q5_loss: 0.0217 - val_q6_loss: 0.0179 - val_q7_loss: 0.0115 - val_q8_loss: 0.0195\n",
      "Epoch 281/300\n",
      "269/269 [==============================] - 0s 165us/sample - loss: 0.1397 - q0_loss: 0.0196 - q1_loss: 0.0121 - q2_loss: 0.0081 - q3_loss: 0.0166 - q4_loss: 0.0118 - q5_loss: 0.0200 - q6_loss: 0.0177 - q7_loss: 0.0147 - q8_loss: 0.0219 - val_loss: 0.1737 - val_q0_loss: 0.0197 - val_q1_loss: 0.0121 - val_q2_loss: 0.0078 - val_q3_loss: 0.0221 - val_q4_loss: 0.0143 - val_q5_loss: 0.0274 - val_q6_loss: 0.0237 - val_q7_loss: 0.0106 - val_q8_loss: 0.0170\n",
      "Epoch 282/300\n",
      "269/269 [==============================] - 0s 178us/sample - loss: 0.1558 - q0_loss: 0.0190 - q1_loss: 0.0123 - q2_loss: 0.0111 - q3_loss: 0.0165 - q4_loss: 0.0121 - q5_loss: 0.0200 - q6_loss: 0.0178 - q7_loss: 0.0174 - q8_loss: 0.0278 - val_loss: 0.1648 - val_q0_loss: 0.0159 - val_q1_loss: 0.0115 - val_q2_loss: 0.0134 - val_q3_loss: 0.0158 - val_q4_loss: 0.0152 - val_q5_loss: 0.0194 - val_q6_loss: 0.0199 - val_q7_loss: 0.0235 - val_q8_loss: 0.0259\n",
      "Epoch 283/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.1665 - q0_loss: 0.0194 - q1_loss: 0.0128 - q2_loss: 0.0113 - q3_loss: 0.0189 - q4_loss: 0.0122 - q5_loss: 0.0225 - q6_loss: 0.0200 - q7_loss: 0.0188 - q8_loss: 0.0273 - val_loss: 0.1796 - val_q0_loss: 0.0175 - val_q1_loss: 0.0140 - val_q2_loss: 0.0063 - val_q3_loss: 0.0250 - val_q4_loss: 0.0156 - val_q5_loss: 0.0314 - val_q6_loss: 0.0274 - val_q7_loss: 0.0136 - val_q8_loss: 0.0183\n",
      "Epoch 284/300\n",
      "269/269 [==============================] - 0s 175us/sample - loss: 0.1602 - q0_loss: 0.0188 - q1_loss: 0.0117 - q2_loss: 0.0089 - q3_loss: 0.0210 - q4_loss: 0.0132 - q5_loss: 0.0263 - q6_loss: 0.0217 - q7_loss: 0.0147 - q8_loss: 0.0231 - val_loss: 0.1527 - val_q0_loss: 0.0173 - val_q1_loss: 0.0118 - val_q2_loss: 0.0094 - val_q3_loss: 0.0208 - val_q4_loss: 0.0102 - val_q5_loss: 0.0266 - val_q6_loss: 0.0222 - val_q7_loss: 0.0112 - val_q8_loss: 0.0231\n",
      "Epoch 285/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.1475 - q0_loss: 0.0193 - q1_loss: 0.0106 - q2_loss: 0.0100 - q3_loss: 0.0184 - q4_loss: 0.0084 - q5_loss: 0.0222 - q6_loss: 0.0177 - q7_loss: 0.0138 - q8_loss: 0.0275 - val_loss: 0.1154 - val_q0_loss: 0.0168 - val_q1_loss: 0.0104 - val_q2_loss: 0.0082 - val_q3_loss: 0.0113 - val_q4_loss: 0.0070 - val_q5_loss: 0.0137 - val_q6_loss: 0.0116 - val_q7_loss: 0.0124 - val_q8_loss: 0.0176\n",
      "Epoch 286/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.1226 - q0_loss: 0.0189 - q1_loss: 0.0117 - q2_loss: 0.0108 - q3_loss: 0.0100 - q4_loss: 0.0076 - q5_loss: 0.0113 - q6_loss: 0.0094 - q7_loss: 0.0163 - q8_loss: 0.0269 - val_loss: 0.1490 - val_q0_loss: 0.0158 - val_q1_loss: 0.0108 - val_q2_loss: 0.0159 - val_q3_loss: 0.0112 - val_q4_loss: 0.0123 - val_q5_loss: 0.0138 - val_q6_loss: 0.0147 - val_q7_loss: 0.0218 - val_q8_loss: 0.0260\n",
      "Epoch 287/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1453 - q0_loss: 0.0188 - q1_loss: 0.0112 - q2_loss: 0.0122 - q3_loss: 0.0149 - q4_loss: 0.0089 - q5_loss: 0.0187 - q6_loss: 0.0150 - q7_loss: 0.0165 - q8_loss: 0.0282 - val_loss: 0.1219 - val_q0_loss: 0.0190 - val_q1_loss: 0.0121 - val_q2_loss: 0.0125 - val_q3_loss: 0.0083 - val_q4_loss: 0.0065 - val_q5_loss: 0.0103 - val_q6_loss: 0.0083 - val_q7_loss: 0.0165 - val_q8_loss: 0.0264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1150 - q0_loss: 0.0181 - q1_loss: 0.0108 - q2_loss: 0.0091 - q3_loss: 0.0099 - q4_loss: 0.0068 - q5_loss: 0.0117 - q6_loss: 0.0103 - q7_loss: 0.0147 - q8_loss: 0.0247 - val_loss: 0.1354 - val_q0_loss: 0.0200 - val_q1_loss: 0.0138 - val_q2_loss: 0.0095 - val_q3_loss: 0.0136 - val_q4_loss: 0.0111 - val_q5_loss: 0.0161 - val_q6_loss: 0.0152 - val_q7_loss: 0.0149 - val_q8_loss: 0.0209\n",
      "Epoch 289/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1254 - q0_loss: 0.0196 - q1_loss: 0.0110 - q2_loss: 0.0077 - q3_loss: 0.0129 - q4_loss: 0.0096 - q5_loss: 0.0150 - q6_loss: 0.0133 - q7_loss: 0.0139 - q8_loss: 0.0226 - val_loss: 0.1268 - val_q0_loss: 0.0169 - val_q1_loss: 0.0119 - val_q2_loss: 0.0070 - val_q3_loss: 0.0138 - val_q4_loss: 0.0087 - val_q5_loss: 0.0174 - val_q6_loss: 0.0158 - val_q7_loss: 0.0116 - val_q8_loss: 0.0193\n",
      "Epoch 290/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.1250 - q0_loss: 0.0181 - q1_loss: 0.0113 - q2_loss: 0.0079 - q3_loss: 0.0132 - q4_loss: 0.0091 - q5_loss: 0.0150 - q6_loss: 0.0142 - q7_loss: 0.0142 - q8_loss: 0.0231 - val_loss: 0.1340 - val_q0_loss: 0.0181 - val_q1_loss: 0.0107 - val_q2_loss: 0.0072 - val_q3_loss: 0.0155 - val_q4_loss: 0.0078 - val_q5_loss: 0.0182 - val_q6_loss: 0.0142 - val_q7_loss: 0.0109 - val_q8_loss: 0.0208\n",
      "Epoch 291/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1537 - q0_loss: 0.0195 - q1_loss: 0.0122 - q2_loss: 0.0099 - q3_loss: 0.0186 - q4_loss: 0.0129 - q5_loss: 0.0223 - q6_loss: 0.0199 - q7_loss: 0.0156 - q8_loss: 0.0249 - val_loss: 0.2189 - val_q0_loss: 0.0164 - val_q1_loss: 0.0160 - val_q2_loss: 0.0250 - val_q3_loss: 0.0206 - val_q4_loss: 0.0213 - val_q5_loss: 0.0261 - val_q6_loss: 0.0260 - val_q7_loss: 0.0338 - val_q8_loss: 0.0399\n",
      "Epoch 292/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1861 - q0_loss: 0.0211 - q1_loss: 0.0150 - q2_loss: 0.0188 - q3_loss: 0.0169 - q4_loss: 0.0151 - q5_loss: 0.0207 - q6_loss: 0.0186 - q7_loss: 0.0255 - q8_loss: 0.0349 - val_loss: 0.2130 - val_q0_loss: 0.0182 - val_q1_loss: 0.0140 - val_q2_loss: 0.0243 - val_q3_loss: 0.0274 - val_q4_loss: 0.0107 - val_q5_loss: 0.0335 - val_q6_loss: 0.0247 - val_q7_loss: 0.0282 - val_q8_loss: 0.0489\n",
      "Epoch 293/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1993 - q0_loss: 0.0189 - q1_loss: 0.0120 - q2_loss: 0.0223 - q3_loss: 0.0218 - q4_loss: 0.0089 - q5_loss: 0.0268 - q6_loss: 0.0189 - q7_loss: 0.0262 - q8_loss: 0.0432 - val_loss: 0.2263 - val_q0_loss: 0.0161 - val_q1_loss: 0.0172 - val_q2_loss: 0.0327 - val_q3_loss: 0.0105 - val_q4_loss: 0.0187 - val_q5_loss: 0.0139 - val_q6_loss: 0.0159 - val_q7_loss: 0.0423 - val_q8_loss: 0.0552\n",
      "Epoch 294/300\n",
      "269/269 [==============================] - 0s 169us/sample - loss: 0.1803 - q0_loss: 0.0197 - q1_loss: 0.0135 - q2_loss: 0.0179 - q3_loss: 0.0161 - q4_loss: 0.0137 - q5_loss: 0.0187 - q6_loss: 0.0172 - q7_loss: 0.0252 - q8_loss: 0.0376 - val_loss: 0.1946 - val_q0_loss: 0.0161 - val_q1_loss: 0.0109 - val_q2_loss: 0.0111 - val_q3_loss: 0.0333 - val_q4_loss: 0.0162 - val_q5_loss: 0.0411 - val_q6_loss: 0.0318 - val_q7_loss: 0.0108 - val_q8_loss: 0.0247\n",
      "Epoch 295/300\n",
      "269/269 [==============================] - 0s 173us/sample - loss: 0.1801 - q0_loss: 0.0205 - q1_loss: 0.0130 - q2_loss: 0.0092 - q3_loss: 0.0248 - q4_loss: 0.0158 - q5_loss: 0.0311 - q6_loss: 0.0257 - q7_loss: 0.0158 - q8_loss: 0.0257 - val_loss: 0.1395 - val_q0_loss: 0.0173 - val_q1_loss: 0.0111 - val_q2_loss: 0.0143 - val_q3_loss: 0.0117 - val_q4_loss: 0.0102 - val_q5_loss: 0.0143 - val_q6_loss: 0.0135 - val_q7_loss: 0.0190 - val_q8_loss: 0.0233\n",
      "Epoch 296/300\n",
      "269/269 [==============================] - 0s 171us/sample - loss: 0.1550 - q0_loss: 0.0186 - q1_loss: 0.0129 - q2_loss: 0.0154 - q3_loss: 0.0107 - q4_loss: 0.0111 - q5_loss: 0.0130 - q6_loss: 0.0123 - q7_loss: 0.0236 - q8_loss: 0.0354 - val_loss: 0.1510 - val_q0_loss: 0.0177 - val_q1_loss: 0.0112 - val_q2_loss: 0.0146 - val_q3_loss: 0.0108 - val_q4_loss: 0.0120 - val_q5_loss: 0.0126 - val_q6_loss: 0.0130 - val_q7_loss: 0.0199 - val_q8_loss: 0.0259\n",
      "Epoch 297/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.1382 - q0_loss: 0.0185 - q1_loss: 0.0109 - q2_loss: 0.0094 - q3_loss: 0.0143 - q4_loss: 0.0106 - q5_loss: 0.0169 - q6_loss: 0.0149 - q7_loss: 0.0160 - q8_loss: 0.0250 - val_loss: 0.1230 - val_q0_loss: 0.0155 - val_q1_loss: 0.0117 - val_q2_loss: 0.0089 - val_q3_loss: 0.0115 - val_q4_loss: 0.0104 - val_q5_loss: 0.0146 - val_q6_loss: 0.0143 - val_q7_loss: 0.0165 - val_q8_loss: 0.0196\n",
      "Epoch 298/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.1242 - q0_loss: 0.0187 - q1_loss: 0.0117 - q2_loss: 0.0089 - q3_loss: 0.0113 - q4_loss: 0.0101 - q5_loss: 0.0131 - q6_loss: 0.0122 - q7_loss: 0.0156 - q8_loss: 0.0234 - val_loss: 0.1084 - val_q0_loss: 0.0182 - val_q1_loss: 0.0123 - val_q2_loss: 0.0067 - val_q3_loss: 0.0115 - val_q4_loss: 0.0078 - val_q5_loss: 0.0127 - val_q6_loss: 0.0110 - val_q7_loss: 0.0110 - val_q8_loss: 0.0182\n",
      "Epoch 299/300\n",
      "269/269 [==============================] - 0s 167us/sample - loss: 0.1713 - q0_loss: 0.0216 - q1_loss: 0.0153 - q2_loss: 0.0126 - q3_loss: 0.0173 - q4_loss: 0.0171 - q5_loss: 0.0215 - q6_loss: 0.0214 - q7_loss: 0.0226 - q8_loss: 0.0276 - val_loss: 0.1364 - val_q0_loss: 0.0186 - val_q1_loss: 0.0131 - val_q2_loss: 0.0115 - val_q3_loss: 0.0130 - val_q4_loss: 0.0093 - val_q5_loss: 0.0165 - val_q6_loss: 0.0152 - val_q7_loss: 0.0163 - val_q8_loss: 0.0255\n",
      "Epoch 300/300\n",
      "269/269 [==============================] - 0s 177us/sample - loss: 0.1555 - q0_loss: 0.0202 - q1_loss: 0.0133 - q2_loss: 0.0091 - q3_loss: 0.0158 - q4_loss: 0.0138 - q5_loss: 0.0195 - q6_loss: 0.0186 - q7_loss: 0.0186 - q8_loss: 0.0261 - val_loss: 0.1441 - val_q0_loss: 0.0159 - val_q1_loss: 0.0114 - val_q2_loss: 0.0115 - val_q3_loss: 0.0121 - val_q4_loss: 0.0136 - val_q5_loss: 0.0152 - val_q6_loss: 0.0154 - val_q7_loss: 0.0191 - val_q8_loss: 0.0217\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_mo, epochs=300,\n",
    "                    validation_data=(X_val, y_val_mo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4129757 , 0.4880843 , 0.6146252 , 0.646021  , 0.7335391 ,\n",
       "        0.8156561 , 0.85565615, 0.9854189 , 1.0627505 ],\n",
       "       [0.8513606 , 1.003415  , 1.243402  , 1.3182895 , 1.485189  ,\n",
       "        1.6518749 , 1.7270449 , 1.973032  , 2.114153  ]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# revert multi output format to (n_samples, quantiles)\n",
    "y_pred = np.array(list(zip(*y_pred))).squeeze()\n",
    "y_pred[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: training seems slower! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement custom layer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Assumes the following order of statistical distribution parameters for\n",
    "# the neurons in the layer before this custom layer:\n",
    "# x[0] = mu\n",
    "# x[1] = sigma\n",
    "# x[2] = skewness\n",
    "# x[3] = kurtosis\n",
    "def DistributionLayer(quantile_index,x):\n",
    "    mu = x[0]\n",
    "    sigma = x[1] \n",
    "    skewness = None\n",
    "    kurtosis = None\n",
    "    \n",
    "    # absorb extra statistical distribution parameters if present\n",
    "    if (len(x.get_shape().as_list())>2):\n",
    "        skewness= x[2]\n",
    "    if (len(x.get_shape().as_list())>3):\n",
    "        kurtosis = x[3]\n",
    "        \n",
    "    if (skewness==None):\n",
    "        if (kurtosis==None):\n",
    "            # Source of Z-scores: https://www.wolframalpha.com/input/?i=percentiles+of+a+normal+distribution\n",
    "             return {\n",
    "                0.005: mu-2.57583*sigma, # https://www.wolframalpha.com/input/?i=0.5+percentiles+of+a+normal+distribution\n",
    "                0.025: mu-1.95996*sigma, # https://www.wolframalpha.com/input/?i=2.5+percentiles+of+a+normal+distribution\n",
    "                0.165: mu-0.974114*sigma, # https://www.wolframalpha.com/input/?i=16.5+percentiles+of+a+normal+distribution\n",
    "                0.25: mu-0.674*sigma, # https://www.wolframalpha.com/input/?i=25+percentiles+of+a+normal+distribution\n",
    "                0.5: mu, # https://www.wolframalpha.com/input/?i=50+percentiles+of+a+normal+distribution\n",
    "                0.75: mu+0.674*sigma, # https://www.wolframalpha.com/input/?i=75+percentiles+of+a+normal+distribution\n",
    "                0.835: mu+0.9741114*sigma, #https://www.wolframalpha.com/input/?i=83.5+percentiles+of+a+normal+distribution\n",
    "                0.975: mu+1.95996*sigma, #https://www.wolframalpha.com/input/?i=97.5+percentiles+of+a+normal+distribution\n",
    "                0.995: mu+2.57583*sigma, #https://www.wolframalpha.com/input/?i=99.5+percentiles+of+a+normal+distribution\n",
    "            }[quantile]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.wolframalpha.com/input/?i=percentiles+of+a+normal+distribution\n",
    "sigma_coefs = [-2.57583, -1.95996, -0.974114, -0.674, 0, 0.674, 0.9741114, 1.95996, 2.57583]\n",
    "import scipy\n",
    "\n",
    "# Source:https://stats.stackexchange.com/questions/443334/numerical-approximation-to-quantile-function-for-gamma-distribution\n",
    "# Source:http://wwwens.aero.jussieu.fr/lefrere/master/SPE/docs-python/scipy-doc/generated/scipy.special.gdtrix.html\\\n",
    "# takes quantile in range [0,1] or (0,1) or [0,1) or (0,1] # doubt\n",
    "def get_custom_layer_skewed(quantile):\n",
    "    def custom_layer(tensor):\n",
    "        tensor1 = tensor[0] # The rate parameter Beta (float). It is also the reciprocal of the scale parameter theta.\n",
    "        tensor2 = tensor[1] # The shape parameter Alpha of the gamma distribution, sometimes denoted  (float).\n",
    "        print(f'quantile={quantile},beta={tensor1},alpha={tensor2}')\n",
    "        print(f'returning={scipy.special.gdtrix(tensor1, tensor2, quantile, out=None)}')\n",
    "        return scipy.special.gdtrix(tensor1, tensor2, quantile, out=None)\n",
    "\n",
    "    return custom_layer\n",
    "\n",
    "def get_dist_model(inp_shape, quantiles):\n",
    "    # clear previous sessions\n",
    "    K.clear_session()\n",
    "\n",
    "    inp = Input(inp_shape, name=\"input\")\n",
    "    x = inp\n",
    "    x = Dense(16)(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Dense(64)(x)\n",
    "\n",
    "    beta = Dense(1)(x)  # represents beta\n",
    "    alpha = Dense(1)(x)  # represents alpha\n",
    "\n",
    "    outs = []\n",
    "\n",
    "    for i, quantile in enumerate(quantiles):\n",
    "        #custom_layer = get_custom_layer(sigma_coef=sigma_coef)\n",
    "        custom_layer = get_custom_layer_skewed(quantile=quantile)\n",
    "        out_q = Lambda(custom_layer, name=\"q{}\".format(i))([beta, alpha])\n",
    "        outs.append(out_q)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outs) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantile=0.005,beta=Tensor(\"dense_3/Identity:0\", shape=(None, 1), dtype=float32),alpha=Tensor(\"dense_4/Identity:0\", shape=(None, 1), dtype=float32)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (dense_3/Identity:0) to a numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-042c8d753aea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_dist_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquantiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"MAE\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-d9268b87f855>\u001b[0m in \u001b[0;36mget_dist_model\u001b[1;34m(inp_shape, quantiles)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m#custom_layer = get_custom_layer(sigma_coef=sigma_coef)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mcustom_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_custom_layer_skewed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquantile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquantile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mout_q\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"q{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mouts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_q\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    771\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[0;32m    772\u001b[0m                   \u001b[1;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m                     \u001b[1;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m                     \u001b[1;31m# circular dependencies.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training)\u001b[0m\n\u001b[0;32m    844\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwatch_accessed_variables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m         \u001b[0mvariable_scope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_variable_creator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 846\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreated_variables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatched_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-d9268b87f855>\u001b[0m in \u001b[0;36mcustom_layer\u001b[1;34m(tensor)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mtensor2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# The shape parameter Alpha of the gamma distribution, sometimes denoted  (float).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'quantile={quantile},beta={tensor1},alpha={tensor2}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'returning={scipy.special.gdtrix(tensor1, tensor2, quantile, out=None)}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgdtrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquantile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    726\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m     raise NotImplementedError(\"Cannot convert a symbolic Tensor ({}) to a numpy\"\n\u001b[1;32m--> 728\u001b[1;33m                               \" array.\".format(self.name))\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (dense_3/Identity:0) to a numpy array."
     ]
    }
   ],
   "source": [
    "model = get_dist_model(inp_shape=(train_df.columns.size,), quantiles=quantiles)\n",
    "model.compile(optimizer=\"adam\", loss=\"MAE\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_mo = {'q'+str(i): y_train[:, i] for i in range(len(quantiles))}\n",
    "y_val_mo = {'q'+str(i): y_val[:, i] for i in range(len(quantiles))}\n",
    "y_train_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train_mo, epochs=300,\n",
    "                    validation_data=(X_val, y_val_mo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# revert multi output format to (n_samples, quantiles)\n",
    "y_pred = np.array(list(zip(*y_pred))).squeeze()\n",
    "y_pred[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employ pinball loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Daniel Sch., at:\n",
    "# https://stackoverflow.com/questions/43151694/define-pinball-loss-function-in-keras-with-tensorflow-backend\n",
    "def create_pinball_loss(tau=0.5):\n",
    "    def pinball_loss(y_true, y_pred):\n",
    "        err = y_true - y_pred\n",
    "        return K.mean(K.maximum(tau * err, (tau - 1) * err), axis=-1)\n",
    "    return pinball_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {'q'+str(i): create_pinball_loss(tau=q) for (i, q) in enumerate(quantiles)}\n",
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(inp_shape=(train_df.columns.size,), quantiles=quantiles)\n",
    "model.compile(optimizer=\"adam\", loss=losses)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train_mo, epochs=300,\n",
    "                    validation_data=(X_val, y_val_mo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict validation set\n",
    "y_pred = model.predict(X_val)\n",
    "# revert multi output format to (n_samples, quantiles)\n",
    "y_pred = np.array(list(zip(*y_pred))).squeeze()\n",
    "y_pred[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With distribution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_dist_model(inp_shape=(train_df.columns.size,), sigma_coefs=sigma_coefs)\n",
    "model.compile(optimizer=\"adam\", loss=losses)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train_mo, epochs=300,\n",
    "                    validation_data=(X_val, y_val_mo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict validation set\n",
    "y_pred = model.predict(X_val)\n",
    "# revert multi output format to (n_samples, quantiles)\n",
    "y_pred = np.array(list(zip(*y_pred))).squeeze()\n",
    "y_pred[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the distribution of predictions is wider than when trained with the MAE. This is in line with what we would expect: over-predicting the lower quantiles is punished much harder than before, and the same for under-predicting the higher quantiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pinball loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/keras-team/keras/pull/8033/files\n",
    "def test_pinball_loss():\n",
    "    y_pred = K.variable(np.array([0.3, 0.6, 0.1]))\n",
    "    y_true = K.variable(np.array([0.3, 0.4, 0.5]))\n",
    "    quantile = 0.25\n",
    "    loss_fcn = create_pinball_loss(tau=quantile)#losses.PinballLoss(quantile)\n",
    "    expected_loss = (quantile * 0.4 + (1 - quantile) * 0.2) / 3\n",
    "    loss = K.eval(loss_fcn(y_true, y_pred))\n",
    "    assert np.isclose(expected_loss, loss)\n",
    "\n",
    "test_pinball_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"custom_layer/features2.csv\", index_col=0)\n",
    "target_df = pd.read_csv(\"custom_layer/targets2.csv\", index_col=0, header=None, names=['target'])\n",
    "quantiles = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Daniel Sch., at:\n",
    "# https://stackoverflow.com/questions/43151694/define-pinball-loss-function-in-keras-with-tensorflow-backend\n",
    "def create_pinball_loss(tau=0.5):\n",
    "    def pinball_loss(y_true, y_pred):\n",
    "        err = y_true - y_pred\n",
    "        return K.mean(K.maximum(tau * err, (tau - 1) * err), axis=-1)\n",
    "    return pinball_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.values\n",
    "y = target_df.values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train_mo = {'q'+str(i): y_train for i in range(len(quantiles))}\n",
    "y_val_mo = {'q'+str(i): y_val for i in range(len(quantiles))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {'q'+str(i): create_pinball_loss(tau=q) for (i, q) in enumerate(quantiles)}\n",
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(inp_shape, quantiles):\n",
    "    # clear previous sessions\n",
    "    K.clear_session()\n",
    "\n",
    "    inp = Input(inp_shape, name=\"input\")\n",
    "    x = inp\n",
    "    x = Dense(16)(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = Dense(2)(x)  # represents mu, sigma\n",
    "    \n",
    "    out_q0 = Dense(1, name=\"q0\")(x)  # DistributionLayer(quantile=quantiles[0])(x)\n",
    "    out_q1 = Dense(1, name=\"q1\")(x)  # DistributionLayer(quantile=quantiles[1])(x)\n",
    "    out_q2 = Dense(1, name=\"q2\")(x)  # ...\n",
    "    out_q3 = Dense(1, name=\"q3\")(x)\n",
    "    out_q4 = Dense(1, name=\"q4\")(x)\n",
    "    out_q5 = Dense(1, name=\"q5\")(x)\n",
    "    out_q6 = Dense(1, name=\"q6\")(x)\n",
    "    out_q7 = Dense(1, name=\"q7\")(x)\n",
    "    out_q8 = Dense(1, name=\"q8\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=[out_q0, out_q1, out_q2, out_q3, out_q4, out_q5, out_q6, out_q7, out_q8])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(inp_shape=(X_train.shape[1],), quantiles=quantiles)\n",
    "model.compile(optimizer=\"adam\", loss=losses)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train_mo, epochs=300,\n",
    "                    validation_data=(X_val, y_val_mo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicted distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict validation set\n",
    "y_pred = model.predict(X_val)\n",
    "# revert multi output format to (n_samples, quantiles)\n",
    "y_pred = np.array(list(zip(*y_pred))).squeeze()\n",
    "y_pred[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With distribution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_dist_model(inp_shape=(X_train.shape[1],), sigma_coefs=sigma_coefs)\n",
    "model.compile(optimizer=\"adam\", loss=losses)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dist = model.fit(X_train, y_train_mo, epochs=300,\n",
    "                         validation_data=(X_val, y_val_mo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(18, 6))\n",
    "start_step = 0\n",
    "\n",
    "# ax.plot(history.history['loss'][start_step:], label=\"Train (dense)\")\n",
    "ax.plot(history.history['val_loss'][start_step:], label=\"Validation (dense)\")\n",
    "# ax.plot(history_dist.history['loss'][start_step:], label=\"Train (distibution layer)\")\n",
    "ax.plot(history_dist.history['val_loss'][start_step:], label=\"Validation (distibution layer)\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(18, 6))\n",
    "start_step = 50\n",
    "\n",
    "# ax.plot(history.history['loss'][start_step:], label=\"Train (dense)\")\n",
    "ax.plot(history.history['val_loss'][start_step:], label=\"Validation (dense)\")\n",
    "# ax.plot(history_dist.history['loss'][start_step:], label=\"Train (distibution layer)\")\n",
    "ax.plot(history_dist.history['val_loss'][start_step:], label=\"Validation (distibution layer)\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"true\" quantiles\n",
    "pd.Series(np.random.normal(0.75, 0.13, size=100000)).quantile(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"true\" quantiles\n",
    "pd.Series(np.random.normal(1.5, 0.25, size=100000)).quantile(quantiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**: The toy dataset contains two simple distributions, with either $\\mu=0.75, \\sigma=0.13$ or $\\mu=1.5, \\sigma=0.25$, depending on whether it is a weekday or weekend. The observed 'demand' are samples distributed as such. For these simple distributions, the Pinball Loss is able to (approximately) retrieve the correct quantiles!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01324b84642f4e1fafc49cf89f9ff391": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2573316ddee1409d897edfcfbd87e8ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5712bde02e284e70bc5e9c1d7367502a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_01324b84642f4e1fafc49cf89f9ff391",
       "placeholder": "",
       "style": "IPY_MODEL_860311c3b0564afa891a4c6e24da2d3a",
       "value": " 42840/42840 [00:49&lt;00:00, 860.84it/s]"
      }
     },
     "5f64a626e4d342ccaa955ea1c68afacd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6314c6fca7a845618625e72ac301f6a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2573316ddee1409d897edfcfbd87e8ba",
       "max": 42840,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7370a0011b714f6dbba43bf3f3725de8",
       "value": 42840
      }
     },
     "7370a0011b714f6dbba43bf3f3725de8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "860311c3b0564afa891a4c6e24da2d3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8c18e2818bf648e08be0bc8e855fcb3f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6314c6fca7a845618625e72ac301f6a7",
        "IPY_MODEL_5712bde02e284e70bc5e9c1d7367502a"
       ],
       "layout": "IPY_MODEL_5f64a626e4d342ccaa955ea1c68afacd"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
